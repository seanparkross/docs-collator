

--- File: ../ddn-docs/docs/index.mdx ---
# Hasura DDN Documentation

---
title: Hasura DDN Documentation
description:
  "Explore comprehensive documentation on the Hasura Data Delivery Network (DDN). Discover its ability to generate
  instant GraphQL APIs on various data sources and the suite of developer tools it offers for increased productivity and
  management. Learn how Hasura DDN can enable your backend teams to provide data access from the edge efficiently and
  securely."
keywords:
  - hasura ddn
  - graphql api
  - instant graphql api
  - data sources integration
  - developer productivity tools
  - hasura ddn documentation
  - data delivery network
  - backend data access
  - api edge delivery
  - declarative metadata
sidebar_label: Introduction
sidebar_class_name: hidden-sidebar-item
sidebar_position: 0
slug: index
hide_table_of_contents: true
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";
import Basics from "@site/static/icons/book-open-01.svg";
import GraphQLAPI from "@site/static/icons/graphql-logo.svg";
import ChevronRight from "@site/static/icons/chevron-right.svg";
import Quickstart from "@site/static/icons/cloud-lightning.svg";
import Introduction from "@site/static/icons/award-02.svg";
import { OverviewIconCard } from "@site/src/components/OverviewIconCard";
import Link from "@docusaurus/Link";

# Hasura DDN Documentation

<div className={'front-matter'}>
  <div>
    Hasura DDN streamlines the development and operation of modern federated data APIs, <Link to="/how-to-build-with-ddn/overview/">solving key challenges</Link> in the process.

    With powerful tooling and a code-driven workflow, Hasura DDN is not just an improvement in the data access
    portion of the product development lifecycle, but a complete game-changer in the way you create, run, and manage APIs.

    For information on upgrading from Hasura v2, see the [upgrade guide](/upgrade/overview.mdx).

    <Link to="/quickstart/" className="cta-button mb-5 xl:mb-0 xl:mt-8">
      <span>Quickstart.</span> Free. Easy. Fast.
    </Link>

  </div>
  <div className={'video-wrapper'}>
    <div className={'video-aspect-ratio'}>
      <iframe
        src={"https://www.youtube.com/embed/wppOexzYOMw"}
        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
        allowFullScreen
      />
    </div>
  </div>
</div>



--- File: ../ddn-docs/docs/quickstart.mdx ---
# Quickstart

---
sidebar_position: 0
sidebar_label: Quickstart
description: "Create your first supergraph API with Hasura DDN in less than a minute."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
  - quickstart
sidebar_class_name: quickstart-icon
hide_table_of_contents: true
is_guide: true
---

import Step from "@site/src/components/CodeStep";
import InstallTheCli from "@site/docs/_install-the-cli.mdx";
import CodeBlock from "@theme/CodeBlock";
import SimpleVideo from "@site/src/components/SimpleVideo";
import Admonition from "@theme/Admonition";

# Quickstart with Hasura DDN

In less than <em>a minute</em> and without needing a data source connection string, you can have a supergraph API
running locally and deployed on Hasura DDN. Check out the video [here](https://www.youtube.com/watch?v=OsO6TzwFb30).

## Prerequisites

1. **Install the DDN CLI**

   <InstallTheCli />

2. **Install [Docker](https://docs.docker.com/engine/install/)** (needed for local development)

   The Docker based workflow helps you iterate and develop locally without deploying any changes to Hasura DDN, making
   the development experience faster and your feedback loops shorter. **You'll need Docker Compose `v2.20` or later.**

3. **Validate the installation**

   You can verify that the DDN CLI is installed correctly by running:

   <CodeBlock language="bash">{`ddn doctor`}</CodeBlock>

<Step
id="login"
language="bash"
code={`ddn auth login`}
heading={`Log in via the CLI`}
>

After you log in, the CLI will acknowledge your login and give you access to Hasura Cloud resources.

</Step>

<Step
id="supergraph-init"
language="bash"
code={'ddn supergraph init mysupergraph\ncd mysupergraph'}
heading={`Initialize a new supergraph in a new directory`}
>

Once you move into this new directory, you'll see your project's files scaffolded out for you by running `ls`.

</Step>

<Step
id="connector-init"
language="bash"
code={`ddn connector init my_connector -i`}
heading={`Connect to data`}
>

From the dropdown, choose the `hasura/postgres` data connector and connect to our sample PostgreSQL database using this
URL: `postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app`

</Step>

<Step
id="connector-introspect"
language="bash"
code={`ddn connector introspect my_connector`}
heading={`Introspect your data source`}
>

This will create Hasura metadata describing the schema of your data source. After running this, you should see tables
present in your `app/connector/my_connector/configuration.json` file, which you can open in your preferred editor.

</Step>

<Step
id="add-resources"
language="bash"
code={'ddn model add my_connector \'*\'\nddn command add my_connector \'*\'\nddn relationship add my_connector \'*\''}
heading={`Add your resources`}
>

Create metadata for [models](/reference/metadata-reference/models.mdx),
[commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx) in your supergraph. These define the structure,
operations, and connections within your supergraph and are generated as Hasura Metadata Language (HML) present in your
`app/metadata` directory.

</Step>

<Step
id="build-local"
language="bash"
code={`ddn supergraph build local`}
heading={`Build your supergraph`}
>

Create an immutable build of your supergraph and the local assets to run the engine. The build is stored as a set of
JSON files in `engine/build`.

</Step>

<Step
id="run-local"
language="bash"
code={`ddn run docker-start`}
heading={`Start your supergraph`}
>

Start your engine, connector, and other services. You can see a list of all running services using `docker ps`.

</Step>

<Step
id="open-local-console"
language="bash"
code={`ddn console --local`}
heading={`Open the console`}
>

In a new terminal tab open to the project's directory, open the console â€” Hasura DDN's GUI â€” so you can test out your
API.

</Step>

<Step
id="local-query"
language="graphql"
code={`query UsersAndOrders{
  users {
    id
    name
    email
    orders {
      id 
      created_at
      status
    }
  }
}
`}
heading={`Query your data`}
>

Execute this query via your console. You'll see a list of all users and their orders returned!

</Step>

<Step
id="project-init"
language="bash"
code={`ddn project init`}
heading={`Create a Hasura DDN project`}
>

Provision a new project on Hasura DDN, which serves as the deployment environment for your supergraph. Once complete,
the CLI will return the project's name.

</Step>

<Step
id="build-create"
language="bash"
code={`ddn supergraph build create`}
heading={`Build and deploy your supergraph`}
>

When this process is complete, the CLI will return a link to the hosted API where you can run the same query as before.

**That's it! you've created your very first, fully-functioning supergraph and deployed it to Hasura DDN ðŸŽ‰**

</Step>

## Next steps

What we've laid out above is the quickest way to get started with Hasura DDN. However, you have complete, granular
control over each step in the process and can extend and customize your supergraph to fit your teams' needs. After
deploying your first supergraph, take your next steps with Hasura DDN ðŸ‘‡

- [Connect to your own PostgreSQL](/how-to-build-with-ddn/with-postgresql.mdx)



--- File: ../ddn-docs/docs/how-to-build-with-ddn/overview.mdx ---
# Basics

---
sidebar_position: 1
sidebar_label: Basics
description: "Learn how to get started with Hasura DDN and your GraphQL API."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
---

import { OverviewTopSectionIconNoVideo } from "@site/src/components/OverviewTopSectionIconNoVideo";
import { OverviewPlainCard } from "@site/src/components/OverviewPlainCard";
import Icon from "@site/static/icons/speedometer-04.svg";
import Link from "@docusaurus/Link";

# Basics

## Introduction

Hasura DDN enables a powerful, yet simple-to-create API on all your data. Each tutorial in this section will guide you
through creating your first Hasura DDN API on top of your preferred data source. First, let's go over the broad stokes
of how it works.

### You have data

Your data can live in a variety of places - relational or non-relational databases, OLAP or OLTP databases, vector
stores, third-party services, or output by your code. With Hasura you can access all of it from a single API endpoint.

### You connect it to Hasura

Regardless of where your data lives, you'll connect it to your API using a **native data connector**. These connectors
will introspect your data sources and generate metadata that Hasura DDN will use to build your API.

### You configure your API in metadata

This **semantic metadata layer** is written as Hasura Metadata Language (HML), which is an extension of YAML. You can
use the DDN CLI and your text editor to fine-tune and configure the API to your liking; this includes easily creating
metadata objects for permissions, relationships, configuring authentication, etc.

### You serve the API

To convert your metadata into something that you can query, you'll create a build - an immutable snapshot of your API at
that point in time which the Hasura Engine can run and serve. You can then use the **console** - Hasura's GUI - to query
and analyze your GraphQL API, or integrate it with your client apps.

You can serve this API locally during development for testing and iteration, on Hasura DDN for scalability and ease of
use, or on your own infrastructure if you need complete control and customization.

## Get started with your favorite source

- [PostgreSQL](/how-to-build-with-ddn/with-postgresql.mdx)
- [MongoDB](/how-to-build-with-ddn/with-mongodb.mdx)
- [ClickHouse](/how-to-build-with-ddn/with-clickhouse.mdx)
- [Others](/how-to-build-with-ddn/with-others.mdx)



--- File: ../ddn-docs/docs/how-to-build-with-ddn/with-postgresql.mdx ---
# With PostgreSQL

---
sidebar_position: 3
sidebar_label: With PostgreSQL
description: "Learn the basics of Hasura DDN and how to get started with a Postgres database."
keywords:
  - hasura ddn
  - graphql api
  - getting started
  - guide
  - postgres
---

import Prereqs from "@site/docs/_prereqs.mdx";

# Get Started with Hasura DDN and PostgreSQL

## Overview

This tutorial takes about twenty minutes to complete. You'll learn how to:

- Set up a new Hasura DDN project
- Connect it to a PostgreSQL database
- Generate Hasura metadata
- Create a build
- Run your first query
- Create relationships
- Mutate data

Additionally, we'll familiarize you with the steps and workflows necessary to iterate on your API.

This tutorial assumes you're starting from scratch; a PostgreSQL docker image ships with the data connector you'll use
in just a bit to connect a locally-running database to Hasura, but you can easily follow the steps if you already have
data seeded; Hasura will never modify your source schema.

<Prereqs />

## Tutorial

### Step 1. Authenticate your CLI

```sh title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your PostgreSQL connector

```sh title="In your project directory, run:"
ddn connector init my_pg -i
```

From the dropdown, select `/hasura/postgres` (you can type to filter the list), then hit enter to accept the default of all the options.

The CLI will output something similar to this:

```plaintext
HINT To access the local Postgres database:
- Run: docker compose -f app/connector/my_pg/compose.postgres-adminer.yaml up -d
- Open Adminer in your browser at http://localhost:5143 and create tables
- To connect to the database using other clients use postgresql://user:password@local.hasura.dev:8105/dev
```

### Step 4. Start the local PostgreSQL container and Adminer

```sh title="Use the hint from the CLI output:"
docker compose -f app/connector/my_pg/compose.postgres-adminer.yaml up -d
```

Run `docker ps` to see on which port Adminer is running. Then, you can then navigate to the address below to access it:

```plaintext
http://localhost:<ADMINER_PORT>
```

### Step 5. Create a table in your PostgreSQL database

```sql title="Next, via Adminer select SQL command from the left-hand nav, then enter the following:"
--- Create the table
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  age INT NOT NULL
);

--- Insert some data
INSERT INTO users (name, age) VALUES ('Alice', 25);
INSERT INTO users (name, age) VALUES ('Bob', 30);
INSERT INTO users (name, age) VALUES ('Charlie', 35);
```

You can verify this worked by using Adminer to query all records from the `users` table:

```sql
SELECT * FROM users;
```

### Step 6. Introspect your PostgreSQL database

```sh title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect my_pg
```

After running this, you should see a representation of your database's schema in the
`app/connector/my_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Additionally, you can check which resources are available â€”Â and their status â€” at any point using the CLI:"
ddn connector show-resources my_pg
```

### Step 7. Add your model

```sh title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add my_pg users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 8. Create a new build

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 9. Start your local services

```sh title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 10. Run your first query

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query {
  users {
    id
    name
    age
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "users": [
      {
        "id": 1,
        "name": "Alice",
        "age": 25
      },
      {
        "id": 2,
        "name": "Bob",
        "age": 30
      },
      {
        "id": 3,
        "name": "Charlie",
        "age": 35
      }
    ]
  }
}
```

### Step 11. Iterate on your PostgreSQL schema

```sql title="Via Adminer, add a new table and insert some data to your PostgreSQL database:"
-- Create the posts table
CREATE TABLE posts (
  id SERIAL PRIMARY KEY,
  user_id INT NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert some seed data
INSERT INTO posts (user_id, title, content) VALUES
  (1, 'My First Post', 'This is Alice''s first post.'),
  (1, 'Another Post', 'Alice writes again!'),
  (2, 'Bob''s Post', 'Bob shares his thoughts.'),
  (3, 'Hello World', 'Charlie joins the conversation.');
```

```sql title="Using Adminer, verify this by running the following query:"
-- Fetch all posts with user information
SELECT
  posts.id AS post_id,
  posts.title,
  posts.content,
  posts.created_at,
  users.name AS author
FROM
  posts
JOIN
  users ON posts.user_id = users.id;
```

You should see a list of posts returned with the author's information joined from the `users` table

### Step 12. Refresh your metadata and rebuild your project

:::tip

The following steps are necessary each time you make changes to your **source** schema. This includes, adding,
modifying, or dropping tables.

:::

#### Step 12.1. Re-introspect your data source

```sh title="Run the introspection command again:"
ddn connector introspect my_pg
```

In `app/connector/my_pg/configuration.json`, you'll see schema updated to include operations for the `posts` table. In
`app/metadata/my_pg.hml`, you'll see `posts` present in the metadata as well.

#### Step 12.2. Update your metadata

```sh title="Add the posts model:"
ddn model add my_pg "posts"
```

#### Step 12.3. Create a new build

```sh title="Next, create a new build:"
ddn supergraph build local
```

#### Step 12.4. Restart your services

```sh title="Bring down the servies by pressing CTRL+C and start them back up:"
ddn run docker-start
```

### Step 13. Query your new build

```graphql title="Head back to your console and query the posts model:"
query GetPosts {
  posts {
    id
    title
    content
  }
}
```

```json title="You'll get a response like this:"
{
  "data": {
    "posts": [
      {
        "id": 1,
        "title": "My First Post",
        "content": "This is Alice's first post."
      },
      {
        "id": 2,
        "title": "Another Post",
        "content": "Alice writes again!"
      },
      {
        "id": 3,
        "title": "Bob's Post",
        "content": "Bob shares his thoughts."
      },
      {
        "id": 4,
        "title": "Hello World",
        "content": "Charlie joins the conversation."
      }
    ]
  }
}
```

### Step 14. Create a relationship

```sh title="Since there's already a foreign key on the posts table in PostgreSQL, we can easily add the relationship:"
ddn relationship add my_pg "posts"
```

You'll see a new metadata object added to the `app/metadata/posts.hml` file of kind `Relationship` explaining the
relationship between `posts` and `users`.

### Step 15. Rebuild your project

```sh title="As your metadata has changed, create a new build:"
ddn supergraph build local
```

```sh title="Bring down the servies by pressing CTRL+C and start them back up:"
ddn run docker-start
```

### Step 16. Query using your relationship

```graphql title="Now, execute a nested query using your relationship:"
query GetPosts {
  posts {
    id
    title
    content
    user {
      id
      name
      age
    }
  }
}
```

```json title="Which should return a result like this:"
{
  "data": {
    "posts": [
      {
        "id": 1,
        "title": "My First Post",
        "content": "This is Alice's first post.",
        "user": {
          "id": 1,
          "name": "Alice",
          "age": 25
        }
      },
      {
        "id": 2,
        "title": "Another Post",
        "content": "Alice writes again!",
        "user": {
          "id": 1,
          "name": "Alice",
          "age": 25
        }
      },
      {
        "id": 3,
        "title": "Bob's Post",
        "content": "Bob shares his thoughts.",
        "user": {
          "id": 2,
          "name": "Bob",
          "age": 30
        }
      },
      {
        "id": 4,
        "title": "Hello World",
        "content": "Charlie joins the conversation.",
        "user": {
          "id": 3,
          "name": "Charlie",
          "age": 35
        }
      }
    ]
  }
}
```

### Step 17. Add all commands

We'll track the available operations â€” for inserting, updating, and deleting â€” on our `users` and `posts` tables as
commands.

```sh title="Add all available commands:"
ddn command add my_pg "*"
```

You'll see newly-generated metadata files in the `metadata` directory for your connector that represent insert, update,
and delete operations.

```sh title="As your metadata has changed, create a new build:"
ddn supergraph build local
```

```sh title="Bring down the servies by pressing CTRL+C and start them back up:"
ddn run docker-start
```

### Step 18. Insert new data

```graphql title="Create a new post for Charlie:"
mutation InsertSinglePost {
  insertPosts(
    objects: {
      content: "I am an expert in Bird Law and I demand satisfcation."
      title: "Charlie has more to say"
      userId: "3"
    }
  ) {
    returning {
      id
      title
      content
      user {
        id
        name
      }
    }
  }
}
```

You should see a response that returns your inserted data along with the `id` and `name` fields for the author.

## Next steps

Congratulations on completing your first Hasura DDN project with PostgreSQL! ðŸŽ‰

Here's what you just accomplished:

- You started with a fresh project and connected it to a local PostgreSQL database.
- You set up metadata to represent your tables and relationships, which acts as the blueprint for your API.
- Then, you created a build â€” essentially compiling everything into a ready-to-use API â€” and successfully ran your first
  GraphQL queries to fetch data.
- Along the way, you learned how to iterate on your schema and refresh your metadata to reflect changes.
- Finally, we looked at how to enable mutations and insert data using your new API.

Now, you're equipped to connect and expose your data, empowering you to iterate and scale with confidence. Great work!

Take a look at our PostgreSQL docs to learn more about how to use Hasura DDN with PostgreSQL. Or, if you're ready, get
started with adding [permissions](/reference/metadata-reference/permissions.mdx) to control access to your API.



--- File: ../ddn-docs/docs/how-to-build-with-ddn/with-mongodb.mdx ---
# With MongoDB

---
sidebar_position: 4
sidebar_label: With MongoDB
description: "Learn the basics of Hasura DDN and how to get started with a MongoDB database."
keywords:
  - hasura ddn
  - graphql api
  - getting started
  - guide
  - mongodb
  - mongo
---

import Prereqs from "@site/docs/_prereqs.mdx";

# Get Started with Hasura DDN and MongoDB

## Overview

This tutorial takes about fifteen minutes to complete. You'll learn how to:

- Set up a new Hasura DDN project
- Connect it to a MongoDB database
- Generate Hasura metadata
- Create a build
- Run your first query

Additionally, we'll familiarize you with the steps and workflows necessary to iterate on your API.

This tutorial assumes you're starting from scratch. We'll use a locally running MongoDB instance via Docker and connect
it to Hasura, but you can easily follow the steps if you already have data seeded; Hasura will never modify your source
schema.

<Prereqs />

## Tutorial

### Step 1. Install mongosh

This tutorial uses mongosh â€” the Mongo shell â€” to interact with the local MongoDB instance. You can download it
[here](https://www.mongodb.com/try/download/shell).

### Step 2. Authenticate your CLI

```sh title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 3. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 4. Initialize your MongoDB connector

```sh title="In your project directory, run:"
ddn connector init my_mongo -i
```

From the dropdown, start typing `mongo` and hit enter to accept the default port. Then, provide the following connection
string:

```plaintext
mongodb://local.hasura.dev:27017/my_database
```

### Step 5. Start the local MongoDB container

```sh title="Begin by creating a compose file for the Mongo service:"
touch app/connector/my_mongo/compose.mongo.yaml
```

```yaml title="Then, open the file and add the following:"
services:
  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27017:27017"
```

```sh title="Run the container:"
docker compose -f app/connector/my_mongo/compose.mongo.yaml up -d
```

```sh title="Use the mongosh shell to seed the database:"
docker exec -it mongodb mongosh my_database --eval "
db.users.insertMany([
  { user_id: 1, name: 'Alice', age: 25 },
  { user_id: 2, name: 'Bob', age: 30 },
  { user_id: 3, name: 'Charlie', age: 35 }
]);
"
```

The shell will return information about the newly-inserted `users` records.

### Step 6. Introspect your MongoDB database

```sh title="Next, use the CLI to introspect your MongoDB database:"
ddn connector introspect my_mongo
```

After running this, you should see a representation of your collection's schema in
`app/connector/my_mongo/schema/users.json`; you can view this using `cat` or open the file in your editor.

For each collection in your database, the MongoDB connector will generate a separate JSON file representing it.

```sh title="Additionally, you can check which resources are available â€”Â and their status â€” at any point using the CLI:"
ddn connector show-resources my_mongo
```

### Step 7. Add your model

```sh title="Now, track the collection from your MongoDB database as a model in your DDN metadata:"
ddn models add my_mongo users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` collections from MongoDB in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 8. Create a new build

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 9. Start your local services

```sh title="Start your local Hasura DDN Engine and MongoDB connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 10. Run your first query

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query {
  users {
    userId
    name
    age
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "users": [
      {
        "userId": 1,
        "name": "Alice",
        "age": 25
      },
      {
        "userId": 2,
        "name": "Bob",
        "age": 30
      },
      {
        "userId": 3,
        "name": "Charlie",
        "age": 35
      }
    ]
  }
}
```

### Step 11. Iterate on your MongoDB schema

```sh title="Let's add a new collection for posts:"

docker exec -it mongodb mongosh my_database --eval "
db.posts.insertMany([
  { user_id: 1, post_id: 1, title: 'My First Post', content: 'This is Alice\'s first post.' },
  { user_id: 1, post_id: 2, title: 'Another Post', content: 'Alice writes again!' },
  { user_id: 2, post_id: 3, title: 'Bob\'s Post', content: 'Bob shares his thoughts.' },
  { user_id: 3, post_id: 4, title: 'Hello World', content: 'Charlie joins the conversation.' }
]);
"
```

### Step 12. Refresh your metadata and rebuild your project

:::tip

The following steps are necessary each time you make changes to your **source** schema. This includes, adding,
modifying, or dropping collections.

:::

#### Step 12.1. Re-introspect your data source

```sh title="Run the introspection command again:"
ddn connector introspect my_mongo
```

In `app/connector/my_mongo/configuration.json`, you'll see schema updated to include operations for the `posts`
collection. You'll also see a `posts.json` file in the `schema` directory.

#### Step 12.2. Update your metadata

```sh title="Add the posts model:"
ddn model add my_mongo posts
```

#### Step 12.3. Kill your services

Bring down the services by pressing `CTRL+C` in the terminal tab logging their activity.

#### Step 12.4. Create a new build

```sh title="Next, create a new build:"
ddn supergraph build local
```

#### Step 12.5 Restart your services

```sh title="Bring everything back up:"
ddn run docker-start
```

### Step 13. Query your new build

```graphql title="Head back to your console and query the posts model:"
query GetPosts {
  posts {
    userId
    postId
    title
    content
  }
}
```

```json title="You'll get a response like this:"
{
  "data": {
    "posts": [
      {
        "userId": 1,
        "postId": 1,
        "title": "My First Post",
        "content": "This is Alice's first post."
      },
      {
        "userId": 1,
        "postId": 2,
        "title": "Another Post",
        "content": "Alice writes again!"
      },
      {
        "userId": 2,
        "postId": 3,
        "title": "Bob's Post",
        "content": "Bob shares his thoughts."
      },
      {
        "userId": 3,
        "postId": 4,
        "title": "Hello World",
        "content": "Charlie joins the conversation."
      }
    ]
  }
}
```

### Step 14. Create a relationship

```yaml title="Open the Posts.hml file and add the following to the end:"
---
kind: Relationship
version: v1
definition:
  name: user
  sourceType: Posts
  target:
    model:
      name: Users
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: userId
      target:
        modelField:
          - fieldName: userId
```

:::tip LSP-Assisted authoring is available

We've created an extension for VS Code that leverages LSP to make authoring these metadata objects easier. Check it out
[here](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura).

:::

### Step 15. Rebuild your project

Bring down the services by pressing `CTRL+C` in the terminal tab logging their activity.

```sh title="As your metadata has changed, create a new build:"
ddn supergraph build local
```

```sh title="Bring everything back up:"
ddn run docker-start
```

### Step 16. Query using your relationship

```graphql title="Now, execute a nested query using your relationship:"
query GetPosts {
  posts {
    postId
    title
    content
    user {
      userId
      name
      age
    }
  }
}
```

```json title="Which should return a result like this:"
{
  "data": {
    "posts": [
      {
        "postId": 1,
        "title": "My First Post",
        "content": "This is Alice's first post.",
        "user": {
          "userId": 1,
          "name": "Alice",
          "age": 25
        }
      },
      {
        "postId": 2,
        "title": "Another Post",
        "content": "Alice writes again!",
        "user": {
          "userId": 1,
          "name": "Alice",
          "age": 25
        }
      },
      {
        "postId": 3,
        "title": "Bob's Post",
        "content": "Bob shares his thoughts.",
        "user": {
          "userId": 2,
          "name": "Bob",
          "age": 30
        }
      },
      {
        "postId": 4,
        "title": "Hello World",
        "content": "Charlie joins the conversation.",
        "user": {
          "userId": 3,
          "name": "Charlie",
          "age": 35
        }
      }
    ]
  }
}
```

## Next steps

Congratulations on completing your first Hasura DDN project with MongoDB! ðŸŽ‰

Here's what you just accomplished:

- You started with a fresh project and connected it to a local MongoDB database.
- You set up metadata to represent your collections and relationships, which acts as the blueprint for your API.
- Then, you created a build â€” essentially compiling everything into a ready-to-use API â€” and successfully ran your first
  GraphQL queries to fetch data.
- Along the way, you learned how to iterate on your schema and refresh your metadata to reflect changes.

Now, you're equipped to connect and expose your data, empowering you to iterate and scale with confidence. Great work!

Take a look at our MongoDB docs to learn more about how to use Hasura DDN with MongoDB. Or, if you're ready, get started
with adding [permissions](/reference/metadata-reference/permissions.mdx) to control access to your API.



--- File: ../ddn-docs/docs/how-to-build-with-ddn/with-clickhouse.mdx ---
# With ClickHouse

---
sidebar_position: 5
sidebar_label: With ClickHouse
description: "Learn the basics of Hasura DDN and how to get started with a ClickHouse instance."
keywords:
  - hasura ddn
  - graphql api
  - getting started
  - guide
  - clickhouse
---

import Prereqs from "@site/docs/_prereqs.mdx";

# Get Started with Hasura DDN and ClickHouse

## Overview

This tutorial takes about fifteen minutes to complete. You'll learn how to:

- Set up a new Hasura DDN project
- Connect it to a ClickHouse instance
- Generate Hasura metadata
- Create a build
- Run your first query

Additionally, we'll familiarize you with the steps and workflows necessary to iterate on your API.

This tutorial assumes you're starting from scratch. We'll use a locally running ClickHouse instance via Docker and
connect it to Hasura, but you can easily follow the steps if you already have data seeded or are using ClickHouse's
hosted service; Hasura will never modify your source schema.

<Prereqs />

## Tutorial

### Step 1. Authenticate your CLI

```sh title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your ClickHouse connector

```sh title="In your project directory, run:"
ddn connector init my_ch -i
```

From the dropdown, start typing `clickhouse` and hit enter to accept the default port. Then, provide the following
values:

**Connection string**

```plaintext
http://local.hasura.dev:8123
```

**Username**

```plaintext
default_user
```

**Password**

```plaintext
default_password
```

### Step 4. Start the local ClickHouse container

```sh title="Begin by creating a compose file for the ClickHouse service:"
touch app/connector/my_ch/compose.clickhouse.yaml
```

```yaml title="Then, open the file and add the following:"
services:
  clickhouse:
    image: clickhouse/clickhouse-server
    container_name: clickhouse-server
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./clickhouse-data:/var/lib/clickhouse
    environment:
      CLICKHOUSE_USER: "default_user"
      CLICKHOUSE_PASSWORD: "default_password"
      CLICKHOUSE_DB: "default"
```

```sh title="Run the container:"
docker compose -f app/connector/my_ch/compose.clickhouse.yaml up -d
```

```sh title="Send a curl request to create the table in the database:"
curl -u default_user:default_password -X POST \
    --data "CREATE TABLE users (user_id UInt32, name String, age UInt8) ENGINE = MergeTree() ORDER BY user_id;" \
    http://localhost:8123
```

```sh title="Then, seed the table:"
curl -u default_user:default_password -X POST \
    --data "INSERT INTO users (user_id, name, age) VALUES (1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 35);" \
    http://localhost:8123
```

```sh title="You can verify this by running:"
curl -u default_user:default_password -X POST \
    --data "SELECT * FROM users;" \
    http://localhost:8123
```

You should see a list of users returned.

### Step 5. Introspect your ClickHouse database

```sh title="Next, use the CLI to introspect your ClickHouse database:"
ddn connector introspect my_ch
```

After running this, you should see a representation of your database's schema in the
`app/connector/my_ch/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Additionally, you can check which resources are available â€”Â and their status â€” at any point using the CLI:"
ddn connector show-resources my_ch
```

### Step 6. Add your model

```sh title="Now, track the table from your ClickHouse database as a model in your DDN metadata:"
ddn models add my_ch users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from ClickHouse in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 7. Create a new build

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```sh title="Start your local Hasura DDN Engine and ClickHouse connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Run your first query

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query {
  users {
    userId
    name
    age
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "users": [
      {
        "userId": 1,
        "name": "Alice",
        "age": 25
      },
      {
        "userId": 2,
        "name": "Bob",
        "age": 30
      },
      {
        "userId": 3,
        "name": "Charlie",
        "age": 35
      }
    ]
  }
}
```

### Step 10. Iterate on your ClickHouse schema

```sh title="Let's add a new table for posts:"
curl -u default_user:default_password -X POST \
    --data "CREATE TABLE posts (
        user_id UInt32,
        post_id UInt32,
        title String,
        content String
    ) ENGINE = MergeTree()
    ORDER BY user_id;" \
    http://localhost:8123
```

```sh title="Then, seed it:"
curl -u default_user:default_password -X POST \
    --data "INSERT INTO posts (user_id, post_id, title, content) VALUES
    (1, 1, 'My First Post', 'This is Alice''s first post.'),
    (1, 2, 'Another Post', 'Alice writes again!'),
    (2, 3, 'Bob''s Post', 'Bob shares his thoughts.'),
    (3, 4, 'Hello World', 'Charlie joins the conversation.');" \
    http://localhost:8123
```

```sh title="Finally, we can check the posts were generated:"
curl -u default_user:default_password -X POST \
    --data "SELECT * FROM posts;" \
    http://localhost:8123
```

### Step 11. Refresh your metadata and rebuild your project

:::tip

The following steps are necessary each time you make changes to your **source** schema. This includes, adding,
modifying, or dropping tables.

:::

#### Step 11.1. Re-introspect your data source

```sh title="Run the introspection command again:"
ddn connector introspect my_ch
```

In `app/connector/my_ch/configuration.json`, you'll see schema updated to include operations for the `posts` table. In
`app/metadata/my_ch.hml`, you'll see `posts` present in the metadata as well.

#### Step 11.2. Update your metadata

```sh title="Add the posts model:"
ddn model add my_ch posts
```

#### Step 11.3. Kill your services

Bring down the services by pressing `CTRL+C` in the terminal tab logging their activity.

#### Step 11.4. Create a new build

```sh title="Next, create a new build:"
ddn supergraph build local
```

#### Step 11.5 Restart your services

```sh title="Bring everything back up:"
ddn run docker-start
```

### Step 12. Query your new build

```graphql title="Head back to your console and query the posts model:"
query GetPosts {
  posts {
    userId
    postId
    title
    content
  }
}
```

```json title="You'll get a response like this:"
{
  "data": {
    "posts": [
      {
        "userId": 1,
        "postId": 1,
        "title": "My First Post",
        "content": "This is Alice's first post."
      },
      {
        "userId": 1,
        "postId": 2,
        "title": "Another Post",
        "content": "Alice writes again!"
      },
      {
        "userId": 2,
        "postId": 3,
        "title": "Bob's Post",
        "content": "Bob shares his thoughts."
      },
      {
        "userId": 3,
        "postId": 4,
        "title": "Hello World",
        "content": "Charlie joins the conversation."
      }
    ]
  }
}
```

### Step 13. Create a relationship

```yaml title="Open the Posts.hml file and add the following to the end:"
---
kind: Relationship
version: v1
definition:
  name: user
  sourceType: Posts
  target:
    model:
      name: Users
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: userId
      target:
        modelField:
          - fieldName: userId
```

:::tip LSP-Assisted authoring is available

We've created an extension for VS Code that leverages LSP to make authoring these metadata objects easier. Check it out
[here](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura).

:::

### Step 14. Rebuild your project

Bring down the services by pressing `CTRL+C` in the terminal tab logging their activity.

```sh title="As your metadata has changed, create a new build:"
ddn supergraph build local
```

```sh title="Bring everything back up:"
ddn run docker-start
```

### Step 15. Query using your relationship

```graphql title="Now, execute a nested query using your relationship:"
query GetPosts {
  posts {
    postId
    title
    content
    user {
      userId
      name
      age
    }
  }
}
```

```json title="Which should return a result like this:"
{
  "data": {
    "posts": [
      {
        "postId": 1,
        "title": "My First Post",
        "content": "This is Alice's first post.",
        "user": {
          "userId": 1,
          "name": "Alice",
          "age": 25
        }
      },
      {
        "postId": 2,
        "title": "Another Post",
        "content": "Alice writes again!",
        "user": {
          "userId": 1,
          "name": "Alice",
          "age": 25
        }
      },
      {
        "postId": 3,
        "title": "Bob's Post",
        "content": "Bob shares his thoughts.",
        "user": {
          "userId": 2,
          "name": "Bob",
          "age": 30
        }
      },
      {
        "postId": 4,
        "title": "Hello World",
        "content": "Charlie joins the conversation.",
        "user": {
          "userId": 3,
          "name": "Charlie",
          "age": 35
        }
      }
    ]
  }
}
```

## Next steps

Congratulations on completing your first Hasura DDN project with ClickHouse! ðŸŽ‰

Here's what you just accomplished:

- You started with a fresh project and connected it to a local ClickHouse database.
- You set up metadata to represent your tables and relationships, which acts as the blueprint for your API.
- Then, you created a build â€” essentially compiling everything into a ready-to-use API â€” and successfully ran your first
  GraphQL queries to fetch data.
- Along the way, you learned how to iterate on your schema and refresh your metadata to reflect changes.

Now, you're equipped to connect and expose your data, empowering you to iterate and scale with confidence. Great work!

Take a look at our ClickHouse docs to learn more about how to use Hasura DDN with ClickHouse. Or, if you're ready, get
started with adding [permissions](/reference/metadata-reference/permissions.mdx) to control access to your API.



--- File: ../ddn-docs/docs/how-to-build-with-ddn/with-elasticsearch.mdx ---
# With Elasticsearch

---
sidebar_position: 6
sidebar_label: With Elasticsearch
description: "Learn the basics of Hasura DDN and how to get started with an Elasticsearch instance."
keywords:
  - hasura ddn
  - graphql api
  - getting started
  - guide
  - elasticsearch
---

import Prereqs from "@site/docs/_prereqs.mdx";

# Get Started with Hasura DDN and Elasticsearch

## Overview

This tutorial takes about fifteen minutes to complete. You'll learn how to:

- Set up a new Hasura DDN project
- Connect it to an Elasticsearch instance
- Generate Hasura metadata
- Create a build
- Run your first query

Additionally, we'll familiarize you with the steps and workflows necessary to iterate on your API.

This tutorial assumes you're starting from scratch. We'll use a locally running Elasticsearch instance via Docker and
connect it to Hasura, but you can easily follow the steps if you already have data seeded or are using Elasticsearch's
hosted service; Hasura will never modify your source schema.

<Prereqs />

## Tutorial

### Step 1. Authenticate your CLI

```sh title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your Elasticsearch connector

```sh title="In your project directory, run:"
ddn connector init my_es -i
```

From the dropdown, start typing `elasticsearch` and hit enter to accept the default port. Then, provide the following
values:

**ELASTICSEARCH_URL**

```plaintext
http://local.hasura.dev:9200/
```

**ELASTICSEARCH_USERNAME**

```plaintext
elastic
```

**ELASTICSEARCH_PASSWORD**

```plaintext
elastic
```

For now, you can leave the rest of the values as blanks (or their defaults, if present).

### Step 4. Start the local Elasticsearch container

```sh title="Begin by starting a local Elasticsearch container on port 9200:"
docker run --rm -p 127.0.0.1:9200:9200 -d --name elasticsearch \
  -e ELASTIC_PASSWORD="elastic" \
  -e "discovery.type=single-node" \
  -e "xpack.security.http.ssl.enabled=false" \
  -e "xpack.license.self_generated.type=trial" \
  docker.elastic.co/elasticsearch/elasticsearch:8.17.1
```

```sh title="Then, add a mapping to the database:"
curl -X PUT "http://localhost:9200/customers/" -u elastic:elastic -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "customer_id": {
        "type": "keyword"
      },
      "name": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword"
          }
        }
      },
      "email": {
        "type": "keyword",
        "index": true
      },
      "location": {
        "type": "geo_point"
      }
    }
  }
}
'
```

```sh title="Finally, add some data to the database:"
curl -X POST "http://localhost:9200/_bulk" -u elastic:elastic -H 'Content-Type: application/json' -d'
{ "index": { "_index": "customers", "_id": "1" } }
{ "customer_id": "CUST001", "name": "John Doe", "email": "john.doe@example.com", "location": { "lat": 40.7128, "lon": -74.0060 } }
{ "index": { "_index": "customers", "_id": "2" } }
{ "customer_id": "CUST002", "name": "Jane Smith", "email": "jane.smith@example.com", "location": { "lat": 34.0522, "lon": -118.2437 } }
{ "index": { "_index": "customers", "_id": "3" } }
{ "customer_id": "CUST003", "name": "Alice Johnson", "email": "alice.j@example.com", "location": { "lat": 51.5074, "lon": -0.1278 } }
{ "index": { "_index": "customers", "_id": "4" } }
{ "customer_id": "CUST004", "name": "Bob Brown", "email": "bob.brown@example.com", "location": { "lat": 48.8566, "lon": 2.3522 } }
{ "index": { "_index": "customers", "_id": "5" } }
{ "customer_id": "CUST005", "name": "Charlie Davis", "email": "charlie.d@example.com", "location": { "lat": 35.6895, "lon": 139.6917 } }
'
```

```sh title="You can verify the data by running:"
curl --location 'http://localhost:9200/customers/_search' \
-H 'Content-Type: application/json' \
-u elastic:elastic \
--data '{
  "_source": [
    "_id",
    "name"
  ]
}'
```

### Step 5. Introspect your Elasticsearch database

```sh title="Next, use the CLI to introspect your Elasticsearch database:"
ddn connector introspect my_es
```

After running this, you should see a representation of your database's schema in the
`app/connector/my_es/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Additionally, you can check which resources are available â€”Â and their status â€” at any point using the CLI:"
ddn connector show-resources my_es
```

### Step 6. Add your model

```sh title="Now, track the index from your Elasticsearch database as a model in your DDN metadata:"
ddn models add my_es customers
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Customers.hml`. The DDN CLI will use this
Hasura Metadata Language file to represent the `customers` index from Elasticsearch in your API as a
[model](/reference/metadata-reference/models.mdx).

:::tip

You can import all the indexes from your Elasticsearch database as models by running `ddn model add my_es "*"`.

:::

### Step 7. Create a new build

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```sh title="Start your local Hasura DDN Engine and Elasticsearch connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Run your first query

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query MyQuery {
  customers {
    email
    name
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "customers": [
      {
        "email": "john.doe@example.com",
        "name": "John Doe"
      },
      {
        "email": "jane.smith@example.com",
        "name": "Jane Smith"
      },
      {
        "email": "alice.j@example.com",
        "name": "Alice Johnson"
      },
      {
        "email": "bob.brown@example.com",
        "name": "Bob Brown"
      },
      {
        "email": "charlie.d@example.com",
        "name": "Charlie Davis"
      }
    ]
  }
}
```

### Step 10. Iterate on your Elasticsearch schema

```sh title="Let's add a new index for transactions:"
curl -X PUT "http://localhost:9200/transactions/" -u elastic:elastic -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "transaction_id": {
        "type": "keyword"
      },
      "timestamp": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      },
      "customer_id": {
        "type": "keyword"
      },
      "transaction_details": {
        "properties": {
          "item_id": {
            "type": "keyword"
          },
          "item_name": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "price": {
            "type": "float"
          },
          "quantity": {
            "type": "integer"
          },
          "currency": {
            "type": "keyword"
          }
        }
      }
    }
  }
}
'
```

```sh title="Add some data to the new index:"
curl -X POST "http://localhost:9200/_bulk" -u elastic:elastic -H 'Content-Type: application/json' -d'
{ "index": { "_index": "transactions", "_id": "1" } }
{ "transaction_id": "TXN001", "timestamp": "2024-09-01T12:00:00", "customer_id": "CUST001", "transaction_details": { "item_id": "ITEM001", "item_name": "Laptop", "price": 999.99, "quantity": 1, "currency": "USD" } }
{ "index": { "_index": "transactions", "_id": "2" } }
{ "transaction_id": "TXN002", "timestamp": "2024-09-02T14:30:00", "customer_id": "CUST002", "transaction_details": { "item_id": "ITEM002", "item_name": "Smartphone", "price": 599.99, "quantity": 2, "currency": "USD" } }
{ "index": { "_index": "transactions", "_id": "3" } }
{ "transaction_id": "TXN003", "timestamp": "2024-09-03T09:45:00", "customer_id": "CUST003", "transaction_details": { "item_id": "ITEM003", "item_name": "Tablet", "price": 299.99, "quantity": 1, "currency": "USD" } }
{ "index": { "_index": "transactions", "_id": "4" } }
{ "transaction_id": "TXN004", "timestamp": "2024-09-04T16:15:00", "customer_id": "CUST004", "transaction_details": { "item_id": "ITEM004", "item_name": "Headphones", "price": 199.99, "quantity": 1, "currency": "USD" } }
{ "index": { "_index": "transactions", "_id": "5" } }
{ "transaction_id": "TXN005", "timestamp": "2024-09-05T11:30:00", "customer_id": "CUST005", "transaction_details": { "item_id": "ITEM005", "item_name": "Monitor", "price": 149.99, "quantity": 2, "currency": "USD" } }
'
```

### Step 11. Refresh your metadata and rebuild your project

:::tip

The following steps are necessary each time you make changes to your **source** schema. This includes adding,
modifying, or dropping tables.

:::

#### Step 11.1. Kill your services

Bring down the services by pressing `CTRL+C` in the terminal tab logging their activity.

#### Step 11.2. Re-introspect your data source

```sh title="Run the introspection command again:"
ddn connector introspect my_es
```

In `app/connector/my_es/configuration.json`, you'll see schema updated to include the `transactions` index. In
`app/metadata/my_es.hml`, you'll see `transactions` present in the metadata as well.

#### Step 11.3. Update your metadata

```sh title="Add the transactions index as a model:"
ddn model add my_es transactions
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Transactions.hml`. The DDN CLI will use this
Hasura Metadata Language file to represent the `transactions` index from Elasticsearch in your API as a
[model](/reference/metadata-reference/models.mdx).

:::tip

You can import all the indexes from your Elasticsearch database as models by running `ddn model add my_es "*"`.

:::

#### Step 11.4. Create a new build

```sh title="Next, create a new build:"
ddn supergraph build local
```

#### Step 11.5 Restart your services

```sh title="Bring everything back up:"
ddn run docker-start
```

### Step 12. Query your new build

```graphql title="Head back to your console and query the transactions model:"
query MyQuery {
  transactions {
    customerId
    transactionId
    transactionDetails {
      currency
      price
    }
  }
}
```

```json title="You'll get a response like this:"
{
  "data": {
    "transactions": [
      {
        "customerId": "CUST001",
        "transactionId": "TXN001",
        "transactionDetails": [
          {
            "currency": "USD",
            "price": 999.99
          }
        ]
      },
      {
        "customerId": "CUST002",
        "transactionId": "TXN002",
        "transactionDetails": [
          {
            "currency": "USD",
            "price": 599.99
          }
        ]
      },
      {
        "customerId": "CUST003",
        "transactionId": "TXN003",
        "transactionDetails": [
          {
            "currency": "USD",
            "price": 299.99
          }
        ]
      },
      {
        "customerId": "CUST004",
        "transactionId": "TXN004",
        "transactionDetails": [
          {
            "currency": "USD",
            "price": 199.99
          }
        ]
      },
      {
        "customerId": "CUST005",
        "transactionId": "TXN005",
        "transactionDetails": [
          {
            "currency": "USD",
            "price": 149.99
          }
        ]
      }
    ]
  }
}
```

## Next steps

Congratulations on completing your first Hasura DDN project with Elasticsearch! ðŸŽ‰

Here's what you just accomplished:

- You started with a fresh project and connected it to a local Elasticsearch database.
- You set up metadata to represent your tables and relationships, which acts as the blueprint for your API.
- Then, you created a build â€” essentially compiling everything into a ready-to-use API â€” and successfully ran your first
  GraphQL queries to fetch data.
- Along the way, you learned how to iterate on your schema and refresh your metadata to reflect changes.

Now, you're equipped to connect and expose your data, empowering you to iterate and scale with confidence. Great work!

Take a look at our Elasticsearch docs to learn more about how to use Hasura DDN with Elasticsearch. Or, if you're ready,
get started with adding [permissions](/reference/metadata-reference/permissions.mdx) to control access to your API.



--- File: ../ddn-docs/docs/how-to-build-with-ddn/with-others.mdx ---
# With other sources

---
sidebar_position: 7
sidebar_label: With other sources
description: "Learn the basics of Hasura DDN and how to get started with other sources."
keywords:
  - hasura ddn
  - graphql api
  - getting started
  - guide
---

import Prereqs from "@site/docs/_prereqs.mdx";

# Get Started with Hasura DDN and other Sources

On the [Connector Hub](https://hasura.io/connectors), you can find information about our wealth of data connectors.
These data connectors allow you to connect nearly any data source â€” be them relational, vector, custom business logic,
or even a third-party API â€” to Hasura DDN and build an API on top of it.

We recommend using the [Quickstart](/quickstart.mdx) and referencing the individual connector's docs from â˜ï¸ to get
started with a connector-specific setup.



--- File: ../ddn-docs/docs/data-sources/overview.mdx ---
# Basics

---
sidebar_position: 1
sidebar_label: Basics
description:
  "Hasura DDN offers a variety of native data connectors that allow you to bring nearly any form of data into a single
  API."
keywords:
  - hasura ddn
  - data sources
  - data connector
---

import Gallery from "@site/src/components/ConnectorGallery";

# Basics

## What is a data source?

When we talk about data, many people first think of structured tables in a database. While thatâ€™s a significant part of
the picture, data comes in many formsâ€”some structured, others not. A data source, in the context of Hasura DDN, can be
as straightforward as a relational database or as dynamic as the output of a custom function.

With Hasura DDN, you can connect it all and serve it from a single endpoint.

## How do I connect my data to Hasura DDN?

We use **native data connectors** to make your data accessible through a GraphQL API. These connectors are tailored for
specific sources, including relational databases, NoSQL, external GraphQL APIs, REST APIs, and more.

We have the following data connectors available today:

<Gallery />

## Building your own data connector

You are also able to build your own data connector. This is useful if you have a custom data source that is not
supported. You can find information for this in the [NDC Spec](https://github.com/hasura/ndc-spec) and the rendered
[NDC Spec data connector tutorial](https://hasura.github.io/ndc-spec/tutorial/index.html) as well as the SDKs for
various languages:

- [Rust](https://github.com/hasura/ndc-sdk-rs)
- [TypeScript](https://github.com/hasura/ndc-sdk-typescript)
- [Python](https://github.com/hasura/ndc-sdk-python)
- [Go](https://github.com/hasura/ndc-sdk-go)

## Get started

- Learn how to [connect your data](/data-sources/connect-to-a-source.mdx)



--- File: ../ddn-docs/docs/data-sources/connect-to-a-source.mdx ---
# Connect to a Source

---
sidebar_position: 2
sidebar_label: Connect to a Source
description: "Learn how to connect any data source to Hasrua DDN."
keywords:
  - hasura ddn
  - data sources
  - connect
  - data connector
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Connect to a source

## Introduction

This guide explains how to initialize a connector, configure its environment variables, and link it to your data source.
Once initialized, you'll be ready to introspect the source and integrate it into your API.

You'll need a [project](/reference/cli/commands/ddn_supergraph_init.mdx) before initializing a connector.

## Step 1. Initialize a connector

Regardless which connector you're using, you'll always begin by initializing it with a unique name:

```sh title="Initialize a new connector in a project directory:"
ddn connector init <your_name_for_the_connector> -i
```

A wizard will appear with a dropdown list. If you know the name of the connector, start typing the name. Otherwise, use
the arrows to scroll through the list. Hit `ENTER` when you've selected your desired connector; you'll then be prompted
to enter some values.

:::info Customization

You can customize which subgraph this connector is added to by
[changing your project's context](/reference/cli/commands/ddn_context.mdx) or using flags. More information can be found
in the [CLI docs](/reference/cli/commands/ddn_connector_init.mdx) for the `ddn connector init` command.

:::

## Step 2. Add environment variables

The CLI will assign a random port for the connector to use during local development. You can hit `ENTER` to accept the
suggested value or enter your own. Then, depending on your connector, there may be a set of environment variables that
it requires:

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="PostgreSQL" label="PostgreSQL">

| ENV              | Example                                       | Description                                                            |
| ---------------- | --------------------------------------------- | ---------------------------------------------------------------------- |
| `CONNECTION_URI` | `postgresql://user:password@host:5432/dbname` | The full connection string used to connect to the PostgreSQL database. |

</TabItem>
<TabItem value="MongoDB" label="MongoDB">

| ENV            | Example                        | Description                                                                                             |
| -------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| `MONGO_DB_URL` | `mongodb://host:27017/db_name` | The full connection string â€”Â **including the database name** â€” used to connect to the MongoDB instance. |

</TabItem>
<TabItem value="ClickHouse" label="ClickHouse">

| ENV                 | Example             | Description                                               |
| ------------------- | ------------------- | --------------------------------------------------------- |
| `CONNECTION_STRING` | `https://host:8123` | The HTTP(S) connection string to the ClickHouse instance. |
| `USERNAME`          | `default`           | The database username.                                    |
| `PASSWORD`          | `default`           | The database password.                                    |

</TabItem>
<TabItem value="Elasticsearch" label="Elasticsearch">

| ENV                           | Example                                                        | Description                                                                                                                                                                |
| ----------------------------- | -------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ELASTICSEARCH_URL`           | `https://example.es.gcp.cloud.es.io:9200`                      | The comma-separated list of Elasticsearch host addresses for connection (Use `local.hasura.dev` instead of `localhost` if your connector is running on your local machine) |
| `ELASTICSEARCH_USERNAME`      | `default`                                                      | The username for authenticating to the Elasticsearch cluster                                                                                                               |
| `ELASTICSEARCH_PASSWORD`      | `default`                                                      | The password for the Elasticsearch user account                                                                                                                            |
| `ELASTICSEARCH_API_KEY`       | `ABCzYWk0NEI0aDRxxxxxxxxxx1k6LWVQa2gxMUpRTUstbjNwTFIzbGoyUQ==` | The Elasticsearch API key for authenticating to the Elasticsearch cluster                                                                                                  |
| `ELASTICSEARCH_CA_CERT_PATH`  | `/etc/connector/cacert.pem`                                    | The path to the Certificate Authority (CA) certificate for verifying the Elasticsearch server's SSL certificate                                                            |
| `ELASTICSEARCH_INDEX_PATTERN` | `hasura*`                                                      | The pattern for matching Elasticsearch indices, potentially including wildcards, used by the connector                                                                     |

</TabItem>

</Tabs>

If your data source requires a connection string or endpoint, the CLI will confirm that it successfully tested the
connection to your source. Additionally, it generates configuration files, which you can find in the `connector`
directory of the subgraph where you added the connector (default: `app`). Finally, the CLI will create a
[DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) in your connector's `metadata` directory.

## Next steps

Now that you've initialized a connector and connected it to your data, you're ready to introspect the source and
populate the configuration files with source-specific information that Hasura will need to build your API. Check out the
[introspection page](/data-sources/introspect-a-source.mdx) to learn more.



--- File: ../ddn-docs/docs/data-sources/introspect-a-source.mdx ---
# Introspect a Source

---
sidebar_position: 3
sidebar_label: Introspect a Source
description: "Learn how to introspect a data source and generate configuration files for your data connector."
keywords:
  - hasura ddn
  - data sources
  - introspect
  - configuration
  - data connector
---

# Introspect a source

## Introduction

This guide explains how to use a connector to introspect a data source. After introspection, you'll be ready to begin
generating Hasura metadata that represents resources (such as tables or collections) in your data source.

You'll need an [initialized data connector](/data-sources/connect-to-a-source.mdx) before attempting to introspect a
source.

## Step 1. Run the introspect command

```sh title="All connectors use the same command to introspect a data source:"
ddn connector introspect <name_of_connector>
```

:::tip Is your Docker daemon running?

When you run the introspection command, the connector will start. As it runs as a Docker service, the Docker daemon must
be running.

:::

The connector will reach out to your data source and update the configuration files with information about your source
schema. This will vary depending on the connector but, at a minimum, you'll see source-specific information updated in
two files, both located in your connector's directory:

| File                      | Description                                                                                                                                                                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `configuration.json`      | An [Open Data Domain Specification](https://github.com/hasura/open-data-domain-specification) description of your data source. The CLI uses this file to update the DataConnectorLink object with information about your data source. |
| `<name_of_connector>.hml` | Contains the [DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) for your connector.                                                                                                                          |

:::info When to use this command

This command will be used at least once with every connector; Hasura **must** introspect your data source in order to
understand the resources that are present.

**Additionally, any time you modify your source schema, you should re-run this command to ensure your Hasura metadata is
up-to-date with your source schema.**

:::

## Next steps

With your source introspected and the configuration files populated, you can now start creating Hasura metadata that
will represent your source's schema in your API. Check out the [Data Modeling](/data-modeling/overview.mdx) section to
begin creating models, commands, and relationships for your data.



--- File: ../ddn-docs/docs/data-sources/troubleshooting.mdx ---
# Troubleshooting

---
sidebar_position: 4
sidebar_label: Troubleshooting
description: "Learn how to troubleshoot common issues when connecting to data sources with Hasura DDN."
keywords:
  - hasura ddn
  - data sources
  - data connectors
  - troubleshoot
  - help
---

# Troubleshooting

## Common issues

### My connector can't connect to the data source

When initializing a new connector, we verify the connection string you provide to ensure your data source is reachable.
If this process fails, try the following solutions:

#### Your source isn't accessible to the public internet

If your data source is hosted behind a firewall or restricts access to specific IP addresses, it may block external
connections. Unless you're using [Private DDN](https://hasura.io/pricing), you need to allow access from all IP
addresses. You can do this by adding the following to your allowlist:

```plaintext
0.0.0.0/0
```

#### Docker networking issues

Hasura DDN resolves `local.hasura.dev` to your machine's `localhost`. This avoids Docker networking conflicts, where
`localhost` might point to your source's container instead of your local machine. To fix this, update any `localhost`
references in your connection strings to `local.hasura.dev`.

### My connector won't introspect my data source

If you encounter an error like the following:

```plaintext
ERR Failed building the container: exit status 17
```

You'll likely see the following in the logs:

```plaintext
Failed to resolve source metadata for ghcr.io/hasura/<connector>: failed to authorize: failed to fetch oauth token: unexpected status from GET request to https://ghcr.io/token?scope=repository%3Ahasura%2F<connector>%3Apull&service=ghcr.io: 403 Forbidden
```

When this happens, your GitHub access token located in your Docker credentials store is likely out-of-date.

To resolve this, generate a new PAT with `package` scope and run the following:

```sh
echo "<GH_PAT>" | docker login ghcr.io -u <YOUR_GITHUB_USERNAME> --password-stdin
```

Your terminal should return `Login Succeeded`.

### There's a port conflict for my connector

Use `docker ps` to check for any running processes that would conflict with your source's container. If you find one,
kill it and then try initializing the connector again.

### My connector isn't reflecting schema changes in my data source

You may have not re-run the introspection steps. Remember, each time your data source's schema changes, you'll need to
re-run this command to make Hasura DDN aware of the new schema. Follow the guide
[here](/data-sources/introspect-a-source.mdx).

### My connector isn't reflecting schema changes in my API

If you've modified your metadata and are getting errors when trying to use your API, you may need to force a rebuild of
the connector.

Identify the connector's container and image; kill the container and delete the image. Then, re-run the
`ddn run docker-start` command to rebuild the connector's image in addition to your other services.

### My data source operations time out

If a data source operation like `ddn introspect` hangs, eventually times out, and you encounter a log entry like the
following:

```plaintext
INF Waiting for the Connector service to be healthy...
```

Follow these steps to resolve the issue:

#### Step 1: Enable Debug Logging

Add the `--log-level DEBUG` flag to your command to generate more detailed logs. If the debug logs include the following
entry:

```plaintext
dial tcp: lookup local.hasura.dev: no such host
```

Then the issue is related to DNS resolution.

#### Step 2: Diagnose the DNS Issue

The most common cause of this DNS error is that your DHCP server is setting the Domain Search option. This is specified
in [DHCP Option 119/RFC 3397](https://www.rfc-editor.org/rfc/rfc3397).

#### Step 3: Resolve the DNS Issue

To resolve this issue, you have two options:

- **Disable the Domain Search Option**

  Disabling this option in your DHCP server settings will prevent the DNS error. Consult your network administrator if
  you are unsure how to make this change.

- **Add a Static Hostname Entry**

  If disabling the Domain Search option is not feasible, you can manually add a static entry to your hosts file. This
  approach will override DNS lookup for the hostname `local.hasura.dev`.

  **Windows:** Follow the instructions
  [here](https://www.howtogeek.com/27350/beginner-geek-how-to-edit-your-hosts-file/) to edit your hosts file.

  **Linux/Unix/MacOS:** Add the following line to your `/etc/hosts` file:

  ```plaintext
  127.0.0.1 local.hasura.dev
  ```

  This entry maps the hostname `local.hasura.dev` to the IP address `127.0.0.1` and does not interfere with other
  applications.

By following these steps, you should be able to resolve the introspection hang or timeout issue with your data source.

### I have a lot of tables

Typically, you won't experience any issues when connecting to or introspecting a database with a large number of tables.
However, _builds_ of your supergraph may exceed timeouts given the size of your project.

```sh title="If possible, consider removing unused models via the CLI:"
ddn model remove <table>
```

```sh title="Then, retry your supergraph build:"
ddn supergraph build <local|create>
```

:::info I need all of my tables

If all tracked tables are needed, and you're running into build issues,
[raise a ticket or drop us a line on Discord](/help/overview.mdx).

:::

## Get help

### Discord

We're available on Discord! Check out
[the `#v3-help-forum`](https://discord.com/channels/407792526867693568/1205357708677480468) to post your question and
get help from the community and Hasura team members!

### GitHub

Each connector has a public repository on GitHub. Typically, they'll follow the naming convention of
`hasura/ndc-<connector-name>`. Create issues on these repositories to get help directly from the teams responsible for
the connectors.

You can search the list of public repositories [here](https://github.com/orgs/hasura/repositories).



--- File: ../ddn-docs/docs/data-sources/publish-your-own-connector.mdx ---
# Publish a Data Connector

---
sidebar_position: 5
sidebar_label: Publish a Data Connector
description:
  "Learn how to publish your own data connector to the Hasura Connector Hub, empowering the entire community to use it!"
keywords:
  - hasura ddn
  - data sources
  - connector
  - publish
---

# Publish a Data Connector

## Introduction

This guide is for **connector authors** and details the automated process for publishing new connectors and updating
existing versions on the [Hasura Connector Hub](https://hasura.io/connectors), which is powered by
[the `ndc-hub` repository](https://github.com/hasura/ndc-hub).

The automated publication process simplifies and accelerates the deployment of NDC connectors by:

- Managing documentation updates
- Streamlining the approval workflow
- Uploading connector packages to Hasura DDN
- Updating the Hasura Hub Registry database so connectors can be deployed on DDN
- Making a new connector/connector version available in the DDN ecosystem

:::info How do I build a connector?

You can find information about building a connector in the [NDC Spec](https://github.com/hasura/ndc-spec) and the
rendered [NDC Spec data connector tutorial](https://hasura.github.io/ndc-spec/tutorial/index.html) as well as the SDKs
for various languages:

- [Rust](https://github.com/hasura/ndc-sdk-rs)
- [TypeScript](https://github.com/hasura/ndc-sdk-typescript)
- [Python](https://github.com/hasura/ndc-sdk-python)
- [Go](https://github.com/hasura/ndc-sdk-go)

:::

## Publish a new connector

### Step 1. Clone the repository

```sh title="Clone the ndc-hub repository:"
git clone https://github.com/hasura/ndc-hub.git
```

### Step 2. Create a new directory for your connector in the registry {#step-2}

Create a new folder structure in [the `registry` directory](https://github.com/hasura/ndc-hub/tree/main/registry) of the
`ndc-hub` repo:

```plaintext
registry/
  â””â”€â”€ [namespace]/
      â””â”€â”€ [connector-name]/
```

Replace `[namespace]` with your organization's namespace and `[connector-name]` with the name of your connector.

### Step 3. Create a `connector-packaging.json` {#step-3}

Within your newly created connector's directory in the registry, create a new `releases` directory with a folder for
your first release and a `connector-packaging.json` for it:

```plaintext {4-6}
registry/
  â””â”€â”€ [namespace]/
      â””â”€â”€ [connector-name]/
          â””â”€â”€ releases/
              â””â”€â”€ v1.0.0/
                  â””â”€â”€ connector-packaging.json
```

#### Format

Below is the required shape for the `connector-packaging.json` file for each connector version:

```json
{
  "version": "1.0.0",
  "uri": "https://github.com/hasura/ndc-mongodb/releases/download/v0.0.1/connector-definition.tgz",
  "checksum": {
    "type": "sha256",
    "value": "2cd3584557be7e2870f3488a30cac6219924b3f7accd9f5f473285323843a0f4"
  },
  "source": {
    "hash": "c32adbde478147518f65ff465c40a0703239288a"
  }
}
```

#### Required fields:

- **version**: The version of the connector (e.g., "1.0.0").
- **uri**: The URL to download the connector package. This should be a tarball containing the connector package
  definition and must be accessible without authentication.
- **checksum**: The checksum of the connector package.
  - **type**: The algorithm used for the checksum (currently only "sha256" is supported).
  - **value**: The actual checksum value.
- **source**: Information about the source code used to build the package.
  - **hash**: The commit hash of the source code used to build the connector package.

Ensure all fields are correctly filled out for each new connector version you publish.

### Step 4. Add the other required files

Add the following files to your connector's directory in the registry; details are below.

```plaintext {5-7}
registry/
  â””â”€â”€ [namespace]/
      â””â”€â”€ [connector-name]/
          â”œâ”€ releases/
          â”œâ”€ logo.(png|svg)
          â”œâ”€ metadata.json
          â””â”€ README.md
```

- `logo.png` or `logo.svg`: The logo for your connector (preferably in SVG format).
- `README.md`: Documentation for your connector, including description, usage instructions, and any other relevant
  information.
- `metadata.json`: Metadata file containing information about the connector.

The `metadata.json` file in your connector's folder contains crucial information about your connector:

```json
{
  "overview": {
    "namespace": "your_namespace",
    "description": "A brief description of your connector",
    "title": "Your Connector Title",
    "logo": "logo.png",
    "tags": [],
    "latest_version": "v1.0.0"
  },
  "author": {
    "support_email": "support@example.com",
    "homepage": "https://www.example.com",
    "name": "Your Organization Name"
  },
  "is_verified": false,
  "is_hosted_by_hasura": false,
  "source_code": {
    "is_open_source": true,
    "repository": "https://github.com/your-org/your-connector-repo"
  }
}
```

**Field explanations:**

- **overview**: General information about your connector.

  - **namespace**: Your organization's namespace (e.g., `sqlserver` for the SQL Server connector).
  - **description**: A brief description of your connector.
  - **title**: The title of your connector (e.g., BigQuery, SQL Server).
  - **logo**: Filename containing your connector logo (acceptable formats: PNG, SVG).
  - **tags**: Keywords related to your connector.
  - **latest_version**: The most recent version of your connector.

- **author**: Information about the connector's author or organization.
- **is_verified**: Set this to `false` as only connectors developed by Hasura can be tagged as verified.
- **is_hosted_by_hasura**: Set this to `false` as connectors not developed by Hasura cannot be hosted by Hasura.

### Step 5. Submit for review

1. Commit all your changes to a new branch.
2. Create a pull request targeting the `main` branch of the `ndc-hub` repository.
3. In the pull request description provide any additional context about your new connector.
4. Wait for review and approval from the Hasura team.

#### Post-approval process

Once your pull request is approved and merged:

- The new connector will be added to the Hasura Hub Registry.
- It will become available in the staging environment for testing.
- After successful testing, it will be published to the production environment.

## Create a new release for existing connectors

### Step 1. Check repository structure

Ensure your connector and connector versions follow this directory structure in the `ndc-hub` repository:

```
registry/
  â”œâ”€â”€ [namespace]/
  â”‚   â”œâ”€â”€ [connector-name]/
  â”‚   â”‚   â”œâ”€â”€ releases/
  â”‚   â”‚   â”‚   â”œâ”€â”€ [version]/
  â”‚   â”‚   â”‚   â”‚   â””â”€â”€ connector-packaging.json
  â”‚   â”‚   â”œâ”€â”€ logo.(png|svg)
  â”‚   â”‚   â””â”€â”€ README.md
```

### Step 2. Create a new connector version

1. Create a new folder under `registry/[namespace]/[connector-name]/releases/` with the version number.
2. The version must start with the letter 'v', for example: `v1.0.0`.
3. Add a new `connector-packaging.json` file in this folder with the connector metadata.

### Step 3. Update connector information

- To update the logo: Modify the `logo.png` or `logo.svg` file in the connector's root folder.
- To update the documentation: Modify the `README.md` file in the connector's root folder.

### Step 4. Create a PR

1. Commit your changes to a new branch.
2. Create a pull request targeting the `main` branch of the `ndc-hub` repository.
3. Wait for approval from a Hasura team member.

#### Post-approval process

Once the pull request is approved, the GitHub workflow will automatically:

- Run the registry automation program.
- Upload connector packages.
- Update the Hasura Hub Registry database.

## Development workflow

The connector publication automation is designed to run automatically for every commit made to a pull request targeting
the `main` branch. This process ensures that your changes are continuously validated and updated in our staging
environment. Here's how it works:

### PR creation

When you create a pull request against the `main` branch with changes in the registry folder, it triggers the automation
process.

### Commit-based triggers

Every new commit to the pull request will trigger
[the `registry-updates` GitHub Actions workflow](https://github.com/hasura/ndc-hub/blob/main/.github/workflows/registry-updates.yaml).
This includes:

- Initial commits when opening the PR.
- Additional commits pushed to the same PR.
- Commits made in response to review comments.

### Staging environment updates

For each commit:

- The `registry-updates` workflow runs automatically.
- It validates the changes in the registry folder, including the `connector-packaging.json` file.
- If validation is successful, it updates the connector information on the `staging` Hasura DDN environment.
- Each new commit overwrites the previous version of that connector on the `staging` environment.

### Continuous updates

This process allows for continuous iteration and testing in the staging environment:

- You can push multiple commits to refine your connector changes.
- Each commit provides a new opportunity to test and verify your changes in the staging environment.
- Reviewers can check the latest version of your connector in the staging DDN at any time during the review process.

### Production deployment

Once the pull request is approved and merged into the `main` branch:

- The final version of the connector (based on the last commit in the PR) is automatically published to the `production`
  Hasura DDN.
- This process ensures that only thoroughly reviewed and tested connector versions reach the `production` environment.

### Important Notes

- The automation only triggers for changes made in the `registry` folder.
- Ensure your `connector-packaging.json` file is valid for each commit to avoid automation failures.
- Multiple connector versions or updates to multiple connectors can be included in a single PR, and the automation will
  handle all changes appropriately.

## Troubleshooting

### Pull request automation doesn't trigger.

Ensure the PR targets the main branch and modifies files under the `registry/` directory.

### Automation fails due to missing environment variables.

Ping `@scriptnull` or `@codingkarthik` in the PR.



--- File: ../ddn-docs/docs/data-modeling/overview.mdx ---
# Basics

---
sidebar_position: 1
sidebar_label: Basics
description:
  "Data modeling with Hasura DDN is simultaneously simple and powerful. Learn how to model your data with Hasura DDN to
  get the best API performance on top of your data."
keywords:
  - hasura ddn
  - data modeling
hide_table_of_contents: true
---

import { OverviewTopSectionIconNoVideo } from "@site/src/components/OverviewTopSectionIconNoVideo";
import { OverviewPlainCard } from "@site/src/components/OverviewPlainCard";
import Icon from "@site/static/icons/features/data-modeling.svg";
import Thumbnail from "@site/src/components/Thumbnail";

# Data Modeling

## Introduction

Hasura DDN defines your API schema declaratively. An approach that centralizes all your data collections, operations,
relationships, and permissions in one place. This makes it easy to organize, modify, secure, reason about, and grow the
schema which represents your API.

## Lifecycle

Hasura DDN uses data connectors to connect to your data sources.

Data connectors introspect the source to understand the source schema.

The DDN CLI uses the introspection results to generate the API schema metadata objects. This can also be done manually.

The metadata is then "built" by the DDN CLI into a format which the Hasura GraphQL engine can use to serve your API.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" width="1000px" />

## Metadata Objects

There are [many types of metadata objects](/reference/metadata-reference/index.mdx) which define your API, but the most
important ones which form the backbone of your API are:

- [Models](/data-modeling/model.mdx) which read data
- [Commands](/data-modeling/command.mdx) which modify data
- [Relationships](/data-modeling/relationship.mdx) which connect data
- [Permissions](/data-modeling/permissions.mdx) which protect data

We will cover each of these in more detail in the following sections.



--- File: ../ddn-docs/docs/graphql-api/overview.mdx ---
# Queries Overview

---
description:
  "Get details on how to handle and interact with your databases using Hasura's GraphQL API. Learn methods to initialize
  data connectors and manage queries effectively."
title: Queries Overview
sidebar_label: Basics
keywords:
  - hasura graphql api
  - data connectors
  - graphql queries
  - api tutorial
  - database management
  - query executions
  - api integrations
  - query filters
  - query sorting
  - hasura data connect
sidebar_position: 1
seoFrontMatterUpdated: true
---

# Basics

## Introduction

When you connect a source to Hasura DDN using a
[data connector](/data-sources/overview.mdx#how-do-i-connect-my-data-to-hasura-ddn) and generate Hasura metadata to
represent its resources, the source becomes accessible through a unified GraphQL API. Your Hasura metadata lets you
control how the data is exposed and interacted with.

Every data connector supports querying and subscribing to data out-of-the-box. Some connectors also provide
auto-generated mutations for inserting, updating, or deleting data. For most connectors without auto-generated
mutations, you can define custom native mutations.

Queries, subscriptions, and mutations are exposed as root-level fields, enabling you to fetch, monitor, or modify data
directly through the GraphQL API.

:::tip Check out connector docs

For questions about the capabilities of individual connectors, check out their
[reference docs](/reference/connectors/index.mdx).

:::

The sections below explain how to interact with your data using the GraphQL API.

## Learn more

- [Queries](/graphql-api/queries/index.mdx)
- [Mutations](/graphql-api/mutations/index.mdx)
- [Subscriptions](/graphql-api/subscriptions/index.mdx)
- [Global IDs](/graphql-api/global-ids.mdx)
- [API Versioning](/graphql-api/versioning.mdx)
- [Apollo Federation](/graphql-api/apollo-federation.mdx)
- [Errors](/graphql-api/errors.mdx)
- [GraphQL Schema Diff](/graphql-api/graphql-schema-diff.mdx)



--- File: ../ddn-docs/docs/data-modeling/model.mdx ---
# Models read data

---
sidebar_position: 2
sidebar_label: "Models read data"
description: "Models allow you to define the structure of your data and how it can be queried."
keywords:
  - model
  - data modeling
toc_max_heading_level: 4
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import Thumbnail from "@site/src/components/Thumbnail";

import ModelCreatePostgreSQL from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_model-create-tutorial.mdx";
import ModelCreateMongoDB from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_model-create-tutorial.mdx";
import ModelCreateClickHouse from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_model-create-tutorial.mdx";
import ModelCreateNativeQueryHowToPostgreSQL from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_model-create-native-query-how-to.mdx";
import ModelCreateNativeQueryTutorialPostgreSQL from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_model-create-native-query-tutorial.mdx";
import ModelCreateNativeQueryHowToMongoDB from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_model-create-native-query-how-to.mdx";
import ModelCreateNativeQueryTutorialMongoDB from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_model-create-native-query-tutorial.mdx";
import ModelCreateNativeQueryHowToClickHouse from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_model-create-native-query-how-to.mdx";
import ModelCreateNativeQueryTutorialClickHouse from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_model-create-native-query-tutorial.mdx";

# Models Read Data

## Introduction

In DDN, models represent entities or collections that can be queried in your data sources, such as tables, views,
collections, native queries, and more.

## Lifecycle

The lifecycle in creating a model in your metadata is as follows:

1. Have some entity in your data source that you want to make queryable via your API.
2. Introspect your data source using the DDN CLI with the relevant data connector to fetch the entity resources.
3. Add the model to your metadata with the DDN CLI.
4. Create a build of your supergraph API with the DDN CLI.
5. Serve your build as your API with the Hasura engine either locally or in the cloud.
6. Iterate on your API by repeating this process or by editing your metadata manually as needed.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" />

## Create a model

To add a model you will need to have a data connector already set up and connected to the data source. Follow the
relevant tutorial for your data source in [How to Build with DDN](/how-to-build-with-ddn/overview.mdx) to get to that
point.

### From a source entity

```bash title="Introspect your data source:"
ddn connector introspect <connector_name>
```

Whenever you update your data source, you can run the above command to fetch the latest resources.

```bash title="Show the resources discovered from your data source:"
ddn connector show-resources <connector_name>
```

This is an optional step and will output a list of resources that DDN discovered in your data source in the previous
step.

```bash
ddn model add <connector_link_name> <collection_name>
```

Or you can optionally add all the models by specifying `"*"`.

```bash
ddn model add <connector_link_name> "*"
```

This will add models with their accompanying metadata definitions to your metadata.

You can now build your supergraph API, serve it, and query your data.

:::info Context for CLI commands

Note that the above CLI commands work without also adding the relevant subgraph to the command with the `--subgraph`
flag because this has been set in the CLI context. You can learn more about creating and switching contexts in the
[CLI context](/reference/cli/commands/ddn_context.mdx) section.

:::

For a walkthrough on how to create a model, see the [Model Tutorials](#model-tutorials) section.

### Via native query

You can use the query syntax of the underlying data source to create a native query and expose it as a model in your
supergraph API.

The process of adding a native query is more or less unique for each data connector as each data source by nature has
its own syntax.

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">

    <ModelCreateNativeQueryHowToPostgreSQL />

  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">

    <ModelCreateNativeQueryHowToMongoDB />

  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">

    <ModelCreateNativeQueryHowToClickHouse />

  </TabItem>
</Tabs>

## Update a model

If you want to update your model to reflect a change that happened in the underlying data source you should first
introspect to get the latest resources and then update the relevant model.

```bash title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```bash title="Then, update your existing model:"
ddn model update <connector_link_name> <model_name>
```

You will see an output which explains how new resources were added or updated in the model.

You can now build your supergraph API, serve it, and query your data with the updated model.

You can also update the model by editing the metadata manually.

For a walkthrough on how to update a model, see the [Model Tutorials](#model-tutorials) section.

## Extend a model

A model can be extended in order to return nested data or to enrich or add to the data.

For example you can extend a model like `Customers` to also return the related `Orders` for each customer.

Or you can add a custom piece of logic on a model like `Orders` to compute and return the current currency conversion of
the total price of the order.

The way this is done is via a `Relationship`. Read more about
[creating relationships here](/data-modeling/relationship.mdx).

## Delete a model

```bash title="If you no longer need a model, you can delete it:"
ddn model remove users
```

In addition to removing the `Model` object itself, the DDN CLI will also remove the associated metadata definitions.

## Tutorials {#model-tutorials}

The tutorials below follow on from each particular tutorial in the
[How to Build with DDN](/how-to-build-with-ddn/overview.mdx) section. Select the relevant data connector to follow the
tutorial.

### Creating a model

To query data from your API, you'll first need to create a model that represents that data.

#### From a source entity

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">

    <ModelCreatePostgreSQL />

  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">

    <ModelCreateMongoDB />

  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">

    <ModelCreateClickHouse />

  </TabItem>
</Tabs>

#### Via native query

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">

    <ModelCreateNativeQueryTutorialPostgreSQL />

  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">

    <ModelCreateNativeQueryTutorialMongoDB />

  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">

    <ModelCreateNativeQueryTutorialClickHouse />

  </TabItem>
</Tabs>

### Updating a model

Your underlying data source may change over time. You can update your model to reflect these changes.

You'll need to update the mapping of your model to the data source by updating the
[DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) object.

```bash title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```bash title="Then, update your model:"
ddn model update <connector_link_name> <model_name>
```

This will find changed resources in the data source and attempt to merge them into the model.

If you'd like to completely add the model again, you can first run the `model remove` command (below) and then re-create
your model.

### Extending a model {#tutorial-extend-model}

Find tutorials about extending a model with related information or custom logic in the
[Relationships](/data-modeling/relationship.mdx) section.

### Deleting a model

```bash title="If you no longer need a model, you can delete it:"
ddn model remove users
```

## Reference

You can learn more about models in the metadata reference [docs](/reference/metadata-reference/models.mdx).



--- File: ../ddn-docs/docs/data-modeling/command.mdx ---
# Commands modify data

---
sidebar_position: 4
sidebar_label: "Commands modify data"
description: "Commands allow you to modify data in your data source."
toc_max_heading_level: 4
keywords:
  - command
  - data modeling
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import Thumbnail from "@site/src/components/Thumbnail";

import CommandCreateClickHouse from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_command-create-tutorial.mdx";
import CommandCreateClickHouseNativeMutationTutorial from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_command-create-native-mutation-tutorial.mdx";
import CommandCreateClickHouseNativeMutationHowTo from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_command-create-native-mutation-how-to.mdx";
import CommandCreateLambdaGo from "@site/docs/data-modeling/partials/lambda-connectors/go/_command-create-tutorial.mdx";
import CommandCreateLambdaPython from "@site/docs/data-modeling/partials/lambda-connectors/python/_command-create-tutorial.mdx";
import CommandCreateLambdaTypescript from "@site/docs/data-modeling/partials/lambda-connectors/typescript/_command-create-tutorial.mdx";
import CommandCreateMongoDB from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_command-create-tutorial.mdx";
import CommandCreateMongoDBNativeMutationTutorial from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_command-create-native-mutation-tutorial.mdx";
import CommandCreateMongoDBNativeMutationHowTo from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_command-create-native-mutation-how-to.mdx";
import CommandCreatePostgreSQL from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_command-create-tutorial.mdx";
import CommandCreatePostgreSQLNativeMutationTutorial from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_command-create-native-mutation-tutorial.mdx";
import CommandCreatePostgreSQLNativeMutationHowTo from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_command-create-native-mutation-how-to.mdx";
import CommandUpdateClickHouse from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_command-update-tutorial.mdx";
import CommandUpdateLambdaGo from "@site/docs/data-modeling/partials/lambda-connectors/go/_command-update-tutorial.mdx";
import CommandUpdateLambdaPython from "@site/docs/data-modeling/partials/lambda-connectors/python/_command-update-tutorial.mdx";
import CommandUpdateLambdaTypescript from "@site/docs/data-modeling/partials/lambda-connectors/typescript/_command-update-tutorial.mdx";
import CommandUpdateMongoDB from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_command-update-tutorial.mdx";
import CommandUpdatePostgreSQL from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_command-update-tutorial.mdx";

# Commands Modify Data

## Introduction

In DDN, commands represent operations in your API that can be executed such as those which modify data in your data
sources, (inserts, updates, deletes), or custom business logic operations.

## Lifecycle

The lifecycle in creating a command in your metadata is as follows:

1. Have some operation in your data source that you want to make executable via your API.
2. Introspect your data source using the DDN CLI with the relevant data connector to fetch the operation resources.
3. Add the command to your metadata with the DDN CLI.
4. Create a build of your supergraph API with the DDN CLI.
5. Serve your build as your API with the Hasura engine either locally or in the cloud.
6. Iterate on your API by repeating this process or by editing your metadata manually as needed.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" />

## Create a command

To add a command you will need to have a data connector already set up and connected to the data source. Follow the
[Quickstart](/quickstart.mdx) or the tutorial in [How to Build with DDN](/how-to-build-with-ddn/overview.mdx) to get to
that point.

### From a source operation

```bash title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```bash title="Show the resources discovered from your data source:"
ddn connector show-resources <connector_name>
```

```bash title="Add the command from the discovered resources to your metadata:"
ddn command add <connector_link_name> <operation_name>
```

Or you can optionally add all the commands by specifying `"*"`.

```bash title="Add all commands from your data source:"
ddn command add <connector_link_name> "*"
```

This will add commands with their accompanying metadata definitions to your metadata.

### Via native mutation {#via-native-mutation-how-to}

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">
    <CommandCreatePostgreSQLNativeMutationHowTo />
  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">
    <CommandCreateMongoDBNativeMutationHowTo />
  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">
    <CommandCreateClickHouseNativeMutationHowTo />
  </TabItem>
</Tabs>

You can now build your supergraph API, serve it, and execute your commands.

For a walkthrough on how to create a command, see the [Command Tutorials](#command-tutorials) section below.

## Update a command

If you want to update your command to reflect a change that happened in the underlying data source you should first
introspect to get the latest resources and then update the relevant command.

```bash title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```bash title="Then, update your existing command:"
ddn command update <connector_link_name> <command_name>
```

You will see an output which explains how new resources were added or updated in the command.

You can now build your supergraph API, serve it, and execute your commands with the updated definitions.

You can also update the command by editing the command's metadata manually.

## Delete a command

```bash title="If you no longer need a command, you can delete it:"
ddn command remove <command_name>
```

Along with the command itself, the associated metadata is also removed.

## Tutorials {#command-tutorials}

The tutorials below follow on from each particular tutorial in the
[How to Build with DDN](/how-to-build-with-ddn/overview.mdx) section. Select the relevant data connector to follow the
tutorial.

### Creating a command

To modify data, you first need to create a command that maps to a specific operation within your data source.

#### From a source operation

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">
    <CommandCreatePostgreSQL />
  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">
    <CommandCreateMongoDB />
  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">
    <CommandCreateClickHouse />
  </TabItem>
  <TabItem value="TypeScript" label="Node.js TypeScript">
    <CommandCreateLambdaTypescript />
  </TabItem>
  <TabItem value="Python" label="Python">
    <CommandCreateLambdaPython />
  </TabItem>
  <TabItem value="Go" label="Go">
    <CommandCreateLambdaGo />
  </TabItem>
</Tabs>

#### Via native mutation {#via-native-mutation-tutorials}

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">
    <CommandCreatePostgreSQLNativeMutationTutorial />
  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">
    <CommandCreateMongoDBNativeMutationTutorial />
  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">
    <CommandCreateClickHouseNativeMutationTutorial />
  </TabItem>
</Tabs>

:::info Lambda connectors

Lambda connectors allow you to execute custom business logic directly via your API. You can learn more about Lambda
connectors in the [docs](/business-logic/overview.mdx).

:::

### Updating a command

Your underlying data source may change over time. You can update your command to reflect these changes.

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">
    <CommandUpdatePostgreSQL />
  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">
    <CommandUpdateMongoDB />
  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">
    <CommandUpdateClickHouse />
  </TabItem>
  <TabItem value="TypeScript" label="Node.js TypeScript">
    <CommandUpdateLambdaTypescript />
  </TabItem>
  <TabItem value="Python" label="Python">
    <CommandUpdateLambdaPython />
  </TabItem>
  <TabItem value="Go" label="Go">
    <CommandUpdateLambdaGo />
  </TabItem>
</Tabs>

### Deleting a command

```bash title="If you no longer need a command, you can delete it with the CLI:"
ddn command remove <command_name>
```

## Reference

You can learn more about commands in the metadata reference [docs](/reference/metadata-reference/commands.mdx).



--- File: ../ddn-docs/docs/data-modeling/relationship.mdx ---
# Relationships connect data

---
sidebar_position: 4
sidebar_label: "Relationships connect data"
description: "Relationships allow you to connect data across different models."
keywords:
  - relationship
  - data modeling
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import Thumbnail from "@site/src/components/Thumbnail";

import PostgresRelationshipCreateModelToModel from "@site/docs/data-modeling/partials/classic-connectors/postgreSQL/_relationship-create-model-to-model.mdx";
import MongoDBRelationshipCreateModelToModel from "@site/docs/data-modeling/partials/classic-connectors/mongoDB/_relationship-create-model-to-model.mdx";
import ClickHouseRelationshipCreateModelToModel from "@site/docs/data-modeling/partials/classic-connectors/clickHouse/_relationship-create-model-to-model.mdx";

# Relationships Connect Data

## Introduction

Relationships allow you to connect data, enabling you to query data across multiple entities.

Examples include:

- Querying a `Customer` and at the same time getting their `Orders` and each `Product` item in those orders. (Model to
  Model)
- Querying a `Customer` and also getting the analytics of their app usage from another data source. (Model to Model in
  another subgraph or data connector)
- Querying an `Order` and also getting a live currency conversion for the value of the order enabled with a lambda data
  connector with a connection to a currency exchange API. (Model to Command)

Relationships can be added between _any_ kind of semantically related models and/or commands. They do not need to be
related in the data source by, for example, a foreign key. They also do not need to be backed by the same data source or
be in the same subgraph.

## Lifecycle

Many relationships can be created automatically by the DDN CLI from detected underlying connections such as foreign
keys. In such cases the lifecycle in creating a relationship in your metadata is as follows:

1. Introspect your data source using the DDN CLI with the relevant data connector to fetch the entity resources.
2. Add the detected relationships to your metadata with the DDN CLI.
3. Create a build of your supergraph API with the DDN CLI.
4. Serve your build as your API with the Hasura engine either locally or in the cloud.
5. Iterate on your API by repeating this process or by editing your metadata manually as needed.

<Thumbnail src="/img/data-modeling/ddn-cli-process.png" alt="Data modeling lifecycle" width="1000px" />

If the relationship cannot be detected automatically, you can easily manually create a relationship in your metadata and
then perform lifecycle steps 3-5 from above as needed.

## Create a relationship

Relationships are defined in metadata from an
[object type](/reference/metadata-reference/types.mdx#objecttype-objecttype), to a
[model](/reference/metadata-reference/models.mdx) or [command](/reference/metadata-reference/commands.mdx). But since
models and commands are also defined with object types, you can think of relationships as being between models and/or
commands.

The target command can be enabled with a a custom piece of business logic on a lambda data connector, or a native
mutation operation.

### Using the DDN CLI

The DDN CLI and your data connectors will detect many relationships in your data sources automatically, for instance
from foreign keys in a relational database, and once introspected, you can add them to your metadata.

```bash title="Introspect your data source:"
ddn connector introspect <connector_name>
```

```bash title="Show the found relationships:"
ddn connector show-resources <connector_name>
```

```bash title="Add a relationship to your metadata:"
ddn relationship add <connector_link_name> <collection_name>
```

Or optionally add all relationships found for a connector at once:

```bash
ddn relationship add <connector_link_name> "*"
```

:::info Context for CLI commands

Note that the above CLI commands work without also adding the relevant subgraph to the command with the `--subgraph`
flag because this has been set in the CLI context. You can learn more about creating and switching contexts in the
[CLI context](/) section. {/* TODO: Add link */}

:::

### Manually creating a relationship

Relationships can also be manually added to your metadata.

The [VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can help you to author
relationships.

For example, you can configure a relationship so that you can also get Orders when querying a Customer.

```yaml title="Create a relationship in your metadata:"
---
kind: Relationship
version: v1
definition:
  sourceType: Customers # The existing source object type which also defines the model
  name: orders # A name we want to use when we query the Orders from the Customer
  target:
    model: # The target can be a model or a command
      name: Orders # The existing model that we want to access when we query the Orders from the Customer
      relationshipType: Array # The relationship type which can be Object or Array. Since a customer can have many orders, we use an Array.
  mapping: # The mapping defines which field on the source object type maps to which field on the target model
    - source:
        fieldPath:
          - fieldName: customerId # The existing field on the source object type that we want to map to the target model
      target:
        modelField:
          - fieldName: customerId # The existing field on the target model that we want to map to the source object type
```

By defining this `Relationship` object, all other [models](/reference/metadata-reference/models.mdx) or
[commands](/reference/metadata-reference/commands.mdx) whose output type is the source object type in the relationship
will now have a connection to the target model or command.

Learn more about the [Relationship](/reference/metadata-reference/relationships.mdx) object.

## Update a relationship

Your underlying data source may change over time. You can update your relationship to reflect these changes.

If you have an automatically detected relationship and a property on the source object type has changed, you can update
the relationship to reflect this change.

First, update your connector configuration and models.

```bash title="Update your source introspection:"
ddn connector introspect <connector_name>
```

```bash title="Then, update your model:"
ddn model update <connector_name> <model_name>
```

Now, you can either delete the existing `Relationship` object and use the DDN CLI to add it again:

```bash title="Delete your existing relationship manually and add it again:"
ddn relationship add <connector_link_name> <collection_name>
```

Or you can update the `Relationship` object manually. Learn more about the
[Relationship](/reference/metadata-reference/relationships.mdx) object.

## Delete a relationship

If you no longer need a relationship, simply delete the `Relationship` metadata object manually. It is fully
self-contained.

## Tutorials

These tutorials follow on from the tutorials in the
[How to Build with DDN section](/how-to-build-with-ddn/overview.mdx).

### Creating a relationship

#### Between a model and a model

<Tabs groupId="source-preference" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">

<PostgresRelationshipCreateModelToModel />

  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">

<MongoDBRelationshipCreateModelToModel />

  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">

<ClickHouseRelationshipCreateModelToModel />

  </TabItem>
</Tabs>

#### Between a model and a command

We will use a business logic function defined in a lambda data connector to build a model to command relationship for
all connector types.

<details>
  <summary>Create a custom function with the TypeScript lambda data connector to use in the tutorials</summary>

Initialize a new data connector with the TypeScript connector.

```bash title="Run the following command in your DDN project directory:"
ddn connector init my_ts -i
```

- Select `hasura/nodejs` from the list of connectors.
- Accept the suggested port.
- Edit the `functions.ts` file in the connector directory with the `shoutName` function.

```ts
export function shoutName(name: string) {
  return `${name.toUpperCase()}`;
}
```

```bash title="Introspect the connector:"
ddn connector introspect my_ts
```

Then, we can add the model made available from the introspection.

```bash title="Track the function:"
ddn command add my_ts shoutName
```

</details>

To create a relationship between the `Users` model and the `shoutName` function (see the drop-down above for how to
implement this function) we can create the following relationship. Using the
[VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) makes the authoring and
validation of the relationship easier.

```yaml title="Create a relationship:"
---
kind: Relationship
version: v1
definition:
  name: shoutName # Define a name to expose in the supergraph API
  sourceType: Users # The existing source object type (which also defines the source model Users)
  target:
    command: # The target is a command
      name: ShoutName # The name of the existing command we have defined in metadata
      subgraph: app # The existing subgraph the command is defined in
  mapping:
    - source:
        fieldPath:
          - fieldName: name # The field on the source object type that we want to provide to the target command as an argument
      target:
        argument:
          argumentName: name # The name of the argument on the target command that we want to map to the source field
```

```bash title="Create a build of your supergraph API:"
ddn supergraph build local
```

```bash title="Serve your build as your API locally:"
ddn run docker-start
ddn console --local
```

We can now query the `Users` model and also return the result of the `shoutName` function with its default argument of
`name`.

```graphql title="Query the Users model:"
query {
  users {
    name
    shoutName
  }
}
```

```json title="Result:"
{
  "data": {
    "users": [
      {
        "name": "Alice",
        "shoutName": "ALICE"
      },
      {
        "name": "Bob",
        "shoutName": "BOB"
      },
      {
        "name": "Charlie",
        "shoutName": "CHARLIE"
      }
    ]
  }
}
```

With relationships and custom commands we can transform or enrich any data.

### Updating a relationship

Your underlying data source may change over time. You can update your relationship to reflect these changes.

If you have an automatically detected relationship and a property on the source object type has changed, you can update
the relationship to reflect this change.

First, update your connector configuration and models.

```bash title="Update your source introspection:"
ddn connector introspect <connector_name>
```

```bash title="Then, update your model:"
ddn model update <connector_name> <model_name>
```

Now, you can either delete the existing `Relationship` object and use the DDN CLI to add it again:

```bash title="Delete your existing relationship manually and add it again:"
ddn relationship add <connector_link_name> <collection_name>
```

Or you can update the `Relationship` object manually. Learn more about the
[Relationship](/reference/metadata-reference/relationships.mdx) object.

### Deleting a relationship

If you no longer need a relationship, simply delete the `Relationship` metadata object manually. It is fully
self-contained.

## Reference

You can learn more about relationships in the metadata reference
[docs](/reference/metadata-reference/relationships.mdx).



--- File: ../ddn-docs/docs/data-modeling/permissions.mdx ---
# Permissions protect data

---
sidebar_position: 5
sidebar_label: "Permissions protect data"
description: "Permissions allow you to control who can access your data and what they can do with it."
keywords:
  - permissions
  - data modeling
toc_max_heading_level: 4
---

# Permissions Protect Data

## Introduction

Permissions keep data secure by allowing you to control what can be accessed in your API by which user roles.

When an authentication mode is enabled, the Hasura engine will look for session variables on every API request, it can
then use permissions defined in metadata and the session variables to determine if the request is allowed to proceed.

## Lifecycle

Hasura DDN uses Role Based Access Control (RBAC) to determine which user roles can access which data.

The DDN CLI will automatically create permissions for your models and commands when they are added to your metadata for
only the `admin` role by default.

All other permissions for all other user roles must be added manually.

## Create permissions

### Row access

You can create a `ModelPermission` object to implement row-level security and restrict which rows a user can access.

For example, to only allow users to access their own records in the `Users` table:

```yaml title=""
---
# e.g., Users.hml
kind: ModelPermissions
version: v1
definition:
  modelName: Users
  permissions:
    # admin is present by default
    - role: admin
      select:
        filter: null
    #highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
    #highlight-end
```

The highlighted role above will filter responses from the `Users` field in your API to only those whose `id` matches the
`x-hasura-user-id` passed in the header of the request.

### Field access

To restrict which fields can be queried, you can create a `TypePermission` object.

Below, the user role can only access the `name` field, not the `id` field which the admin role can.

```yaml title="The user role can only access their name field:"
# e.g., Users.hml
---
kind: TypePermissions
version: v1
definition:
  typeName: Users
  permissions:
    # admin is present by default
    - role: admin
      output:
        allowedFields:
          - id
          - name
    #highlight-start
    - role: user
      output:
        allowedFields:
          - name
    #highlight-end
```

### Command (mutation) access

To determine commands can be executed by which roles, you can create a `CommandPermission` object.

```yaml title="In this example, we'll make it so a user can update their own record:"
# e.g., UpdateUsersById.hml
---
kind: CommandPermissions
version: v1
definition:
  commandName: UpdateUsersById # Specify the existing command
  permissions:
    - role: admin
      allowExecution: true
      #highlight-start
    - role: user
      allowExecution: true
      argumentPresets: # Specify the arguments and their values which need to be passed to the command
        - argument: keyId
          value:
            sessionVariable: "x-hasura-user-id" # The value of the argument must equal the session variable
      #highlight-end
```

## Update permissions

Since all permissions are stored in metadata, you can use your text editor to find and update them easily.

For example, to check everything which the `user` role can access, search for `- role: user` and analyze the results.

## Deleting permissions

If you no longer need a role, find all mentions of it in your metadata and remove them all.

If you no longer need a particular permission, simply remove it from the relevant `ModelPermissions`, `TypePermissions`,
or `CommandPermissions` object.

## Reference

You can learn more about permissions in the metadata reference [docs](/reference/metadata-reference/permissions.mdx).



--- File: ../ddn-docs/docs/graphql-api/global-ids.mdx ---
# Global IDs

---
title: Global IDs
sidebar_label: Global IDs
sidebar_position: 5
description:
  "A Global ID is a unique identifier for an object across the entire application, not just within a specific table or
  type. Think of it as an ID which you can use to fetch any object directly, regardless of what kind of object it is.
  This is different from typical database IDs, which are often guaranteed unique only within a particular table.
  Hasura's Global ID implementation can be used to provide options for GraphQL clients to elegantly handle caching and
  data re-fetching in a predictable and standardized way. The Global ID generated by Hasura DDN follows the Relay Global
  ID spec."
keywords:
  - Global ID
  - Global Object Identification
  - Relay
  - Relay GraphQL
seoFrontMatterUpdated: true
---

## Introduction

A Global ID is a unique identifier for an object across the entire application, not just within a specific table or
type. Think of it as an `id` which you can use to fetch any object in your entire supergraph directly, regardless of
what kind of object it is. This is different from typical database IDs, which are often guaranteed unique only within a
particular table.

Hasura's Global ID implementation can be used to provide options for GraphQL clients, such as
[Relay](https://relay.dev/), to elegantly handle caching and data re-fetching in a predictable and standardized way.

The Global ID generated by Hasura DDN follows the
[GraphQL Global Object Identification spec](https://graphql.org/learn/global-object-identification/).

## Using the Global ID

As the example below shows, our users have Global IDs enabled and we can request back the `id` Global ID field for any
particular user. Then, this `id` can be used to fetch this user directly using a `node` query.

For the following GraphQL request on a model which has enabled Global IDs:

```graphql
{
  userById(user_id: 1) {
    id # This is the global ID of the object in the supergraph
    user_id # This is the unique identifier of object. Eg: a row in a table.
    name
  }
}
```

The response obtained should look like the following:

```json
{
  "data": {
    "userById": {
      "id": "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IlVzZXJzIiwiaWQiOnsidXNlcl9pZCI6IjEifX0=",
      "user_id": 1,
      "name": "Bob"
    }
  }
}
```

**Global ID vs Unique Identifier**

- `user_id`: In this example, this is the unique identifier of the object. Eg: a row in a table.
- `id`: This is the global ID of the object in the graph. It is unique across the entire supergraph. It must be named
  `id` to conform with the GraphQL spec.

To enable Global IDs on a Model which already have an `id` field, you will need to remap the existing `id` field to
another name. [See here](#globalid-remap-id-field)

Now, with the Global ID received above, the `User` object corresponding to `user_id: 1` can be retrieved, as shown
below.

```graphql
{
  node(id: "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IlVzZXJzIiwiaWQiOnsidXNlcl9pZCI6IjEifX0=") {
    id
    __typename
    ... on User {
      name
    }
  }
}
```

The response to the above request should identify the `User` with `user_id: 1`.

```json
{
  "node": {
    "id": "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IlVzZXJzIiwiaWQiOnsidXNlcl9pZCI6IjEifX0=",
    "__typename": "User",
    "name": "Bob"
  }
}
```

:::info Global ID format

The Global ID is a base64 encoded string. Eg:

```text
eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IlVzZXJzIiwiaWQiOnsidXNlcl9pZCI6IjEifX0=
```

Is decoded as the following:

```string
{"version":1,"typename":"Users","id":{"user_id":"1"}}
```

This strategy guarantees that the Global ID is unique across the entire supergraph.

:::

## Enabling Global ID in metadata

You will need to edit a minimum of two objects in metadata to enable Global ID:

1. In the `ObjectType` `definition`, specify which field, or set of fields, should be used as the source(s) to create
   the Global ID . The following example uses a `user_id` field to create it:

```yaml
globalIdFields: [user_id]
```

See more in the [ObjectType definition](/reference/metadata-reference/types.mdx#objecttype-objecttypev1).

2. Then add the following to the `definition` section of the `Model`:

```yaml
globalIdSource: true
```

See more in the [Model definition](/reference/metadata-reference/models.mdx#model-modelv1).

## Enabling Global ID for Models which already have an `id` field {#globalid-remap-id-field}

It's a common occurrence to have an existing field in your `ObjectType` named `id`. Since the GraphQL spec mandates that
the `id` field should be for the Global ID, you will need to remap the existing `id` field to a different field name.

With the help of the Hasura VS Code extension and the output errors from the metadata build service in the Hasura CLI,
you can easily determine which fields need to be remapped.

In the ObjectType definition, you can remap the `id` field to a different field name `user_id` for example, as shown
below:

```yaml {5,7-8,18-20}
kind: ObjectType
version: v1
definition:
  name: Users
  globalIdFields: [user_id]
  fields:
    - name: user_id
      type: Uuid!
    - name: name
      type: Text!
  graphql:
    typeName: Users
    inputTypeName: UsersInput
  dataConnectorTypeMapping:
    - dataConnectorName: postgres_connector
      dataConnectorObjectType: users
      fieldMapping:
        user_id:
          column:
            name: id
        name:
          column:
            name: name
```

You will then also need to update the `Model`, `TypePermissions` and `Relationships` definitions and anywhere else where
the previous identifier of `id` for the `User` model was used.

You will also need to edit the `target` metadata for `Relationships` in other Models too so that they reference the new
identifier of `user_id`.

As mentioned, the Hasura VS Code extension and the output errors from the metadata build service in the Hasura CLI will
help you find all the places where the `id` field needs to be remapped.



--- File: ../ddn-docs/docs/graphql-api/versioning.mdx ---
# API Versioning through Field Deprecation

---
title: API Versioning through Field Deprecation
sidebar_label: API Versioning
sidebar_position: 6
description: "Learn how to version your GraphQL API using the @deprecated tag."
keywords:
  - version
  - versioned
  - deprecation
  - deprecated
---

# API Versioning through Field Deprecation

## Introduction

As your API grows and adapts to new features or functionality, you might need to adjust its structure. But changing or
removing fields in API responses can cause breaking changes. These changes may disrupt client applications that depend
on those fields, leading to errors or unexpected behavior.

Hasura supports the `@deprecated` directive in GraphQL, making it straightforward to mark fields as deprecated. You can
include an optional reason to help explain the change, signaling to consumers which fields are outdated or updated.

As an example, imagine we have a type `Car` with an existing field named `engine` which is deprecated in favor of
`motor`:

```graphql
type Car {
  id: ID!
  make: String!
  model: String!
  engine: EngineSpec @deprecated(reason: "Use field 'motor' instead")
  motor: MotorSpec
}
```

Consumers of our API will know that `engine` has been deprecated, _why_ we've deprecated it, and which field to use in
its place.

:::info Learn more

You can learn more about this directive in [the spec](https://spec.graphql.org/October2021/#sec-Field-Deprecation).

:::

## Using field deprecation in DDN

The following metadata objects have field deprecation in their GraphQL configuration.

### Model

The following example shows how to deprecate a field in a [model](/reference/metadata-reference/models.mdx):

```yaml
kind: Model
version: V1
definition:
  name: Cars
  objectType: Car
  orderableFields:
    - Id
  graphql:
    selectUniques:
      - queryRootField: selectCar
        uniqueIdentifier:
          - make
          - model
        deprecated:
          reason: Use selectCarById instead
      - queryRootField: selectCarById
        uniqueIdentifier:
          - Id
```

And the resulting schema:

```graphql
type Query {
  selectCar(make: String!, model: String): Car @deprecated(reason: "use selectCarById instead")
  selectCarById(Id: ID!): Car
}
```

### Command

The following example shows how to deprecate a field in a [command](/reference/metadata-reference/commands.mdx):

```yaml
kind: Command
version: V1
definition:
  name: GetEngineSpec
  outputType: Engine
  graphql:
    rootFieldName: getEngineSpec
    rootFieldKind: Query
    deprecated:
      reason: "Fuel Engines are no longer supported from Jan 01 2035"
```

And the resulting schema:

```graphql
type Query {
  getEngineSpec: Engine @deprecated(reason: "Fuel Engines are no longer supported from Jan 01 2035")
}
```

### ObjectType

The following example shows how to deprecate a field in an
[ObjectType](/reference/metadata-reference/types.mdx#objecttype-objecttype):

```yaml
kind: ObjectType
version: V1
definition:
  name: Car
  fields:
    - name: Id
      type: String
    - name: make
      type: String
    - name: model
      type: String
    - name: engine
      type: String
      deprecated:
        reason: Use motor field instead
    - name: motor
      type: String
  graphql:
    typeName: Car
```

And the resulting schema:

```graphql
type Car {
  Id: String
  make: String
  model: String
  engine: String @deprecated(reason: "Use motor field instead")
  motor: String
}
```

### Relationship

The following example shows how to deprecate a field in a
[relationship](/reference/metadata-reference/relationships.mdx):

```yaml
kind: Relationship
version: V1
definition:
  name: engineSpec
  source: Car
  target:
    command:
    name: GetEngineSpec
  mapping:
    - source:
        fieldPath:
          - fieldName: engine
      target:
        argument:
          argumentName: name
  deprecated:
    reason: Engines on cars are no longer supported from Jan 01 2035
```

And the resulting schema:

```graphql
type Car {
  Id: String
  make: String
  model: String
  engine: String @deprecated(reason: "Use motor field instead")
  motor: String
  engineSpec: Engine @deprecated(reason: "Engines on cars are no longer supported from Jan 01 2035")
  motorSpec: Motor
}
```

## Default deprecation reason

In cases where no deprecation reason is explicitly provided, the `@deprecated` directive defaults to
`No longer supported` as the reason.

The following example from [ObjectType](/reference/metadata-reference/types.mdx#objecttype-objecttype) metadata
illustrates deprecating a field without specifying a reason.

```yaml
kind: ObjectType
version: V1
definition:
  name: Car
  fields:
    - name: Id
      type: String
    - name: make
      type: String
    - name: model
      type: String
    - name: engine
      type: String
      deprecated:
        reason: null
    - name: motor
      type: String
  graphql:
    typeName: Car
```

And the resulting schema:

```graphql
type Car {
  Id: String

  model: String
  engine: String @deprecated(reason: "No longer supported")
  motor: String
}
```



--- File: ../ddn-docs/docs/graphql-api/apollo-federation.mdx ---
# Apollo Federation

---
title: Apollo Federation
sidebar_label: Apollo Federation
sidebar_position: 7
description: "Hasura DDN can be used as a subgraph in a supergraph built using Apollo Federation"
keywords:
  - Subgraph
  - Supergraph
  - Apollo
  - Apollo Supergraph
  - Apollo Federation
  - Apollo Federation v2
  - Federation
  - Federation v2
seoFrontMatterUpdated: true
---

## Introduction

Hasura DDN itself can be used as a subgraph in a supergraph created by
[Apollo Federation](https://www.apollographql.com/docs/federation/). This page is to help you understand how to use
Hasura in conjunction with an existing Apollo supergraph. Alternatively, if you're looking for a guide on how to build a
federated Hasura DDN supergraph, check out our [getting started](/how-to-build-with-ddn/overview.mdx) guide.

Apollo Federation is a way to compose multiple GraphQL services (called subgraphs) into a unified API (called a
supergraph).

:::info Supergraphs and Subgraph terminology in Hasura DDN and Apollo Federation

Some of the naming used in Apollo Federation conflicts with the same names used in Hasura. Here is a quick glossary to
help you understand the terms better:

| Term       | Hasura                                                                                                       | Apollo Federation                                                                                               |
| ---------- | ------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- |
| Subgraph   | A subgraph in Hasura is the notion of a module of Hasura supergraph metadata.                                | A subgraph in Apollo Federation is a **standalone GraphQL service.**                                            |
| Supergraph | A supergraph in Hasura is the collection of subgraph metadata and the resultant GraphQL API that is created. | A supergraph in Apollo Federation is a unified GraphQL API that is created by stitching multiple subgraph APIs. |

:::

## Using DDN as a subgraph

Hasura DDN is compliant with the
[Apollo Federation subgraph specification](https://www.apollographql.com/docs/federation/subgraph-spec/), so you can
plug Hasura DDN in as a subgraph in your Apollo federated supergraph.

```mermaid
graph LR;
  clients(Clients);
  router([Apollo Supergraph <br/>Router]);
  serviceA[GraphQL API <br/>Subgraph A];
  serviceB[Hasura DDN <br/>Subgraph B];
  router --- serviceA & serviceB;
  clients -.- router;
  class clients secondary;

```

## Enabling Apollo Federation fields in Hasura DDN metadata

You will need to edit the
[`GraphqlConfig`](/reference/metadata-reference/graphql-config.mdx#graphqlconfig-graphqlconfig) for the supergraph to
enable the fields required for schema stitching by the Apollo supergraph router. You will have to add the following in
the `definition` for `GraphqlConfig` (usually in `supergraph/graphql-config.hml` file):

```yaml
apolloFederation:
  enableRootFields: true
```

An example of the `graphql-config.hml` file with apollo federation fields enabled:

```yaml {8-9}
kind: GraphqlConfig
version: v1
definition:
  query:
    rootOperationTypeName: Query
  mutation:
    rootOperationTypeName: Mutation
  apolloFederation:
    enableRootFields: true
```

## Marking a Type as an Apollo Entity

Types defined in Hasura DDN can also be marked as
[Apollo Entities](https://www.apollographql.com/docs/federation/entities/). This will allow the same type to be resolved
from DDN (and your other subgraphs that defines the type) as well.

To mark a type as an entity, you will have to edit the metadata for the `ObjectType` and the `Model` which will be used
to resolve the type.

1. Add the keys for the `ObjectType`. The [keys](https://www.apollographql.com/docs/federation/entities/#1-define-a-key)
   can be defined as following in the
   [`definition.graphql`](/reference/metadata-reference/types#objecttype-objecttypegraphqlconfiguration) for
   `ObjectType`:

   ```yaml
   apolloFederation:
     keys: # The fields that uniquely identifies the entity
       - fields:
           - id
   ```

2. Mark the `Model` that should act as the source for the entity. This can be done by adding the following in the
   [`definition.graphql`](/reference/metadata-reference/models#model-modelgraphqldefinition) for `Model`:

   ```yaml
   apolloFederation:
     entitySource: true
   ```

:::info Single direction Federation support

Apollo Federation support in DDN only allows extending Apollo subgraphs with DDN types.

The other way i.e. extending DDN types with other Apollo subgraphs is not currently possible.

Consider the following configuration:

The `Review` type defined in DDN:

```graphql
type Review {
  id: ID!
  productId: ID!
  rating: Int!
  comment: String
}
```

The `Product` type defined in an Apollo subgraph:

```graphql
type Product @key(fields: "id") {
  id: ID!
  name: String!
}
```

In this example we cannot extend the `Review` type in DDN with the `Product` type from another Apollo subgraph.

:::



--- File: ../ddn-docs/docs/graphql-api/errors.mdx ---
# GraphQL API Errors

---
title: GraphQL API Errors
sidebar_label: Errors
sidebar_position: 8
description: "Learn about error responses from the Hasura DDN GraphQL API"
keywords:
  - errors
  - api errors
  - graphql api errors
  - internal errors
  - graphql api internal errors
seoFrontMatterUpdated: true
---

# GraphQL API Errors

## Introduction

The GraphQL API returns an error response when the requested operation fails to execute successfully. Hasura provides
the error responses that adhere to the [GraphQL spec](https://spec.graphql.org/October2021/#sec-Errors). Each error
object contains only the `message` field, providing a string description of the error intended for the API consumer.

:::note

Note that in future iterations, the error object could be expanded with additional information, such as path and error
code.

:::

Below is an example of an error response triggered by using a negative value as the `limit` argument:

```json
{
  "data": null,
  "errors": [
    {
      "message": "Unexpected value: expecting NON-NEGATIVE 32-bit INT, but found: -10"
    }
  ]
}
```

:::info Error status code

GraphQL API errors will always be returned with a `200 OK` status code. The status code is not indicative of the success
or failure of the operation. The `errors` array in the response object should be checked to determine the success or
failure of the operation. Most GraphQL clients will automatically handle this for you.

:::

## Internal Errors

Internal errors are unexpected exceptions **within** the API execution cycle. These often originate from system-level
issues such as database inconsistencies or network disruptions which lie beyond the control and scope of the endpoint
API consumer.

GraphQL API responses for internal errors do not contain detailed error messages, as API consumers lack, and should not
have, the necessary context to resolve such issues. Internal errors may contain sensitive details not meant to be
exposed due to privacy and security concerns. Therefore, the API response for internal errors is simplified to only
include the message `internal error`.

Example:

```json
{
  "data": null,
  "errors": [
    {
      "message": "internal error"
    }
  ]
}
```

However as a Hasura supergraph author, you have access to details of most internal errors, including but not limited to:

- [DDN metadata](/reference/metadata-reference/index.mdx) errors:
  - Such as the absence of a data source for model or command root fields or arguments.
  - The omission of required session variables in the request.
- [Data connector](/data-sources/overview.mdx) interactions:
  - These are useful in debugging connection or interface errors with your data connectors.

The details are available through [traces](/observability/built-in/traces). It is recommended to review all error spans
within the trace to obtain details of the internal error.

If the trace still does not reveal the details of the error, it may originate from Hasura internals. In such cases,
please reach out to [Hasura support](https://hasura.io/help/) with the trace ID.



--- File: ../ddn-docs/docs/graphql-api/graphql-schema-diff.mdx ---
# GraphQL Schema Diff

---
description:
  "Compare two supergraph builds to understand the changes in the graphql schema and the impact on the existing queries
  and mutations."
title: GraphQL Schema Diff
sidebar_label: GraphQL Schema Diff
keywords:
  - graphql
  - graphql analysis
  - hasura v3
  - schema
  - schema diff
  - schema analysis
  - schema comparison
  - graphql schema
  - graphql schema diff
  - Builds
  - impact assessment
  - breaking changes
  - dangerous changes
  - safe changes

sidebar_position: 8
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# GraphQL Schema Diff

When you are managing a GraphQL API, you often need to make changes to the schema. These changes could be adding new
fields, removing existing fields, or changing the types of fields. When you make these changes, you need to understand
the impact of these changes on the existing queries and mutations. The schema diff feature helps you understand these
impacts by comparing the schemas of two supergraph builds.

<Thumbnail src="/img/schema-diff/0.0.1_console_schema_diff_subview.png" alt="GraphQL Schema Diff" width="700px" />

## Using the GraphQL Schema Diff feature in Console

1. Go to [DDN console](https://console.hasura.io) and select a project. (Make sure you have a project with at least two
   supergraph builds if you don't have two supergraph builds, head to [Getting Started](/quickstart.mdx) to create
   builds to get started).
2. Select a build that you want to compare from the **Builds** header toolbar.
3. In the DDN project console, navigate to the **Builds** section from the sidebar
4. In the **Builds** section, navigate to the **Schema Diff** tab.
5. In the **Schema Diff** tab, you can select two supergraph builds to compare their schemas.
6. You can see the differences between the two schemas in the **Schema Diff** tab.

<Thumbnail src="/img/schema-diff/0.0.1_console_schema_diff.png" alt="GraphQL Schema Diff" width="700px" />

## Reference and Compare Builds

Imagine that you already have supergraph build **A** applied to your Hasura DDN project and you want to see the impact
of applying a new supergraph build **B**. You can select build **A** as the `Reference Build` and build B as the
`Compare Build` to compare the schemas.

Which means if there are fields added in build **B** without removing any fields from build **A**, the schema diff will
show these as safe changes and list them in the `Safe` section. If there are fields removed in build **B**, the schema
diff will show these as breaking changes and list them in the `Breaking` section.

## Schema Diff Analysis

The schema diff analysis in Hasura DDN is divided into three sections: `Breaking`, `Dangerous`, and `Safe`.

---

### Breaking Changes

**Definition:** Breaking changes are modifications that can disrupt existing GraphQL operations such as queries,
mutations, or subscriptions. These changes typically involve removing or renaming fields, types, or arguments.

**Examples:**

1. **Field Removal:**

   - **Source Build:**
     ```graphql
     type Query {
       sales_hello: String
     }
     ```
   - **Target Build:**
     ```graphql
     type Query {
       // sales_hello field is removed
     }
     ```
   - **Impact:** Any client queries requesting `sales_hello` will fail.

2. **Type Removal:**
   - **Source Build:**
     ```graphql
     type User {
       id: ID!
       name: String!
     }
     ```
   - **Target Build:**
     ```graphql
     // User type is removed
     ```
   - **Impact:** Any client queries or mutations involving the `User` type will fail.

---

### Dangerous Changes

**Definition:** Dangerous changes are modifications that can potentially alter the behavior of existing GraphQL
operations without necessarily breaking them. These changes include modifications to field types, arguments, or default
values.

**Examples:**

1. **Field Type Change:**

   - **Source Build:**
     ```graphql
     type Query {
       sales_count: Int
     }
     ```
   - **Target Build:**
     ```graphql
     type Query {
       sales_count: Float
     }
     ```
   - **Impact:** Queries that expect an `Int` return type might misinterpret the `Float` value, potentially causing
     issues in client-side logic.

2. **Argument Default Value Change:**
   - **Source Build:**
     ```graphql
     type Query {
       products(limit: Int = 10): [Product]
     }
     ```
   - **Target Build:**
     ```graphql
     type Query {
       products(limit: Int = 20): [Product]
     }
     ```
   - **Impact:** Queries relying on the default value of `limit` will get more results than expected.

---

### Safe Changes

**Definition:** Safe changes are modifications that do not disrupt existing GraphQL operations. These changes typically
involve adding new fields, types, or arguments, enhancing the schema without affecting current functionality.

**Examples:**

1. **Field Addition:**

   - **Source Build:**
     ```graphql
     type Query {
       sales_total: Float
     }
     ```
   - **Target Build:**
     ```graphql
     type Query {
       sales_total: Float
       sales_average: Float // New field added
     }
     ```
   - **Impact:** Existing queries remain unaffected, and new queries can utilize the `sales_average` field.

2. **Type Addition:**
   - **Source Build:**
     ```graphql
     type Query {
       product(id: ID!): Product
     }
     ```
   - **Target Build:**
     ```graphql
     type Query {
       product(id: ID!): Product
       category(id: ID!): Category // New type and query added
     }
     ```
   - **Impact:** Existing queries remain unaffected, and new queries can utilize the `category` query.



--- File: ../ddn-docs/docs/graphql-api/response-size-limit.mdx ---
# Response Size Limit

---
sidebar_position: 12
sidebar_label: Response Size Limit
description: "Hasura rejects responses from a connector when their size exceeds a certain limit"
keywords:
  - size limit
  - response size limit
  - connector response
seoFrontMatterUpdated: true
---

# Connector Response Size Limit

The maximum size for responses from connectors is **30 MB**. Beyond this threshold, Hasura will reject the response to
ensure optimum performance and data processing. It's important to be mindful of this constraint when making data queries
to Hasura's [GraphQL API](/graphql-api/overview/).

To prevent hitting the response size limit, API consumers are encouraged to utilize the
[limit argument](/graphql-api/queries/pagination#limit-results) in their queries to avoid over-fetching data from
sources via data connectors.

When GraphQL API requests exceed this size limit, they will result in an
[internal error](/graphql-api/errors#internal-errors) API response.

```json
{
  "data": null,
  "errors": [
    {
      "message": "internal error"
    }
  ]
}
```

Hasura users are advised to check the traces for more detailed error information, which includes the actual response
size from the connector.

:::note Response size limit increases

If you require an increase in the response size limit, please reach out to [Hasura support](https://hasura.io/help/) for
assistance.

:::



--- File: ../ddn-docs/docs/graphql-api/queries/index.mdx ---
# Queries

---
sidebar_label: Queries
sidebar_position: 0
description:
  "In-depth guide to conducting GraphQL Queries and harnessing the full potential of Hasura's GraphQL API. Discover
  functionalities like simple queries, nesting, sorting, pagination, and employing multiple arguments or queries. Learn
  more about filtering queries and the use of variables, aliases, fragments, and directives."
keywords:
  - graphql queries
  - hasura graphql api
  - api queries
  - data reading
  - hasura console
  - data connector features
  - hasura ddn
  - simple queries
  - database management
  - paginating results
seoFrontMatterUpdated: true
---

# Basics of Queries

## Introduction

GraphQL queries are used to **read data** by interacting with [models](/reference/metadata-reference/models.mdx) in your
Hasura DDN project.

Queries can be used to return multiple records while specifying exactly which fields to include in the response. You can
also query for unique records with the same level of control, retrieving a single result that matches your desired
fields.

Further, you can filter queries to only return records that match your specific conditions **where** certain fields meet
explicit criteria.

In addition to fetching records, queries can perform aggregations, which allow you to compute and summarize data. For
example, you can count the number of records, calculate averages, or find totals directly in the query response.

Since the GraphQL API is self-documenting, you can write queries manually or use tools like auto-completion and the
GraphiQL explorer in the Hasura DDN console to help you build and test them. GraphiQL displays the available root-level
types, fields, and relationships, making it easier to construct queries accurately.

## Configuration

You can configure the overall usage of queries in your GraphQL API using
[the `GraphQlConfig` object](/reference/metadata-reference/graphql-config.mdx#graphqlconfig-querygraphqlconfig) in your
metadata. Additionally, you can customize individual queries by modifying
[the `GraphQlDefinition` metadata object](/reference/metadata-reference/models.mdx#model-modelgraphqldefinitionv2) for a
model.

## Next steps

Depending on the features enabled by a data connector, you will have access to a range of queries as defined here.

- [Simple Queries](/graphql-api/queries/simple-queries.mdx)
- [Nested Queries](/graphql-api/queries/nested-queries.mdx)
- [Sort Query Results](/graphql-api/queries/sorting.mdx)
- [Paginate Query Results](/graphql-api/queries/pagination.mdx)
- [Multiple Arguments](/graphql-api/queries/multiple-arguments.mdx)
- [Multiple Queries](/graphql-api/queries/multiple-queries.mdx)
- [Variables, Aliases, Fragments, Directives](/graphql-api/queries/variables-aliases-fragments-directives.mdx)
- [Filtering Queries](/graphql-api/queries/filters/index.mdx)
  - [Comparing values](/graphql-api/queries/filters/comparison-operators.mdx)
  - [Boolean expressions](/graphql-api/queries/filters/boolean-operators.mdx)
  - [Text](/graphql-api/queries/filters/text-search-operators.mdx)
  - [Nested objects](/graphql-api/queries/filters/nested-objects.mdx)

:::info Not sure what your connector supports?

For questions about feature support, check out the [connector reference docs](/reference/connectors/index.mdx).

:::



--- File: ../ddn-docs/docs/graphql-api/queries/simple-queries.mdx ---
# Simple Queries

---
sidebar_position: 1
sidebar_label: Simple Queries
description:
  "Boost your proficiency in Hasura's GraphQL API with this comprehensive guide on simple object queries. Learn how to
  fetch single or multiple nodes, use pagination, sorting and filtering to streamline your data fetching process and
  more."
keywords:
  - hasura
  - hasura ddn
  - graphql api
  - simple object queries
  - data fetch
  - pagination
  - filtering
  - sorting
  - gql queries
  - api data management
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Simple Object Queries

## Introduction

You can fetch a single node or multiple nodes of the same type using a simple object query.

## Fetch a list of objects

**Example:** Fetch a list of authors:

<GraphiQLIDE
  query={`query Authors {
  authors {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin"
      },
      {
        "id": 2,
        "name": "Beltran"
      },
      {
        "id": 3,
        "name": "Sidney"
      },
      {
        "id": 4,
        "name": "Anjela"
      }
    ]
  }
}`}
/>

## Fetch an object using its primary key

**Example:** Fetch an author using their primary key:

<GraphiQLIDE
  query={`query AuthorById {
  authorsById(id: 1) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authorsById": {
      "id": 1,
      "name": "Justin"
    }
  }
}`}
/>

## Fetch list of objects with pagination

**Example:** Fetch 2 articles after removing the 1st article from the result set.

<GraphiQLIDE
  query={`query TwoArticlesAfterFirst {
  articles(limit: 2, offset: 1) {
    title
    article_id
    }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "article_id": 2,
        "title": "Why Functional Programming Matters"
      },
      {
        "article_id": 3,
        "title": "The Design And Implementation Of Programming Languages"
      }
    ]
  }
}`}
/>

:::caution Warning

Without an `order_by` in `limit` queries, the results may be unpredictable.

:::

## Fetch list of objects with filtering

**Example:** Fetch a list of articles whose title contains the word "The":

<GraphiQLIDE
  query={`query ArticlesWithTitleThe {
  articles(where: {title: {_like: "The"}}) {
    title
    article_id
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "article_id": 1,
        "title": "The Next 700 Programming Languages"
      },
      {
        "article_id": 3,
        "title": "The Design And Implementation Of Programming Languages"
      }
    ]
  }
}`}
/>

## Fetch list of objects with sorting

**Example:** Fetch a list of articles with `article_id` in descending order:

<GraphiQLIDE
  query={`query ArticlesSorted {
  articles(order_by: {article_id: Desc}) {
    title
    article_id
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "article_id": 3,
        "title": "The Design And Implementation Of Programming Languages"
      },
      {
        "article_id": 2,
        "title": "Why Functional Programming Matters"
      },
      {
        "article_id": 1,
        "title": "The Next 700 Programming Languages"
      }
    ]
  }
}`}
/>

## Fetch objects using model arguments

**Example:** Fetch the articles for the given `author_id`:

<GraphiQLIDE
  query={`query ArticlesByAuthor {
  articles_by_author_id(author_id: "2") {
      article_id
      title
    }
}`}
  response={`{
  "data": [
    {
      "article_id": 2,
      "title": "Why Functional Programming Matters"
    },
    {
      "article_id": 3,
      "title": "The Design And Implementation Of Programming Languages"
    }
  ]
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/nested-queries.mdx ---
# Nested Queries

---
sidebar_label: Nested Queries
sidebar_position: 2
description:
  "Master the usage of nested queries in Hasura with comprehensive examples. Explore object and array relationships for
  fetching data of related types. Boost your performance in Hasura environment with efficient nested object queries."
keywords:
  - hasura nested queries
  - hasura object relationship
  - hasura array relationship
  - nested object queries
  - graphql queries
  - database relationships
  - fetch related data
  - hasura api
  - graphql data fetching
  - hasura tutorial
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Nested Object Queries

## Introduction

You can use the object (one-to-one) or array (one-to-many) relationships defined in your metadata to make a nested
queries, i.e. fetch data for one type along with data from a nested or related type.

## Fetch nested object using an object relationship

The following is an example of a nested object query using the **object relationship** between an article and an author.

**Example:** Fetch a list of articles and the name of each articleâ€™s author:

<GraphiQLIDE
  query={`query ArticlesAndAuthors {
  articles {
    id
    title
    author {
      name
    }
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "sit amet",
        "author": {
          "name": "Anjela"
        }
      },
      {
        "id": 2,
        "title": "a nibh",
        "author": {
          "name": "Beltran"
        }
      },
      {
        "id": 3,
        "title": "amet justo morbi",
        "author": {
          "name": "Anjela"
        }
      }
    ]
  }
}`}
/>

## Fetch nested objects using an array relationship

The following is an example of a nested object query using the **array relationship** between an author and articles.

**Example:** Fetch a list of authors and a related, nested list of each authorâ€™s articles:

<GraphiQLIDE
  query={`query AuthorsAndArticles {
  authors {
    id
    name
    articles {
      id
      title
    }
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin",
        "articles": [
          {
            "id": 15,
            "title": "vel dapibus at"
          },
          {
            "id": 16,
            "title": "sem duis aliquam"
          }
        ]
      },
      {
        "id": 2,
        "name": "Beltran",
        "articles": [
          {
            "id": 2,
            "title": "a nibh"
          },
          {
            "id": 9,
            "title": "sit amet"
          }
        ]
      },
      {
        "id": 3,
        "name": "Sidney",
        "articles": [
          {
            "id": 6,
            "title": "sapien ut"
          },
          {
            "id": 11,
            "title": "turpis eget"
          },
          {
            "id": 14,
            "title": "congue etiam justo"
          }
        ]
      }
    ]
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/aggregation-queries.mdx ---
# Aggregation queries

---
sidebar_label: Aggregation queries
sidebar_position: 3
description: Make aggregation queries with Hasura DDN.
keywords:
  - hasura
  - docs
  - postgres
  - query
  - aggregation query
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Aggregation Queries

## **Aggregate** fields

You can fetch aggregations on columns along with nodes using an aggregation query.

The **name of the aggregate field** is of the form `<field-name>Aggregate`.

Common aggregation functions are `count`, `sum`, `avg`, `max`, `min`, etc. You can see the complete specification of the
aggregate field in the [metadata reference](/reference/metadata-reference/aggregate-expressions.mdx). Note that not all
aggregation functions are available for all data types.

## Fetch aggregated data of an object

**Example:** Fetch a list of posts with aggregated data:

<GraphiQLIDE
  query={`query PostsAndAggregate {
    posts {
    id
    title
  }
  postsAggregate {
    _count
  }
}`}
  response={`{
  "data": {
    "posts": [
      {
        "id": 1,
        "title": "My First Post"
      },
      {
        "id": 2,
        "title": "Another Post"
      },
      {
        "id": 3,
        "title": "Bob's Post"
      },
      {
        "id": 4,
        "title": "Hello World"
      },
      {
        "id": 5,
        "title": "Charlie has more to say"
      }
    ],
    "postsAggregate": {
      "_count": 5
    }
  }
}`}
/>

## Fetch aggregated data on nested objects {#nested-aggregate}

The following is an example of a nested object query with aggregations on the **array relationship** between a user and
posts.

**Example:** Fetch user with an `id` of `1` and a nested list of posts with aggregated data:

<GraphiQLIDE
  query={`query ArticlesSummaryByAuthor {
  usersById(id: "1") {
    id
    name
    postsAggregate {
      _count
    }
    posts {
      id
      title
    }
  }
}`}
  response={`{
  "data": {
    "usersById": {
      "id": 1,
      "name": "Alice",
      "postsAggregate": {
        "_count": 2
      },
      "posts": [
        {
          "id": 1,
          "title": "My First Post"
        },
        {
          "id": 2,
          "title": "Another Post"
        }
      ]
    }
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/sorting.mdx ---
# Sort query results

---
sidebar_label: Sort query results
sidebar_position: 5
description:
  "Sort the results of your queries in Hasura with the 'order_by' argument. Learn how to sort objects, nested objects,
  and based on nested object's fields."
keywords:
  - hasura
  - query sorting
  - graphql queries
  - object sorting
  - nested object sorting
  - hasura sorting
  - order_by
  - graphql tutorial
  - graphql best practices
  - hasura guide
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Sort Query Results

## The **order_by** argument

Results from your query can be sorted by using the `order_by` argument. The argument can be used to sort nested objects
too.

The sort order (ascending vs. descending) is set by specifying the `Asc` or `Desc` enum value for the column name in the
`order_by` input object, e.g. `{name: Desc}`.

By default, for ascending ordering `null` values are returned at the end of the results and for descending ordering
`null` values are returned at the start of the results.

The `order_by` argument takes an array of objects to allow sorting by multiple columns.

You can also use nested objects' fields to sort the results. Only columns from object relationships** and **aggregates
from array relationships\*\* can be used for sorting.

The following are example queries for different sorting use cases:

## Sorting objects

**Example:** Fetch a list of authors sorted by their names in ascending order:

<GraphiQLIDE
  query={`query AuthorsSorted {
  authors (
    order_by: {name: Asc}
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 5,
        "name": "Amii"
      },
      {
        "id": 4,
        "name": "Anjela"
      },
      {
        "id": 8,
        "name": "April"
      },
      {
        "id": 2,
        "name": "Beltran"
      },
      {
        "id": 7,
        "name": "Berti"
      },
      {
        "id": 6,
        "name": "Corny"
      }
    ]
  }
}`}
/>

## Sorting nested objects {#pg-nested-sort}

**Example:** Fetch a list of authors sorted by their names with a list of their articles that is sorted by their rating:

<GraphiQLIDE
  query={`query AuthorsAndArticlesSorted {
  authors (order_by: {name: Asc}) {
    id
    name
    articles(order_by: {rating: Desc}) {
      id
      title
      rating
    }
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 5,
        "name": "Amii",
        "articles": [
          {
            "rating": 5,
            "id": 17,
            "title": "montes nascetur ridiculus"
          },
          {
            "rating": 3,
            "id": 12,
            "title": "volutpat quam pede"
          },
          {
            "rating": 2,
            "id": 4,
            "title": "vestibulum ac est"
          }
        ]
      },
      {
        "id": 4,
        "name": "Anjela",
        "articles": [
          {
            "rating": 4,
            "id": 3,
            "title": "amet justo morbi"
          },
          {
            "rating": 1,
            "id": 1,
            "title": "sit amet"
          }
        ]
      },
      {
        "id": 8,
        "name": "April",
        "articles": [
          {
            "rating": 4,
            "id": 13,
            "title": "vulputate elementum"
          },
          {
            "rating": 2,
            "id": 20,
            "title": "eu nibh"
          }
        ]
      }
    ]
  }
}`}
/>

## Sorting by Nested Object Fields

Only **columns from object relationships** can be used for sorting.

### For object relationships

For object relationships only columns can be used for sorting.

**Example:** Fetch a list of articles that are sorted by their author's ids in descending order:

<GraphiQLIDE
  query={`query ArticlesSortedByAuthor {
  articles (
    order_by: {author: {id: Desc}}
  ) {
    id
    rating
    published_on
    author {
      id
      name
    }
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 3,
        "title": "Article 3",
        "content": "Sample article content 3",
        "author": {
          "id": 2,
          "name": "Author 2"
        }
      },
      {
        "id": 1,
        "title": "Article 1",
        "content": "Sample article content 1",
        "author": {
          "id": 1,
          "name": "Author 1"
        }
      },
      {
        "id": 2,
        "title": "Article 2",
        "content": "Sample article content 2",
        "author": {
          "id": 1,
          "name": "Author 1"
        }
      }
    ]
  }
}`}
/>

## Sorting by multiple fields

<GraphiQLIDE
  query={`query ArticlesSortedByAuthor {
  articles (
    order_by: {author: {id: Desc, name: Desc}}
  ) {
    id
    rating
    published_on
    author {
      id
      name
    }
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 3,
        "title": "Article 3",
        "content": "Sample article content 3",
        "author": {
          "id": 2,
          "name": "Author 2"
        }
      },
      {
        "id": 1,
        "title": "Article 1",
        "content": "Sample article content 1",
        "author": {
          "id": 1,
          "name": "Author 1"
        }
      },
      {
        "id": 2,
        "title": "Article 2",
        "content": "Sample article content 2",
        "author": {
          "id": 1,
          "name": "Author 1"
        }
      }
    ]
  }
}`}
/>

:::info Key order is not preserved

Key order in input object for `order_by` is not preserved. This means you should only have a single key per object, or
you may see unexpected behavior.

:::



--- File: ../ddn-docs/docs/graphql-api/queries/pagination.mdx ---
# Paginate query results

---
sidebar_label: Paginate query results
sidebar_position: 7
description:
  "Explore how to paginate your query results using Hasura. This guide will showcase how to utilize limit and offset
  arguments to manage your data pagination, along with practical examples. Get familiar with keyset cursor-based
  pagination and learn the benefits of using the 'where' clause over 'offset'."
keywords:
  - hasura
  - data pagination
  - pagination query
  - limit
  - offset
  - graphql
  - database
  - hasura pagination
  - keyset cursor
  - hasura documentation
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Paginate Query Results

## The **limit** & **offset** arguments

The operators `limit` and `offset` are used for pagination.

`limit` specifies the number of rows to retain from the result set and `offset` determines which slice to retain from
the results.

The following are examples of different pagination scenarios:

## Limit results

**Example:** Fetch the first 5 authors from the list of all authors:

<GraphiQLIDE
  query={`query FirstFiveAuthors {
  authors(
    limit: 5
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin"
      },
      {
        "id": 2,
        "name": "Beltran"
      },
      {
        "id": 3,
        "name": "Sidney"
      },
      {
        "id": 4,
        "name": "Anjela"
      },
      {
        "id": 5,
        "name": "Amii"
      }
    ]
  }
}`}
/>

## Limit results from an offset

**Example:** Fetch 5 authors from the list of all authors, starting with the 6th one:

<GraphiQLIDE
  query={`query AuthorsFromSixth {
  authors(
    limit: 5,
    offset:5
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 6,
        "name": "Corny"
      },
      {
        "id": 7,
        "name": "Berti"
      },
      {
        "id": 8,
        "name": "April"
      },
      {
        "id": 9,
        "name": "Ninnetta"
      },
      {
        "id": 10,
        "name": "Lyndsay"
      }
    ]
  }
}`}
/>

## Limit results in a nested object {#pg-nested-paginate}

**Example:** Fetch a list of authors and a list of their first 2 articles:

<GraphiQLIDE
  query={`query AuthorsAndArticles {
  authors {
    id
    name
    articles (
      limit: 2
      offset: 0
    ) {
      id
      title
    }
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin",
        "articles": [
          {
            "id": 15,
            "title": "vel dapibus at"
          },
          {
            "id": 16,
            "title": "sem duis aliquam"
          }
        ]
      },
      {
        "id": 2,
        "name": "Beltran",
        "articles": [
          {
            "id": 2,
            "title": "a nibh"
          },
          {
            "id": 9,
            "title": "sit amet"
          }
        ]
      },
      {
        "id": 3,
        "name": "Sidney",
        "articles": [
          {
            "id": 6,
            "title": "sapien ut"
          },
          {
            "id": 11,
            "title": "turpis eget"
          }
        ]
      },
      {
        "id": 4,
        "name": "Anjela",
        "articles": [
          {
            "id": 1,
            "title": "sit amet"
          },
          {
            "id": 3,
            "title": "amet justo morbi"
          }
        ]
      }
    ]
  }
}`}
/>

## Keyset cursor based pagination

Cursors are used to traverse across rows of a dataset. They work by returning a pointer to a specific row which can then
be used to fetch the next batch of data.

Keyset cursors are a column (or a set of columns) of the data that are used as the cursor. The column(s) used as the
cursor must be unique and sequential. This ensures that data is read after a specific row rather than relying on the
position of the row in the dataset as done by `offset`, and that duplicate records are not fetched again.

**For example**, consider the following query to fetch a list of authors with a `where` clause used in place of
`offset`:

<GraphiQLIDE
  query={`query AuthorsGreaterThanFive {
  authors(
    limit: 5,
    where: { id: {_gt: 5} }
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 6,
        "name": "Corny"
      },
      {
        "id": 7,
        "name": "Berti"
      },
      {
        "id": 8,
        "name": "April"
      },
      {
        "id": 9,
        "name": "Ninnetta"
      },
      {
        "id": 10,
        "name": "Lyndsay"
      }
    ]
  }
}`}
/>

Here we are fetching authors where the value of `id` is greater than 5. This will always skip the previously fetched
results which would have been ids 1 to 5, ensuring no duplicate results. Column `id` is acting as the cursor here,
unique and sequential.

The choice of cursor columns depends on the order of the expected results i.e. if the query has an `order_by` clause,
the column(s) used in the `order_by` need to be used as the cursor.

Columns such as `id` (auto-incrementing integer/big integer) or `created_at` (timestamp) are commonly used as cursors
when an order is not explicit, as they should be unique and sequential.

:::info Where vs Offset

Keyset cursor based pagination using `where` is more performant than using `offset` because we can leverage database
indexes on the columns that are being used as cursors.

:::

:::info No order_by clause

Because we ran the above example without an `order_by` clause, it is accidental that we received those results. Running
a query without an `order_by` clause will return results in an arbitrary order.

:::



--- File: ../ddn-docs/docs/graphql-api/queries/multiple-arguments.mdx ---
# Use multiple arguments

---
sidebar_label: Use multiple arguments
sidebar_position: 9
description:
  "Extend your Hasura usage by learning how to utilize multiple arguments in a single query. This guide breaks down
  filtering and sorting processes using multiple arguments, enhancing database efficiency and data management."
keywords:
  - hasura multiple arguments
  - graphql query
  - hasura tutorial
  - graphql data filtering
  - graphql data sorting
  - hasura api
  - database efficiency
  - sophisticated data handling
  - hasura guide
  - query arguments
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Use Multiple Query Arguments

Multiple arguments can be used together in the same query.

For example, you can use the `where` argument to filter the results and then use the `order_by` argument to sort them.

**For example**, fetch a list of authors and only 2 of their published articles that are sorted by their date of
\*publication:

<GraphiQLIDE
  query={`query AuthorsAndArticles {
  authors {
    id
    name
    articles(
      where: {is_published: {_eq: true}},
      order_by: {published_on: desc},
      limit: 2
    ) {
      id
      title
      is_published
      published_on
    }
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin",
        "articles": [
          {
            "is_published": true,
            "id": 16,
            "title": "sem duis aliquam",
            "published_on": "2018-02-14"
          },
          {
            "is_published": true,
            "id": 15,
            "title": "vel dapibus at",
            "published_on": "2018-01-02"
          }
        ]
      },
      {
        "id": 2,
        "name": "Beltran",
        "articles": [
          {
            "is_published": true,
            "id": 2,
            "title": "a nibh",
            "published_on": "2018-06-10"
          },
          {
            "is_published": true,
            "id": 9,
            "title": "sit amet",
            "published_on": "2017-05-16"
          }
        ]
      },
      {
        "id": 3,
        "name": "Sidney",
        "articles": [
          {
            "is_published": true,
            "id": 6,
            "title": "sapien ut",
            "published_on": "2018-01-08"
          },
          {
            "is_published": true,
            "id": 11,
            "title": "turpis eget",
            "published_on": "2017-04-14"
          }
        ]
      },
      {
        "id": 4,
        "name": "Anjela",
        "articles": [
          {
            "is_published": true,
            "id": 1,
            "title": "sit amet",
            "published_on": "2017-08-09"
          },
          {
            "is_published": true,
            "id": 3,
            "title": "amet justo morbi",
            "published_on": "2017-05-26"
          }
        ]
      }
    ]
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/multiple-queries.mdx ---
# Multiple queries in a request

---
sidebar_label: Multiple queries in a request
sidebar_position: 10
description:
  "Explore how to run multiple GraphQL queries in a single request using Hasura. Learn about the processes of sequential
  execution and fetching unrelated data types simultaneously for efficient data handling and improved API performance."
keywords:
  - hasura graphql
  - multiple graphql queries
  - simultaneous data fetching
  - sequential query execution
  - graphql api requests
  - efficient data handling
  - api performance optimization
  - hasura features
  - graphql api development
  - hasura tutorials
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Multiple Queries in a Request

## Execution

You can fetch objects of different unrelated types in the same query.

## Run multiple top level queries in the same request

**For example**, fetch a list of `authors` and a list of `articles`:

<GraphiQLIDE
  query={`query AuthorsAndArticles {
  authors(limit: 2) {
    id
    name
  }
  articles(limit: 2) {
    id
    title
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin"
      },
      {
        "id": 2,
        "name": "Beltran"
      }
    ],
    "articles": [
      {
        "id": 1,
        "title": "sit amet"
      },
      {
        "id": 2,
        "title": "a nibh"
      }
    ]
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/variables-aliases-fragments-directives.mdx ---
# Use variables / aliases / fragments

---
sidebar_label: Use variables / aliases / fragments
sidebar_position: 11
description:
  "Explore the use of variables, aliases, and fragments in Hasura queries. Learn to create dynamic and reusable queries,
  fetch data based on query parameters, and modify your GraphQL query results."
keywords:
  - hasura ddn
  - graphql variables
  - graphql aliases
  - graphql fragments
  - graphql directives
  - api query optimization
  - dynamic query creation
  - query reuse
  - data fetching
  - graphql queries
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Use Variables / Aliases / Fragments in Queries

## Using variables

In order to make a query re-usable, it can be made dynamic by using variables.

**Example:** Fetch an author by their `author_id`:

<GraphiQLIDE
  query={`query getArticles($author_id: Int!) {
  articles(
    where: { author_id: { _eq: $author_id } }
  ) {
    id
    title
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 15,
        "title": "How to climb Mount Everest"
      },
      {
        "id": 6,
        "title": "How to be successful on broadway"
      }
    ]
  }
}`}
  variables={`{
  "author_id": 1
}`}
/>

## Using aliases

Aliases can be used to return objects with a different name than their field name. This is especially useful while
fetching the same type of objects with different arguments in the same query.

**Example:** First, fetch all articles. Second, fetch the two top-rated articles. Third, fetch the worst-rated article:

<GraphiQLIDE
  query={`query getArticles {
  articles {
    title
    rating
  }
  topTwoArticles: articles(
    order_by: {rating: desc},
    limit: 2
  ) {
    title
    rating
  }
  worstArticle: articles(
    order_by: {rating: asc},
    limit: 1
  ) {
    title
    rating
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "title": "How to climb Mount Everest",
        "rating": 4
      },
      {
        "title": "How to be successful on broadway",
        "rating": 20
      },
      {
        "title": "How to make fajitas",
        "rating": 6
      }
    ],
    "topTwoArticles": [
      {
        "title": "How to be successful on broadway",
        "rating": 20
      },
      {
        "title": "How to make fajitas",
        "rating": 6
      }
    ],
    "worstArticle": [
      {
        "title": "How to climb Mount Everest",
        "rating": 4
      }
    ]
  }
}`}
/>

## Using fragments

Sometimes, queries can get long and confusing. A fragment is a set of fields with any chosen name. This fragment can
then be used to represent the defined set.

**Example:** Creating a fragment for a set of `article` fields (`id` and `title`) and using it in a query:

<GraphiQLIDE
  query={`fragment articleFields on articles {
  id
  title
}
query getArticles {
  articles {
    ...articleFields
  }
  topTwoArticles: articles(
    order_by: {rating: desc},
    limit: 2
  ) {
    ...articleFields
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 3,
        "title": "How to make fajitas"
      },
      {
        "id": 15,
        "title": "How to climb Mount Everest"
      },
      {
        "id": 6,
        "title": "How to be successful on broadway"
      }
    ],
    "topTwoArticles": [
      {
        "id": 6,
        "title": "How to be successful on broadway"
      },
      {
        "id": 3,
        "title": "How to make fajitas"
      }
    ]
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/filters/index.mdx ---
# ../ddn-docs/docs/graphql-api/queries/filters/index.mdx

---
description:
  "Master the process of filtering query results using Hasura with this comprehensive guide. Learn the syntax, use of
  arguments, operators and more to build search queries and filter data. Cover filtering based on field values, nested
  objects, and using comparison, logical, and text search operators for refined results."
keywords:
  - hasura search
  - query filtering
  - graphql search
  - hasura tutorial
  - hasura arguments
  - hasura operators
  - hasura nested objects
  - hasura syntax
  - advanced filtering
  - hasura graphql
slug: index
seoFrontMatterUpdated: true
---

# Filter Query Results / Search Queries

## Introduction

Hasura provides a powerful yet simple syntax to filter query results. This is useful for building search queries or
filtering data based on some criteria. You can utilize arguments and operators to filter results based on equality,
comparison, pattern matching, etc.

## The **where** argument

You can use the `where` argument in your queries to filter results based on some fieldâ€™s values (even nested objects'
fields). You can even use multiple filters in the same `where` clause using the `_and` or the `_or` operators.

For example, to fetch data for an author whose name is "Sidney":

```graphql {3}
query {
  authors(where: { name: { _eq: "Sidney" } }) {
    id
    name
  }
}
```

You can also use nested objects' fields to filter rows from a table and also filter the nested objects as well.

For example, to fetch a list of authors who have articles with a rating greater than 4 along with those articles:

```graphql {2,5}
query {
  authors(where: { articles: { rating: { _gt: 4 } } }) {
    id
    name
    articles(where: { rating: { _gt: 4 } }) {
      id
      title
      rating
    }
  }
}
```

Here `_eq` and `_gt` are examples of comparison operators that can be used in the `where` argument to filter on
equality.

## Supported operators

| Operator                                                                             | Use case                                                                                                   |
| ------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------- |
| [Simple Comparison Operators](/graphql-api/queries/filters/comparison-operators.mdx) | Utilize comparison operators to selectively filter results by evaluating a field against a specific value. |
| [Boolean Operators](/graphql-api/queries/filters/boolean-operators.mdx)              | Employ boolean operators to refine result filters based on logical expressions.                            |
| [Text Search Operators](/graphql-api/queries/filters/text-search-operators.mdx)      | Apply text search operators to narrow down results according to the presence of text in a field.           |
| [Nested Objects](/graphql-api/queries/filters/nested-objects.mdx)                    | Navigate and filter results using nested object structures for advanced filtering.                         |



--- File: ../ddn-docs/docs/graphql-api/queries/filters/comparison-operators.mdx ---
# Filter by comparing values

---
sidebar_label: Filter by comparing values
sidebar_position: 1
description:
  "This documentation page provides a comprehensive overview on how to use comparison operators, such as equality and
  varying degrees of greater or less than operators, to filter and refine your Hasura API data queries. Topical examples
  are included for a variety of data types, including integers, strings, and booleans."
keywords:
  - hasura
  - data filtering
  - comparison operators
  - graphql queries
  - api data
  - hasura ddn
  - equality operators
  - comparison filtering
  - graphql api
  - data querying
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Filter by Comparing Values

## Introduction

Comparison operators are used to compare values of the same type. For example, to compare two numbers, two strings, two
dates, etc.

## Equality operators (\_eq, \_neq)

The `_eq` (equal to) or the `_neq` (not equal to) operators are compatible with any type other than `json` or `jsonB`
(like `Integer`, `Float`, `Double`, `Text`, `Boolean`, `Date`/`Time`/`Timestamp`, etc.).

[//]: # '[//]: # "For more details on equality operators and PostgreSQL equivalents, refer to the"'
[//]: # '[//]: # "[API reference](/api-reference/graphql-api/query.mdx#generic-operators)."'

The following are examples of using the equality operators on different types.

**Example: Integer (works with Double, Float, Numeric, etc.)**

Fetch data about an author whose `id` _(an integer field)_ is equal to 3:

<GraphiQLIDE
  query={`query AuthorById {
  authors(
    where: {id: {_eq: 3}}
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 3,
        "name": "Sidney"
      }
    ]
  }
}`}
/>

**Example: String or Text**

Fetch a list of authors with `name` _(a text field)_ as "Sidney":

<GraphiQLIDE
  query={`query AuthorByName {
  authors(
    where: {name: {_eq: "Sidney"}}
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 3,
        "name": "Sidney"
      }
    ]
  }
}`}
/>

**Example: Boolean**

Fetch a list of articles that have not been published (`is_published` is a boolean field):

<GraphiQLIDE
  query={`query UnpublishedArticles {
  articles(
    where: {is_published: {_eq: false}}
  ) {
    id
    title
    is_published
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 5,
        "title": "ut blandit",
        "is_published": false
      },
      {
        "id": 8,
        "title": "donec semper sapien",
        "is_published": false
      },
      {
        "id": 10,
        "title": "dui proin leo",
        "is_published": false
      },
      {
        "id": 14,
        "title": "congue etiam justo",
        "is_published": false
      }
    ]
  }
}`}
/>

**Example: Date (works with Time, Timezone, etc.)**

Fetch a list of articles that were published on a certain date (`published_on` is a Date field):

<GraphiQLIDE
  query={`query ArticlesByDate {
  articles(
    where: {published_on: {_eq: "2017-05-26"}}
  ) {
    id
    title
    published_on
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 3,
        "title": "amet justo morbi",
        "published_on": "2017-05-26"
      }
    ]
  }
}`}
/>

**Example: Integer (works with Integer, Float, Double, etc.)**

Fetch a list of users whose age is _not_ 30 (`age` is an Integer field):

<GraphiQLIDE
  query={`query UsersNotAge30 {
  users(where: { age: { _neq: 30 } }) {
    id
    name
    age
  }
}`}
  response={`{
  "data": {
    "users": [
      {
        "id": 1,
        "name": "John",
        "age": 20
      },
      {
        "id": 2,
        "name": "Jane",
        "age": 25
      },
      {
        "id": 4,
        "name": "Bob",
        "age": 40
      }
    ]
  }`}
/>

:::info Caveat for "null" values

By design, the `_eq` or `_neq` operators will not return rows with `null` values.

To also return rows with `null` values, the `_is_null` operator needs to be used along with these joined by the `_or`
operator.

For example, to fetch a list of articles where the `is_published` column is either `false` or `null`:

<GraphiQLIDE
  query={`query UnpublishedArticles {
  articles (
    where: {
      _or: [
        {is_published: {_eq: false}},
        {is_published: {_is_null: true}}
      ]
    }
  )
  {
    id
    title
    is_published
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "Robben Island",
        "is_published": false
      },
      {
        "id": 2,
        "title": "The Life of Matthias",
        "is_published": false
      },
      {
        "id": 3,
        "title": "All about Hasura",
        "is_published": null
      },
    ]
  }
}`}
/>

:::

## Greater than or less than operators (\_gt, \_lt, \_gte, \_lte)

The `_gt` (greater than), `_lt` (less than), `_gte` (greater than or equal to), `_lte` (less than or equal to) operators
are compatible with any type other than `json` or `jsonB` (like `Integer`, `Float`, `Double`, `Text`, `Boolean`,
`Date`/`Time`/`Timestamp`, etc.).

[//]: # '[//]: # "For more details on greater than or less than operators and PostgreSQL equivalents, refer to the"'
[//]: # '[//]: # "[API reference](/api-reference/graphql-api/query.mdx#generic-operators)."'

The following are examples of using these operators on different types:

**Example: Integer (works with Double, Float, Numeric, etc.)**

This query retrieves all users whose age is less than 30. The `_lt` operator is a comparison operator that means "less
than". It is used to filter records based on a specified value.

<GraphiQLIDE
  query={`query UsersAgeLessThan30 {
  users(where: { age: { _lt: 30 }}) {
    id
    name
    age
  }
}`}
  response={`{
  "data": {
    "users": [
      {
        "id": 1,
        "name": "John",
        "age": 25
      },
      {
        "id": 2,
        "name": "Jane",
        "age": 28
      }
    ]
  }
}`}
/>

**Example: String or Text**

Fetch a list of authors whose names begin with M or any letter that follows M _(essentially, a filter based on a
dictionary sort)_:

<GraphiQLIDE
  query={`query AuthorsNameGreaterThanM {
  authors(
    where: {name: {_gt: "M"}}
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 3,
        "name": "Sidney"
      },
      {
        "id": 9,
        "name": "Ninnetta"
      }
    ]
  }
}`}
/>

**Example: Integer (works with Double, Float, etc.)**

Fetch a list of all products with a price less than or equal to 10.

<GraphiQLIDE
  query={`query ProductsPriceLessThanOrEqualTo10 {
  products(where: { price: { _lte: 10 } }) {
    name
    price
  }
}`}
  response={`{
  "data": {
    "products": [
      {
        "name": "Bread",
        "price": 2.5
      },
      {
        "name": "Milk",
        "price": 3.5
      },
      {
        "name": "Eggs",
        "price": 4.5
      }
    ]
  }
}`}
/>

**Example: Integer (works with Double, Float, etc.)**

Fetch a list of articles rated 4 or more (`rating` is an integer field):

<GraphiQLIDE
  query={`query ArticlesRatingGTE4 {
  articles(
    where: {rating: {_gte: 4}}
  ) {
    id
    title
    rating
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 3,
        "title": "amet justo morbi",
        "rating": 4
      },
      {
        "id": 7,
        "title": "nisl duis ac",
        "rating": 4
      },
      {
        "id": 17,
        "title": "montes nascetur ridiculus",
        "rating": 5
      }
    ]
  }
}`}
/>

**Example: Date (works with Time, Timezone, etc.)**

Fetch a list of articles that were published on or after date "01/01/2018":

<GraphiQLIDE
  query={`query ArticlesPublishedAfterDate {
  articles(
    where: {published_on: {_gte: "2018-01-01"}}
  ) {
    id
    title
    published_on
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 2,
        "title": "a nibh",
        "published_on": "2018-06-10"
      },
      {
        "id": 6,
        "title": "sapien ut",
        "published_on": "2018-01-08"
      },
      {
        "id": 13,
        "title": "vulputate elementum",
        "published_on": "2018-03-10"
      },
      {
        "id": 15,
        "title": "vel dapibus at",
        "published_on": "2018-01-02"
      }
    ]
  }
}`}
/>

## List based search operators (\_in)

The `_in` (in a list) operator is used to compare field values to a list of values. They are compatible with any type
other than `json` or `jsonB` (like `Integer`, `Float`, `Double`, `Text`, `Boolean`, `Date`/`Time`/`Timestamp`, etc.).

The following are examples of using these operators on different types:

**Example: Integer (works with Double, Float, etc.)**

Fetch a list of articles rated 1, 3 or 5:

<GraphiQLIDE
  query={`query ArticlesRating1or3or5 {
  articles(
    where: {rating: {_in: [1,3,5]}}
  ) {
    id
    title
    rating
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "sit amet",
        "rating": 1
      },
      {
        "id": 2,
        "title": "a nibh",
        "rating": 3
      },
      {
        "id": 6,
        "title": "sapien ut",
        "rating": 1
      },
      {
        "id": 17,
        "title": "montes nascetur ridiculus",
        "rating": 5
      }
    ]
  }
}`}
/>

## Filter or check for null values (\_is_null)

Checking for null values can be achieved using the `_is_null` operator.

**Example: Filter null values in a field**

Fetch a list of articles that have a value in the `published_on` field:

<GraphiQLIDE
  query={`query PublishedArticles {
  articles(
    where: {published_on: {_is_null: false}}
  ) {
    id
    title
    published_on
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "sit amet",
        "published_on": "2017-08-09"
      },
      {
        "id": 2,
        "title": "a nibh",
        "published_on": "2018-06-10"
      },
      {
        "id": 3,
        "title": "amet justo morbi",
        "published_on": "2017-05-26"
      },
      {
        "id": 4,
        "title": "vestibulum ac est",
        "published_on": "2017-03-05"
      }
    ]
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/filters/boolean-operators.mdx ---
# Filter by boolean expressions

---
sidebar_label: Filter by boolean expressions
sidebar_position: 2
description:
  "Discover how to build complex boolean expressions for Hasura-related query filtering in API data. This guide includes
  examples of using '_not', '_and', and, '_or' operators for improved data filtering."
keywords:
  - hasura query
  - graphql api data
  - boolean expressions
  - filter database query
  - api data filtering
  - boolean operators
  - hasura tutorial
  - query optimization
  - graphql
  - hasura data management
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Filter by Boolean Expressions

## Filter based on failure of some criteria (\_not)

The `_not` operator can be used to fetch results for which some condition does not hold true. i.e. to invert the filter
set for a condition.

**Example: \_not**

Fetch all authors who don't have any published articles:

<GraphiQLIDE
  query={`{
  authors(
    where: {
      _not: {
        articles: { is_published: {_eq: true} }
      }
    }) {
    id
    name
    articles {
      title
      is_published
    }
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 7,
        "name": "Berti",
        "articles": [
          {
            "title": "ipsum primis in",
            "is_published": false
          }
        ]
      },
      {
        "id": 9,
        "name": "Ninnetta",
        "articles": []
      },
      {
        "id": 10,
        "name": "Lyndsay",
        "articles": [
          {
            "title": "dui proin leo",
            "is_published": false
          }
        ]
      }
    ]
  }
}`}
/>

## Using multiple filters in the same query (\_and, \_or)

You can group multiple parameters in the same `where` argument using the `_and` or the `_or` operators to filter results
based on more than one criterion.

:::info Note

You can use the `_or` and `_and` operators along with the `_not` operator to create arbitrarily complex boolean
expressions involving multiple filtering criteria.

:::

**Example: \_and**

Fetch a list of articles published in a specific time-frame (for example: in year 2017):

<GraphiQLIDE
  query={`query ArticlesPublishedInCertainYear {
  articles (
    where: {
      _and: [
        { published_on: {_gte: "2017-01-01"}},
        { published_on: {_lte: "2017-12-31"}}
      ]
    }
  )
  {
    id
    title
    published_on
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "sit amet",
        "published_on": "2017-08-09"
      },
      {
        "id": 3,
        "title": "amet justo morbi",
        "published_on": "2017-05-26"
      },
      {
        "id": 4,
        "title": "vestibulum ac est",
        "published_on": "2017-03-05"
      },
      {
        "id": 9,
        "title": "sit amet",
        "published_on": "2017-05-16"
      }
    ]
  }
}`}
/>

:::info Note

Certain `_and` expressions can be expressed in a simpler format using some syntactic sugar.

[//]: # '[//]: # "See the [API reference](/api-reference/graphql-api/query.mdx#andexp) for more details."'

:::

**Example: \_or**

Fetch a list of articles rated more than 4 or published after "01/01/2018":

<GraphiQLIDE
  query={`query ArticlesPublishedAfterCertainDateAndRatedMoreThanFour {
  articles (
    where: {
      _or: [
        {rating: {_gte: 4}},
        {published_on: {_gte: "2018-01-01"}}
      ]
    }
  )
  {
    id
    title
    rating
    published_on
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 2,
        "title": "a nibh",
        "rating": 3,
        "published_on": "2018-06-10"
      },
      {
        "id": 3,
        "title": "amet justo morbi",
        "rating": 4,
        "published_on": "2017-05-26"
      },
      {
        "id": 6,
        "title": "sapien ut",
        "rating": 1,
        "published_on": "2018-01-08"
      },
      {
        "id": 7,
        "title": "nisl duis ac",
        "rating": 4,
        "published_on": "2016-07-09"
      }
    ]
  }
}`}
/>



--- File: ../ddn-docs/docs/graphql-api/queries/filters/text-search-operators.mdx ---
# Filter by text

---
sidebar_label: Filter by text
sidebar_position: 3
description:
  "Explore various operators for pattern matching on string/text fields with Hasura GraphQL engine, including _like,
  _nlike, _ilike, _nilike, _similar, _nsimilar, _regex, _nregex, _iregex, and _niregex. Learn how to use these operators
  to filter query results and conduct text search."
keywords:
  - hasura graphql engine
  - pattern matching
  - graphql queries
  - text search
  - graphql filtering
  - hasura operators
  - text filtering
  - api development
  - data manipulation
  - query optimization
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Filter by Text

## Introduction

The `_like`, `_nlike`, `_ilike`, `_nilike`, `_similar`, `_nsimilar`, `_regex`, `_nregex`, `_iregex`, `_niregex`
operators are used for pattern matching on string/text fields.

[//]: # '[//]: # "For more details on text search operators and PostgreSQL equivalents, refer to the"'
[//]: # '[//]: # "[API reference](/api-reference/graphql-api/query.mdx#text-operators)."'

## \_like

Fetch a list of articles whose titles contain the word â€œametâ€:

<GraphiQLIDE
  query={`query ArticlesWithAmetInTitle {
  articles(
    where: {title: {_like: "%amet%"}}
  ) {
    id
    title
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "sit amet"
      },
      {
        "id": 3,
        "title": "amet justo morbi"
      },
      {
        "id": 9,
        "title": "sit amet"
      }
    ]
  }
}`}
/>

## \_ilike

This query will return all users whose name contains the string "john", regardless of case.

<GraphiQLIDE
  query={`query UsersWithNameLike {
  users(where: { name: { _ilike: "%john%" } }) {
    id
    name
  }
}
`}
  response={`
  {
    "data": {
      "users": [
        {
          "id": 1,
          "name": "John Doe"
        },
        {
          "id": 2,
          "name": "John Smith"
        }
      ]
  }
  `}
/>

## \_nilike

This query would return all users whose name does not contain the string "John".

<GraphiQLIDE
  query={`query UsersWithNameNotLike {
  users(where: { name: { _nilike: "%John%" } }) {
    name
  }
}
`}
  response={`
  {
    "data": {
      "users": [
        {
          "name": "Jane Doe"
        },
        {
          "name": "Jane Smith"
        }
      ]
  }
  `}
/>

:::info Note

`_like` is case-sensitive. Use `_ilike` for case-insensitive search.

:::

## \_similar

Fetch a list of authors whose names begin with A or C:

<GraphiQLIDE
  query={`query AuthorsWithAorC {
  authors(
    where: {name: {_similar: "(A|C)%"}}
  ) {
    id
    name
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 4,
        "name": "Anjela"
      },
      {
        "id": 5,
        "name": "Amii"
      },
      {
        "id": 6,
        "name": "Corny"
      },
      {
        "id": 8,
        "name": "April"
      }
    ]
  }
}`}
/>

## \_nsimilar

Fetch a list of authors whose names do not begin with A or C:

<GraphiQLIDE
  query={`query AuthorsNotWithAorC {
  authors(
    where: {name: {_nsimilar: "(A|C)%"}}
  ) {
    id
    name
  }
}
  `}
  response={`
  {
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Justin"
      },
      {
        "id": 2,
        "name": "Beltran"
      },
      {
        "id": 3,
        "name": "Sidney"
      },
      {
        "id": 7,
        "name": "Dorothy"
      }
    ]
  }
  `}
/>

:::info Note

`_similar` and `_nsimilar` are case-sensitive.

:::

## \_regex

Fetch a list of articles whose titles match the regex `[ae]met`:

<GraphiQLIDE
  query={`query ArticlesWithRegex {
  articles(
    where: {title: {_regex: "[ae]met"}}
  ) {
    id
    title
  }
}`}
  response={`{
  "data": {
    "articles": [
      {
        "id": 1,
        "title": "sit amet"
      },
      {
        "id": 3,
        "title": "cremet justo morbi"
      },
      {
        "id": 9,
        "title": "sit ametist"
      }
    ]
  }
}`}
/>

## \_iregex

This query will return all users whose name matches the regular expression `/^joh?n$/i`, which matches "John" and "Jon".

<GraphiQLIDE
  query={`query UsersWithRegex {
  users(where: { name: { _iregex: "/^joh?n$/i" } }) {
    id
    name
  }
}`}
  response={`{
    "data": {
    "users": [
      {
        "id": 1,
        "name": "John Doe"
      },
      {
        "id": 2,
        "name": "Jon Smith"
      }
    ]
  }`}
/>

## \_nregex

The \_nregex operator in this GraphQL query is a negated regular expression filter that matches all users whose names do
not start with the letter "J".

<GraphiQLIDE
  query={`query UsersWithNRegex {
  users(where: { name: { _nregex: "/^J/" } }) {
    id
    name
  }
}`}
  response={`
  {
    "data": {
      "users": [
        {
          "id": 3,
          "name": "Tom Smith"
        },
        {
          "id": 4,
          "name": "Alan Doe"
        }
      ]
  }
  `}
/>

:::info Note

`_regex` is case-sensitive. Use `_iregex` for case-insensitive search.

:::

:::info Note

`regex` operators are supported in `v2.0.0` and above

:::



--- File: ../ddn-docs/docs/graphql-api/queries/filters/nested-objects.mdx ---
# Filter based on nested fields

---
sidebar_label: Filter based on nested fields
sidebar_position: 7
description:
  "Acquire proficiency use and filter query results with Hasura's nested objects fields. This guide provides explicit
  methodologies, practical examples, and common use-cases."
keywords:
  - hasura filters
  - hasura nested objects
  - graphql nesting
  - data querying
  - local relationships
  - database relationships
  - graphql tutorial
  - api management
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Filter Based on Nested Objects Fields

## Introduction

You can filter your query results by specifying conditions of nested fields of an object.

For example:

```graphql {2}
query {
  articles(where: { author: { name: { _eq: "Sam Jones" } } }) {
    id
    title
  }
}
```

## Applicable Fields for Nested Field Comparisons

Nested field comparisons can be made in the GraphQL API if:

- The field's type is an [Object Type](reference/metadata-reference/types.mdx#objecttype-objecttype)
- The field is a [Relationship](reference/metadata-reference/relationships.mdx)

## Filtering by Object Type Fields

To filter results based on a field within an object type, apply a condition to the nested field.

**Example:**

<GraphiQLIDE
  query={`{
  users (
    where: {
      profile: {
        age: { _gt: 30 }
      }
    }
  ) {
    id
    name
    profile {
      age
    }
  }
}`}
  response={`{
  "data": {
    "users": [
      {
        "id": 1,
        "name": "John Doe",
        "profile": {
          "age": 35
        }
      },
      {
        "id": 2,
        "name": "Jane Smith",
        "profile": {
          "age": 40
        }
      }
    ]
  }
}`}
/>

In this query, `profile` is an [Object Type](reference/metadata-reference/types.mdx#objecttype-objecttype) field, and we
filter users based on the condition that their `profile.age` is greater than 30.

## Filtering by Relationship Fields

You can filter results based on conditions applied to the fields of the target model in a relationship. The relationship
can be either an object or array type.

**Example:** Filter by `Array` relationship fields.

<GraphiQLIDE
  query={`{
  authors (
    where: {
      books: {
        title: { _like: "graphql" }
      }
    }
  ) {
    id
    name
    books {
      title
    }
  }
}`}
  response={`{
  "data": {
    "authors": [
      {
        "id": 1,
        "name": "Alice Johnson",
        "books": [
          {
            "title": "GraphQL Basics"
          }
        ]
      },
      {
        "id": 3,
        "name": "Bob Lee",
        "books": [
          {
            "title": "GraphQL Advanced"
          }
        ]
      }
    ]
  }
}`}
/>

In this query, `books` is an Array relationship, and we filter authors who have written a book titled "GraphQL Basics".

**Example:** Filter by `Object` relationship fields.

<GraphiQLIDE
  query={`{
  books (
    where: {
      author: {
        name: { _eq: "Alice Johnson" }
      }
    }
  ) {
    id
    title
    author {
      name
    }
  }
}`}
  response={`{
  "data": {
    "books": [
      {
        "id": 1,
        "title": "Learning GraphQL",
        "author": {
          "name": "Alice Johnson"
        }
      },
      {
        "id": 2,
        "title": "Advanced GraphQL",
        "author": {
          "name": "Alice Johnson"
        }
      }
    ]
  }
}`}
/>

In this query, `author` is an object relationship field, and we filter books where the `author.name` is "Alice Johnson".

### Data Connector Capability

For relationship filtering to be processed efficiently, the data connector must support the `relation_comparisons`
[relationships capability](https://hasura.github.io/ndc-spec/specification/capabilities.html). This allows the query
engine to push down relationship comparisons directly to the data connector, ensuring that filtering based on
relationships is handled at the data source level, improving performance and reducing unnecessary data transfer.

However, if the `relation_comparisons` capability is absent, the query engine will handle the relationship comparisons
at its own layer. In this case, the query engine fetches the relevant mapping field values of the relationship and
constructs the necessary comparison expressions. While this ensures that the query can still execute, it may result in
reduced efficiency and slower responses, as more data needs to be transferred to the query engine for evaluation. See
[Performance of Relationship Comparisons](graphql-api/queries/filters/performance-relationship-comparisons.mdx) for more
details.

#### Compatibility Date

To utilize relationship comparisons without the `relation_comparisons` capability, you must update the compatibility
date. For detailed instructions, please refer to the
[Compatibility Config](reference/metadata-reference/compatibility-config.mdx#enable-relationships-in-predicates-to-be-used-even-if-the-data-connector-does-not-support-it).

### Remote Relationships

A relationship between two entities from different data connectors is known as a **remote relationship**. This type of
relationship allows you to integrate and query data across multiple sources seamlessly. Filtering with a predicate from
a model in a remote data source using remote relationships enhances query flexibility, enabling precise data retrieval
directly on the server. This eliminates the need for client-side filtering, which not only simplifies your code but also
reduces network data transfer, leading to improved application performance.

For example:

```graphql {2}
query UsersCompletedOrders {
  users(where: { orders: { status: { _eq: "complete" } } }) {
    id
    name
    orders {
      id
      status
    }
  }
}
```

This query retrieves a list of users along with their completed orders. Here, users and orders stored in separate data
sources.

:::info Limitations

- Only remote relationships where the targets are
  [models](reference/metadata-reference/relationships.mdx#object-type-to-a-model) are supported in comparisons, with
  support for remote relationships targeting
  [commands](reference/metadata-reference/relationships.mdx#object-type-to-a-command) coming soon.

- Remote relationships defined across subgraphs are currently not supported.

- Fields involved in
  [relationship mapping](reference/metadata-reference/relationships.mdx#relationship-relationshipmapping) must be backed
  by data connector columns that have support for the `equal` comparison operator.

:::

#### Understanding Predicate Resolution

The predicate of a remote relationship is resolved independently of the data connector's `relation_comparisons`
capability. When a remote relationship is used in a comparison, the engine retrieves the relevant data from the remote
model and constructs the necessary comparison expressions to filter the results, similar to how local relationships are
resolved without the [`relation_comparisons`](#data-connector-capability) capability.

**Consequently, the performance of remote predicates can vary significantly based on the efficiency of the underlying
data sources and the complexity of the relationships being queried. For more details, see
[Performance of Relationship Comparisons](graphql-api/queries/filters/performance-relationship-comparisons.mdx).**



--- File: ../ddn-docs/docs/graphql-api/queries/filters/performance-relationship-comparisons.mdx ---
# Performance of relationship comparisons

---
sidebar_label: Performance of relationship comparisons
sidebar_position: 9
description: "Understand and evaluate the query performance when relationship are involved in filter predicates."
keywords:
  - hasura filters
  - filter performance
  - relationship comparisons
  - predicate pushdown
  - graphql filters
  - graphql performance
---

import Thumbnail from "@site/src/components/Thumbnail";

# Performance of Relationship Comparisons

Relationship comparisons in GraphQL allow for powerful query filtering across related data. However, performance varies
in Hasura DDN depending on how these comparisons are processed.

This page explains the performance implications of relationship comparisons, detailing both optimized query execution
through data connectors and the fallback mechanisms at the query engine level when certain capabilities are unavailable
or when handling remote relationships.

## How Relationship Comparisons Work

Relationship comparisons filter data based on fields in related objects. For example, filtering books by their author's
name is a common relationship comparison:

```graphql {2}
query {
  books(where: { author: { name: { _eq: "Alice Johnson" } } }) {
    id
    title
    author {
      name
    }
  }
}
```

This query filters books where the authorâ€™s name is "Alice Johnson", effectively creating a relationship comparison
between books and authors.

## Data Connector Capability: `relation_comparisons`

To achieve optimal performance, the query engine relies on the data connectorâ€™s ability to perform the comparison at the
data source level. This capability is known as `relation_comparisons` (For more details see
[the native data connector spec](https://hasura.github.io/ndc-spec/specification/capabilities.html)). When the data
connector supports this feature, the query engine pushes down the comparison logic to the underlying data connector to
handle the filtering at the data source layer.

## Missing Data Connector Capability or Remote Relationships

When a data connector lacks support for the `relation_comparisons` capability or when handling predicates from a remote
relationship, the query engine cannot push relationship comparisons down to the data connector for processing. Instead,
these comparisons are handled internally by the query engine.

### How It Works

The query engine performs the following steps:

1. **Fetch Related Data**: The query engine retrieves required fields from the related/remote model.
2. **Construct Comparison Expressions**: Using the fetched data, the engine constructs the necessary comparison
   expressions to filter the results.
3. **Fetch Data**: The engine applies these expressions while fetching data from the primary model.

### Performance Implications

- **Increased Data Transfer**: More data must be transferred from the data connector to the query engine for evaluation,
  which can lead to higher latency.
- **Higher Query Engine Load**: Performing comparisons in the query engine increases its workload, which can degrade
  performance, particularly with large datasets.
- **Potential Bottlenecks**: As the amount of related data increases, processing comparisons at the query engine level
  may become less efficient, which can result in slower query responses.

### Example

Consider a query to filter books by the author's name when the `relation_comparisons` capability is not available:

```graphql
query {
  books(where: { author: { name: { _eq: "Alice Johnson" } } }) {
    id
    title
    author {
      name
    }
  }
}
```

In the above query `author` is an Object Relationship with `book.author_id -> author.id` field mapping. Without
`relation_comparisons`, the query engine will:

1. Fetch the `id`s of all authors whose `name` equals "Alice Johnson".
2. Create a comparison expression to check if `book.author_id` matches any of the author IDs obtained in step 1.
3. Query the books using the constructed comparison expression to filter based on `book.author_id`.
4. Return the filtered books to the client.

## Monitoring Query Performance

To ensure your queries perform optimally, monitor the trace details for relationship comparisons. Specifically, look for
the span labeled `Resolve relationship comparison expression: <name>` in the query trace. This span shows the time and
resources spent resolving the relationship comparison, which can help you identify performance issues. If the query
engine is handling the comparison internally due to missing `relation_comparisons` capability, you might notice
increased execution times.

**Trace Example:**

<Thumbnail
  src="/img/graphql-api/queries/console_insert-trace-resolve-relationships.png"
  alt="Trace for Resolving Relationships"
  width="1000px"
/>



--- File: ../ddn-docs/docs/graphql-api/mutations/index.mdx ---
# Mutations

---
sidebar_label: Mutations
sidebar_position: 0
description:
  "In-depth guide to conducting GraphQL Queries and harnessing the full potential of Hasura's GraphQL API. Discover
  functionalities like simple queries, nesting, sorting, pagination, and employing multiple arguments or queries. Learn
  more about filtering queries and the use of variables, aliases, fragments, and directives."
keywords:
  - graphql mutations
  - hasura graphql api
  - api mutations
---

# Basics of Mutations

## Introduction

GraphQL mutations are used to **modify data** by invoking [commands](/reference/metadata-reference/commands.mdx) in your
Hasura DDN project. Mutations allow you to insert, update, or delete records while maintaining control over what data is
affected and returned.

Mutations can perform inserts to add new records, specifying values for each field in the command. You can also update
existing records by applying changes to fields that match certain conditions. Similarly, you can delete records that
meet specific criteria, ensuring precise control over which data is removed.

Just like queries, mutations allow you to specify fields in the response. This means you can retrieve the affected data
after a mutation, such as returning the new or updated values of fields in the records you modified.

Since the GraphQL API is self-documenting, you can write mutations manually or use tools like auto-completion and the
GraphiQL explorer in the Hasura DDN console to help you build and test them. GraphiQL shows the available mutation
types, fields, and input arguments, making it easier to construct and validate mutations.

## Configuration

Currently, you can perform mutations via the GraphQL API using the following methods:

- Some data connectors support mutations out-of-the-box and â€” when
  [adding `commands`](/reference/cli/commands/ddn_command_add.mdx) that are
  [procedures](/reference/metadata-reference/commands.mdx#command-procedurename) â€” will generate them automatically.
- Some data connectors support authoring native mutations â€” such as with the
  [MongoDB connector](/reference/connectors/mongodb/native-operations/native-mutations.mdx) â€” to enable you to write
  custom logic for inserts, updates, and deletes.
- You can mutate data via any lambda connector using the Command Query Separation (CQS) pattern. Learn more
  [here](/business-logic/tutorials/1-add-custom-logic.mdx).

:::info Not sure what your connector supports?

For questions about feature support, check out the [connector reference docs](/reference/connectors/index.mdx).

:::

You can configure the overall usage of mutations in your GraphQL API using
[the `GraphQlConfig` object](/reference/metadata-reference/graphql-config.mdx#graphqlconfig-mutationgraphqlconfig) in
your metadata. Additionally, you can customize individual mutations by modifying
[the `GraphQlDefinition` metadata object](/reference/metadata-reference/commands.mdx#command-commandgraphqldefinition)
for a command.

## Learn more

- [Insert data](/graphql-api/mutations/insert-data.mdx)
- [Update data](/graphql-api/mutations/update-data.mdx)
- [Delete data](/graphql-api/mutations/delete-data.mdx)



--- File: ../ddn-docs/docs/graphql-api/mutations/insert-data.mdx ---
# Insert data

---
sidebar_label: Insert data
sidebar_position: 1
description: "Learn how to insert data using your Hasura DDN API."
keywords:
  - graphql mutations
  - hasura graphql api
  - api mutations
  - insert data
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Insert Data

## Auto-generated insert mutation schema

Depending on your connector, you'll have a range of insert mutations available to mutate data via your API.

### Insert a single object

Insert a single object of a type by passing the object as an argument to the mutation, including the fields you want to
insert.

<GraphiQLIDE
  query={`mutation InsertUser {
  insertUsers(objects: {name: "Martin", age: 40}) {
    returning {
      id
      name
      age
    }
  }
}`}
  response={`{
  "data": {
    "insertUsers": {
      "returning": [
        {
          "id": 4,
          "name": "Martin",
          "age": 40
        }
      ]
    }
  }
}`}
/>

### Insert multiple objects of the same type

Insert multiple objects of a type by passing an array of objects as an argument to the mutation.

<GraphiQLIDE
  query={`mutation InsertMultipleUsers {
  insertUsers(objects: [{name: "Josh", age: 38}, {name: "Victoria", age: 28}]) {
    returning {
      id
      name
      age
    }
  }
}`}
  response={`{
  "data": {
    "insertUsers": {
      "returning": [
        {
          "id": 5,
          "name": "Josh",
          "age": 38
        },
        {
          "id": 6,
          "name": "Victoria",
          "age": 28
        }
      ]
    }
  }
}`}
/>

### Insert an object and return a nested response

Insert an object and return a nested response by querying nested fields in the response.

<GraphiQLIDE
  query={`mutation InsertPost {
  insertPosts(objects: {content: "A little bit more information.", title: "A New Post", userId: "1"}) {
    returning {
      id
      title
      user {
        id
        name
      }
    }
  }
}`}
  response={`{
  "data": {
    "insertPosts": {
      "returning": [
        {
          "id": 6,
          "title": "A New Post",
          "user": {
            "id": 1,
            "name": "Alice"
          }
        }
      ]
    }
  }
}`}
/>

## Custom insert mutations

For connectors that support native mutations, you can also create custom insert mutations to insert data into your data
source. This allows you to write any custom logic you need to insert data using your data source's native capabilities.

Learn more in your [connector's reference documentation](/reference/connectors/index.mdx).



--- File: ../ddn-docs/docs/graphql-api/mutations/update-data.mdx ---
# Update data

---
sidebar_label: Update data
sidebar_position: 2
description: "Learn how to update data using your Hasura DDN API."
keywords:
  - graphql mutations
  - hasura graphql api
  - api mutations
  - update data
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Update Data

Depending on your connector, you'll have a range of update mutations available to mutate data via your API.

## Update an object by a unique field

You can update an object by targeting it using a unique key according to the source schema and then setting the new
value.

<GraphiQLIDE
  query={`mutation UpdateUserAgeById {
  updateUsersById(keyId: "1", updateColumns: {age: {set: "26"}}) {
    returning {
      id
      name
      age
    }
  }
}`}
  response={`{
  "data": {
    "updateUsersById": {
      "returning": [
        {
          "id": 1,
          "name": "Alice",
          "age": 26
        }
      ]
    }
  }
}`}
/>

## Custom update mutations

For connectors that support native mutations, you can also create custom update mutations to update data in your data
source. This allows you to write any custom logic you need to update data using your data source's native capabilities.

Learn more in your [connector's reference documentation](/reference/connectors/index.mdx).



--- File: ../ddn-docs/docs/graphql-api/mutations/delete-data.mdx ---
# Delete data

---
sidebar_label: Delete data
sidebar_position: 3
description: "Learn how to delete data using your Hasura DDN API."
keywords:
  - graphql mutations
  - hasura graphql api
  - api mutations
  - delete data
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Delete Data

## Delete an object

You can delete an object by targeting it using a unique key according to the source schema.

<GraphiQLIDE
  query={`mutation DeletePost {
  deletePostsById(keyId: "6") {
    affectedRows
  }
}`}
  response={`{
  "data": {
    "deletePostsById": {
      "affectedRows": 1
    }
  }
}`}
/>

## Custom delete mutations

For connectors that support native mutations, you can also create custom delete mutations to delete data in your data
source. This allows you to write any custom logic you need to delete data using your data source's native capabilities.

Learn more in your [connector's reference documentation](/reference/connectors/index.mdx).



--- File: ../ddn-docs/docs/graphql-api/subscriptions/index.mdx ---
# Subscriptions

---
sidebar_label: Subscriptions
sidebar_position: 0
description: "Learn how subscriptions unlock the power of real-time data for your applications."
keywords:
  - graphql subscriptions
  - hasura graphql api
  - hasura subscriptions
  - real-time api
  - live updates
toc_max_heading_level: 4
---

# Basics of Subscriptions

## Introduction

Subscriptions allow you to push **real-time updates** from your data sources to clients, making them ideal for building
reactive applications without constant polling.

:::warning Currently in beta

Subscriptions are in beta and are free to use during this period.

:::

## Configuration

You can enable subscriptions on your GraphQL API in the following ways:

### New Hasura DDN project

If you're starting fresh, set up a Hasura DDN project using the [quickstart](/quickstart.mdx) guide. After adding your
models to the supergraph using the [`ddn model add`](/reference/cli/commands/ddn_model_add.mdx) command, subscription
capabilities are automatically generated! For further customization, you can
[edit the metadata](#metadata-configuration).

### Existing Hasura DDN project

To enable subscriptions in an existing DDN project, use the following CLI command:

```bash
ddn codemod upgrade-graphqlconfig-subscriptions
```

This command updates the local metadata files, adding subscription-specific configurations. After running this, create a
supergraph build and test the subscriptions on the API. For further customization, you can
[edit the metadata](#metadata-configuration).

### Metadata configuration

You can configure the overall usage of subscriptions in your GraphQL API using
[the `GraphQlConfig` object](/reference/metadata-reference/graphql-config.mdx#graphqlconfig-subscriptiongraphqlconfig)
in your metadata.

Additionally, you can customize individual subscriptions by modifying the
[`SubscriptionGraphQlDefinition` field](/reference/metadata-reference/models.mdx#model-subscriptiongraphqldefinition) on
any model's `selectUniques`, `selectMany`, or `aggregate` fields.

## Learn more

Subscriptions are supported for the following queries:

- [Select unique](/graphql-api/subscriptions/select-unique.mdx)
- [Select many](/graphql-api/subscriptions/select-many.mdx)
- [Aggregates](/graphql-api/subscriptions/aggregates.mdx)



--- File: ../ddn-docs/docs/graphql-api/subscriptions/select-unique.mdx ---
# Select unique

---
sidebar_label: Select unique
sidebar_position: 1
description: "Learn how to subscribe to unique data queries using your Hasura DDN API."
keywords:
  - graphql subscription
  - hasura graphql api
  - select unique
---

# Select Unique Subscription

The following subscription will automatically update with new notifications for the **specified** user whenever they're
added to the data source.

```graphql
subscription UserNotificationSubscription {
  notifications(where: { user_id: { _eq: 123 } }) {
    id
    created_at
    message
  }
}
```



--- File: ../ddn-docs/docs/graphql-api/subscriptions/select-many.mdx ---
# Select many

---
sidebar_label: Select many
sidebar_position: 2
description: "Learn how to subscribe to data queries of many records using your Hasura DDN API."
keywords:
  - graphql subscription
  - hasura graphql api
  - select many
---

# Select Many Subscription

The following subscription will automatically update with new notifications for **all** users whenever they're added to
the data source.

```graphql
subscription AllNotificationSubscription {
  notifications {
    id
    created_at
    message
  }
}
```



--- File: ../ddn-docs/docs/graphql-api/subscriptions/aggregates.mdx ---
# Aggregates

---
sidebar_label: Aggregates
sidebar_position: 3
description: "Learn how to subscribe to data queries of aggregates using your Hasura DDN API."
keywords:
  - graphql subscription
  - hasura graphql api
  - aggregates
---

# Aggregates Subscription

The following subscription will automatically update the aggregate for `_count` when new records are added to the data
source.

```graphql
subscription NotificationSubscription {
  notificationsAggregate {
    _count
  }
}
```



--- File: ../ddn-docs/docs/auth/faq.mdx ---
# FAQ

---
sidebar_label: FAQ
description: "Frequently Asked Questions about authentication modes and permissions in Hasura."
keywords:
  - JWT mode
  - Webhook mode
  - Hasura permissions
  - NoAuth mode
---

# Frequently Asked Questions about Auth

## Should I choose JWT or Webhook mode?

JWT mode is recommended for most use cases. It's easy to set up, integrates with many 3rd party providers and provides a
robust security model.

Webhook mode is more flexible and can be useful for custom authentication scenarios. Webhook mode will be slightly
slower than JWT mode due to the additional network request.

## How do I test permissions with JWT mode?

You can test permissions directly in the Hasura Console's API interface:

1. Define the desired permissions for a particular Type, Model, or Command in your metadata.
2. Create a new build of your supergraph.
3. Make a request through the Hasura DDN Console GraphiQL API interface with an auth token that includes the required
   session variables.
4. Check the returned data to ensure it adheres to your permission configurations.

Read more about setting up a test token in the [JWT mode tutorial](/auth/jwt/tutorials/setup-test-jwt.mdx).

## How do I create a new role?

A role comes into existence when it is defined in one of TypePermissions, ModelPermissions, or CommandPermissions.

## How do I delete a role?

To delete a role, you need to remove the role from all TypePermissions, ModelPermissions, and CommandPermissions
objects.

## How do I enable a fully open production API where any user can query anything without any auth in their queries?

Use [NoAuth](/auth/noauth-mode.mdx) mode and [set the API to public](/auth/private-vs-public.mdx) via the console in
`Settings > Summary > API Access Mode` or by using the `ddn project set-api-access-mode public` command.

## How do I enable a **mostly** fully public API with no authentication but where **some** fields are not public?

Enable webhook mode and for any query which doesn't have auth header properties assign the `public` session variable
role in the response. For any user query with proper auth, assign the appropriate role.

## How do I enable a secure API with JWT mode but where some fields are fully public?

In this case queries to â€œpublicâ€ fields still need a valid JWT.



--- File: ../ddn-docs/docs/auth/overview.mdx ---
# Authentication and Authorization Overview

---
title: Authentication and Authorization Overview
sidebar_position: 1
description:
  "Learn about Hasura's powerful authentication process. Understand its flexibility to use JWT, webhook, role emulation
  and your existing solutions. Control user access to data."
sidebar_label: Basics
keywords:
  - hasura authentication
  - jwt authentication
  - webhook authentication
  - role emulation
  - hasura authorization
  - user access control
  - secure hasura
  - data security
  - jwt mode
  - hasura integration
seoFrontMatterUpdated: true
---

import { OverviewPlainCard } from "../../src/components/OverviewPlainCard";
import { OverviewTopSectionIconNoVideo } from "@site/src/components/OverviewTopSectionIconNoVideo";
import Icon from "@site/static/icons/shield-tick.svg";

# Auth

## Introduction

Hasura is agnostic about how you authenticate users. You can integrate many popular auth services or use your own custom
solution.

After authentication, session variables are passed via either a valid JWT or webhook to the engine to be checked against
your access control rules or "permissions" to determine what data the user can access.

## Private vs Public

You can choose to make your Hasura DDN API public or private. [Read more](/auth/private-vs-public.mdx).

## AuthConfig options

Authentication in Hasura DDN can be set up in one of three modes. These modes and their configuration options are
specified in the `AuthConfig` object within your metadata.

### JWT mode

Your authentication service must issue JWTs which contain session variables that are passed to the Hasura Engine by the
client on each request. [Read more](/auth/jwt/index.mdx).

### Webhook mode

Hasura Engine will call a webhook on each request with the client headers forwarded. On successful authentication, the
webhook must return a valid `http` response with session variables in the body. [Read more](/auth/webhook/index.mdx).

### NoAuth mode

No authentication is required for a specific role to access the data. [Read more](/auth/noauth-mode.mdx).



--- File: ../ddn-docs/docs/auth/private-vs-public.mdx ---
# Private vs Public

---
sidebar_position: 1
sidebar_label: Private vs Public
description: Learn how to choose between private and public APIs in Hasura DDN.
keywords:
  - private
  - public
  - api
  - authentication
  - authorization
---

import Thumbnail from "@site/src/components/Thumbnail";

# Private vs Public

You can choose to make your Hasura DDN API public or private.

## Private

A `private` Hasura DDN API is only accessible to collaborators on your project.

Queries to a private Hasura DDN API must include a special reserved header `x-hasura-ddn-token` with a valid JWT token
which the Hasura console generates and regenerates every hour. Currently this token is only available in the console.

If a `private` API is also set to JWT or Webhook mode, rather than `noAuth` mode, queries must **also** include the JWT
or webhook authentication values to be successful in addition to the `x-hasura-ddn-token` header.

Projects set to `private` mode are not meant to be used in production.

## Public

A public Hasura DDN API is accessible to everyone.

If a `public` API is also set to JWT or Webhook mode, rather than `noAuth` mode, queries must include the JWT or webhook
authentication values to be successful.

Queries to a public Hasura DDN API do not require the `x-hasura-ddn-token` header.

:::danger Public APIs with noAuth mode

If set to `public` with `noAuth` mode, queries do not require any authentication and the API is fully public.

:::

## Changing the API mode

### DDN CLI

Set to `private` mode:

```bash
ddn project set-api-access-mode private
```

Set to `public` mode:

```bash
ddn project set-api-access-mode public
```

### Hasura console

Click on the `Settings` gear icon in the bottom left of the sidebar navigation and then the `Summary` tab to access the
API access mode toggle at `https://console.hasura.io/project/<your-project-id>/settings/project-summary`.

<Thumbnail src="/img/auth/console-api-access-mode.png" alt="Console API access mode toggle" width="1000px" />



--- File: ../ddn-docs/docs/business-logic/overview.mdx ---
# Basics

---
sidebar_position: 1
sidebar_label: Basics
description: "Learn how to incorporate custom business logic directly into your GraphQL API."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
  - custom
---

# Basics of Business Logic in Hasura DDN

## Introduction

In Hasura DDN, custom business logic is treated as a first-class citizen and like any other data source. This means,
using a **lambda connector**, you can expose a function written in your language of choice return data from it directly
to your GraphQL API.

We treat custom business logic like a data source because it allows for seamless integration into your GraphQL API, just
like databases or other external services. This approach enables you as the API author to unify your data and logic,
ensuring consistent access patterns, security rules, and relationships across all parts of the API.

This custom logic can be used to query or mutate data independently or to extend the functionality of existing
[models](/reference/metadata-reference/models.mdx) in your API. By connecting custom logic to other resources, you can
enhance and expand the capabilities of data from different sources.

Treating custom logic as a data source simplifies your architecture, reduces duplication, and makes your APIs more
flexible and maintainable.

:::info Which languages are supported?

Currently, we have lambda connectors for TypeScript, Python, and Go; these connectors and their custom functions can be
hosted by Hasura or on your own infrastructure.

:::

## Functions and procedures

Regardless of which language you prefer, your connector will generate a
[command](/reference/metadata-reference/commands.mdx) in your metadata for each function. These commands will be
identified in your metadata as either [functions](/reference/metadata-reference/commands.mdx#command-functionname) â€” for
querying data â€” or [procedures](/reference/metadata-reference/commands.mdx#command-procedurename) â€” for modifying data â€”
via your API.

Each connector has its own conventions for determining if the custom logic you write is identified as either a function
or a procedure.

## Learn more

- [Learn how to add a lambda connector](/business-logic/add-a-lambda-connector.mdx)
- [Learn how to add independent custom logic to your API](/business-logic/tutorials/1-add-custom-logic.mdx)
- [Learn how to add custom logic to an existing model in your API](/business-logic/tutorials/2-extend-a-model.mdx)

:::info What about custom native operations?

If you're curious about native queries and mutations, check out the
[connector-specific reference docs](/reference/connectors/index.mdx) for generating queries and mutations using the
native capabilities of your data source.

:::



--- File: ../ddn-docs/docs/business-logic/add-a-lambda-connector.mdx ---
# Add a lambda connector

---
sidebar_position: 3
sidebar_label: Add a lambda connector
description: "Learn how to add a lambda connector to expose custom business logic as part of your API."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Add a Lambda Connector

## Introduction

You can add a lambda connector just like any other [data source](/data-sources/overview.mdx).

## Add a connector

```sh title="Initialize a new connector in a project directory:"
ddn connector init <your_name_for_the_connector> -i
```

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

When you add the `hasura/nodejs` connector, the CLI will generate a Node.js package with a `functions.ts` file. This
file is the entrypoint for your connector.

As this is a Node.js project, you can easily add any dependencies you desire by running `npm i <package-name>` from this
connector's directory.

</TabItem>
<TabItem value="Python" label="Python">

When you add the `hasura/python` connector, the CLI will generate a Python application with a `functions.py` file. This
file is the entrypoint for your connector.

As this is a Python project, you can easily add any dependencies you desire by adding them to the `requirements.txt` in
this connector's directory.

</TabItem>
<TabItem value="Go" label="Go">

When you add the `hasura/go` connector, the CLI will generate a Go application with a `/functions` directory. The
connector will use this directory â€” and any `*.go` file in it â€” as the entrypoint for your connector.

As this is a Go project, you can easily add any dependencies you desire by adding them to the `go.mod` file and running
`go mod tidy` from this connector's directory.

</TabItem>

</Tabs>

:::info Customization

You can customize which subgraph this connector is added to by
[changing your project's context](/reference/cli/commands/ddn_context.mdx) or using flags. More information can be found
in the [CLI docs](/reference/cli/commands/ddn_connector_init.mdx) for the `ddn connector init` command.

:::

## Next steps

After adding a connector, learn [how to add custom business logic](/business-logic/tutorials/1-add-custom-logic.mdx)
written in your language of choice and expose it via your API.



--- File: ../ddn-docs/docs/business-logic/add-env-vars-to-a-lambda.mdx ---
# Add custom environment variables

---
sidebar_position: 4
sidebar_label: Add custom environment variables
description: "Learn how to add and access custom environment variables to your lambda connector."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
---

# Add Custom Environment Variables

## Introduction

Custom environment variables allow you to define configuration values specific to your deployment. These variables can
be used to set API keys, connection strings, or other parameters that your application or connectors need. When you run
the `ddn connector env add` command, it automatically updates multiple files to ensure the environment variables are
available across your project:

- **`.env`**: Adds the variable to the [current context's](/reference/cli/commands/ddn_context.mdx) environment
  configuration file for easy access during development.
- **`connector.yaml`**: Updates the connector's configuration to include the variable for deployment.
- **`compose.yaml`**: Ensures the variable is included in the Docker Compose configuration for runtime.

## Add an environment variable

```sh title="From a project directory, run:"
ddn connector env add <connector_name> --env FOO=bar
```

Repeat these steps for each additional variable.

## Use an environment variable

After creating a new build and running your connector, you can access your environment variable(s) via whatever
conventions your language of choice prefers. For detailed instructions, refer to the following resources:

- **Node.js**: Using [dotenv](https://github.com/motdotla/dotenv#readme)
- **Python**: Using [python-dotenv](https://pypi.org/project/python-dotenv/)
- **Go**: Using [godotenv](https://github.com/joho/godotenv)



--- File: ../ddn-docs/docs/auth/noauth-mode.mdx ---
# ../ddn-docs/docs/auth/noauth-mode.mdx

---
sidebar_position: 5
description: "Learn how to enable and use NoAuth mode in Hasura DDN for testing or development purposes."
keywords:
  - Hasura
  - NoAuth mode
  - authentication
  - supergraph
  - API
---

import Thumbnail from "@site/src/components/Thumbnail";

# NoAuth

## Introduction

NoAuth mode is a simple way to run your Hasura DDN instance without authentication. This is useful for testing or
development purposes or to run your Hasura supergraph API endpoint as fully public without any authentication.

:::danger Production Warning

Using NoAuth mode in production environments is not advisable unless you intend for your API to be completely public. If
your Hasura DDN supergraph contains any sensitive data, you should enable authentication and avoid using NoAuth mode in
production.

:::

When you create a new Hasura DDN project with the `ddn supergraph init` command, noAuth mode is enabled by default.

## Enabling NoAuth Mode

### Step 1. Update your AuthConfig

On a default, standard new Hasura DDN project you will find an `AuthConfig` file in your `config` directory already
configured for noAuth mode.

```yaml title="An AuthConfig for noAuth mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: admin
      sessionVariables: {}
```

#### `role`

The role to be assumed while running the engine in `noAuth` mode. If you intend for your production API to be fully
public, you can set this to a value such as `public` or `anonymous` so that it's explicit.
[Read more about the `Role` value](/reference/metadata-reference/auth-config.mdx#authconfig-role)

#### `sessionVariables`

Static session variables that will be used while running the engine without authentication. This is helpful when you
want to test requests using particular session variables, such as `x-hasura-user-id` with a non-admin role.
[Read more about the `SessionVariables` value](/reference/metadata-reference/auth-config.mdx#authconfig-sessionvariables)

### Step 2. Check permissions

On a default, standard new Hasura DDN project you will find permissions such as the below for an example `Posts` model:

```yaml title="TypePermissions:"
kind: TypePermissions
version: v1
definition:
  typeName: Posts
  permissions:
    - role: admin
      output:
        allowedFields:
          - authorId
          - content
          - postId
          - title
```

```yaml title="ModelPermissions:"
kind: ModelPermissions
version: v1
definition:
  modelName: Posts
  permissions:
    - role: admin
      select:
        filter: null
        allowSubscriptions: true
```

You can see that the `admin` role has full access to the `Posts` model with no filtering in the `ModelPermissions` and
all fields being allowed in the `TypePermissions`.

### Step 3. Build your supergraph

```bash
ddn supergraph build local
```

### Step 4. Make an un-authenticated request

```title="Open the Hasura DDN console and make a request:"
ddn console --local
```

<Thumbnail src="/img/auth/console-authorization-screen-noauth-mode.png" alt="NoAuth mode" width="1000px" />

If you click on the `Authorization` tab in the console, you can see that `NoAuth` mode is enabled and for this request,
the static session variables are not being used and all fields and data are being returned.



--- File: ../ddn-docs/docs/business-logic/errors.mdx ---
# Handle errors

---
sidebar_position: 5
sidebar_label: Handle errors
description:
  "Learn how to handle errors and pass on custom information to your API consumers using lambda connectors and their
  SDKs."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Handle Errors with Lambda Connectors

## Introduction

By default, lambda connectors return a generic `internal error` message whenever an exception is encountered in your
custom business logic and **log the details of the error in the OpenTelemetry trace associated with the request**.

The Native Data Connector specification identifies a
[valid set of status codes](https://hasura.github.io/ndc-spec/specification/error-handling.html) which a connector can
return to your API consumers.

:::info How detailed should error messages be?

Exposing stack traces to end users is generally discouraged. Instead, API administrators can review traces logged in the
OpenTelemetry traces to access detailed stack trace information.

:::

## Return custom error messages

Lambda connectors allow you to throw classes of errors with your own custom message and metadata to indicate specific
error conditions. These classes are designed to provide clarity in error handling when interacting with your data
sources. To explore the available error classes, use your editor's autocomplete or documentation features to view all
supported classes and their usage details.

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```typescript title="TypeScript examples:" {1,6,14,22}
import * as sdk from "@hasura/ndc-lambda-sdk";

/** @readonly */
export function updateResource(userRole: string): void {
  if (userRole !== "admin") {
    throw new sdk.Forbidden("User does not have permission to update this resource", { role: userRole });
  }
  console.log("Resource updated successfully.");
}

/** @readonly */
export function createResource(id: string, existingIds: string[]): void {
  if (existingIds.includes(id)) {
    throw new sdk.Conflict("Resource with this ID already exists", { existingId: id });
  }
  console.log("Resource created successfully.");
}

/** @readonly */
export function divide(x: number, y: number): number {
  if (y === 0) {
    throw new sdk.UnprocessableContent("Cannot divide by zero", { myErrorMetadata: "stuff", x, y });
  }
  return x / y;
}
```

</TabItem>

<TabItem value="Python" label="Python">

```python title="Python examples:" {4}
# There are different error types including: BadRequest, Forbidden, Conflict, UnprocessableContent, InternalServerError, NotSupported, and BadGateway
@connector.register_query
def error():
    raise UnprocessableContent(message="This is an error", details={"Error": "This is an error!"})
```

</TabItem>

<TabItem value="Go" label="Go">

```go title="Go examples:" {7,29-31}
package functions

import (
	"context"
	"fmt"

	"github.com/hasura/ndc-sdk-go/schema"
	"hasura-ndc.dev/ndc-go/types"
)

// A hello argument
type HelloArguments struct {
	Greeting string `json:"greeting"`
	Count    *int   `json:"count"`
}

// A hello result
type HelloResult struct {
	Reply string `json:"reply"`
	Count int    `json:"count"`
}


func FunctionHello(ctx context.Context, state *types.State, arguments *HelloArguments) (*HelloResult, error) {
	count := 1
	authorized := false // This is just an example

	if !authorized {
		return nil, schema.UnauthorizeError("User is not authorized to perform this operation", map[string]any{
			"function": "hello",
		})
	}

	if arguments.Count != nil {
		count = *arguments.Count + 1
	}
	return &HelloResult{
		Reply: fmt.Sprintf("Hi! %s", arguments.Greeting),
		Count: count,
	}, nil
}
```

</TabItem>

</Tabs>

## Access OpenTelemetry traces

Traces â€” complete with your custom error messages â€” are available for each request. You can access these by clicking on
`View Trace` in the bottom-right corner of the GraphiQL explorer in the console after running a request.

Additionally, you can access the traces list under the `Insights` tab.



--- File: ../ddn-docs/docs/auth/jwt/index.mdx ---
# JWT

---
sidebar_position: 1
sidebar_label: JWT
description:
  "Explore how Hasura supports JWT authentication for your GraphQL API, including setup, configuration, and integration
  with third-party services like Auth0, AWS Cognito, Firebase, and Clerk."
keywords:
  - jwt
  - hasura
  - graphql api
  - jwt authentication
  - auth0 integration
  - aws cognito
  - firebase
  - clerk
  - session variables
  - third-party services
---

# JWT Mode

In JWT mode, session variables are passed to the Hasura Engine on each request in JSON Web Tokens (JWTs).

## JWT mode setup

- [How to set up JWT mode](/auth/jwt/jwt-mode.mdx)
- [JWT configuration information](/auth/jwt/jwt-configuration.mdx)

## Integrations with third-party services

- [Auth0 JWT integration](/auth/jwt/tutorials/integrations/1-auth0.mdx)
- [AWS Cognito JWT integration](/auth/jwt/tutorials/integrations/2-aws-cognito.mdx)
- [Firebase JWT integration](/auth/jwt/tutorials/integrations/3-firebase.mdx)
- [Clerk JWT integration](/auth/jwt/tutorials/integrations/4-clerk.mdx)



--- File: ../ddn-docs/docs/auth/jwt/jwt-mode.mdx ---
# JWT Mode

---
description:
  "Learn how to configure JWT authentication in Hasura, including setting up AuthConfig, defining custom claims, and
  making authenticated requests."
keywords:
  - JWT authentication
  - Hasura
  - AuthConfig
  - JSON Web Token
  - supergraph
sidebar_position: 1
sidebar_label: JWT Mode
---

import Thumbnail from "@site/src/components/Thumbnail";

# JWT Mode

## Introduction

JWT mode requires that the client making the query sends a valid JSON Web Token to the Hasura Engine endpoint. This JWT
is provided by an auth service such as Auth0, AWS Cognito, Firebase, Clerk, or your own custom solution.

Hasura then verifies and decodes the JWT to extract `x-hasura-*` session variable claim values from a defined namespace
in the token.

The `x-hasura-default-role` and `x-hasura-allowed-roles` session variables are required, and you will also most likely
utilize the user id and any other information which you need to determine access to your data.

The token can be passed in the header of the request in a dedicated key, or using the `Authorization` header with the
`Bearer` prefix, or as a cookie. All these options are defined in the `AuthConfig` object in your metadata.

<Thumbnail src="/img/auth/auth-jwt-overview-diagram.png" alt="Authentication using JWT" />

## Session variable requirements

Session variables passed via JWT or webhook can contain any information you want, but must at least contain an
`x-hasura-default-role` property and `x-hasura-allowed-roles` array.

An `x-hasura-role` value can optionally be sent as a plain header in the request to indicate the role which should be
used. If this is not provided, the engine will use the `x-hasura-default-role` value in the JWT.

To clarify, the `x-hasura-role` header is optional and can be used to override the default role in the JWT allowing the
same verified JWT to be used for different roles.

Only keys prefixed with `x-hasura-` will be accessible by the engine.

Session variable keys are case-insensitive. Values are case-sensitive.

## Enabling JWT authentication

You can enable your Hasura DDN instance to use JWTs in just a few steps.

### Step 1. Update your AuthConfig

Hasura utilizes an [AuthConfig](/reference/metadata-reference/auth-config.mdx) object that allows you to define the
configuration for your authentication service. In a standard setup the `auth-config.hml` file can be found in your
`globals` directory.

:::tip Hasura DDN VS Code extension

You can use [Hasura's VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) to
scaffold out your `AuthConfig` object by typing `AuthConfig` and selecting this object from the list of available
options. As you navigate through the skeleton, you can type `CTRL+SPACEBAR` at any point to reveal options for the
different key-value pairs.

:::

Below, we're showing using the `BearerAuthorization` header location format using a fixed secret key from an environment
variable. However, Hasura DDN supports other methods for
[where the engine can locate the JWT](reference/metadata-reference/auth-config.mdx#authconfig-jwttokenlocation) and
[how it is verified](reference/metadata-reference/auth-config.mdx#authconfig-jwtalgorithm).

```yaml title="globals/metadata/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            valueFromEnv: AUTH_SECRET
```

Read more about other setup options [here](/reference/metadata-reference/auth-config.mdx#authconfig-jwtconfig).

### Step 2. Define the JWT with custom claims

Your auth service should include an object with a key of `claims.jwt.hasura.io` in the JWT. Within this, each claim
should be prefixed with `x-hasura-*` and include the relevant information. Note that an extra optional `x-hasura-role`
**header** can be passed to override the default role found in the JWT's custom claims.

| Key                      | Required | Value                                                                                                          |
| ------------------------ | -------- | -------------------------------------------------------------------------------------------------------------- |
| `x-hasura-default-role`  | Yes      | The role that will be used when the optional x-hasura-role header is not passed                                |
| `x-hasura-allowed-roles` | Yes      | A list of allowed roles for the user making the request.                                                       |
| `x-hasura-[custom]`      | No       | Where `[custom]` is any string you wish (e.g., `org`, `user-id`, `customer`). The value can be any JSON value. |

In the simple example below, we're including the required claims by stating the default role is `admin` and the list of
available roles is limited to `user` and `admin`. Additionally, we're passing a custom key of `x-hasura-user-id` which
can be used with [permissions](/reference/metadata-reference/permissions.mdx) when executing queries.
[Read more about the default claims here](/auth/jwt/jwt-configuration.mdx#hasura-jwt-format).

```json title="Example JWT payload"
{
  "iat": 1735916718,
  "exp": 1796916677,
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "admin",
    "x-hasura-allowed-roles": ["user", "admin"],
    "x-hasura-user-id": 1234
  }
}
```

Your auth service will encode this object using a secret and create a token which can then be passed to Hasura. You can
see an example of the above token encoded
[here](https://jwt.io/#debugger-io?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzU5MTY3MTgsImV4cCI6MTc5NjkxNjY3NywiY2xhaW1zLmp3dC5oYXN1cmEuaW8iOnsieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoiYWRtaW4iLCJ4LWhhc3VyYS1hbGxvd2VkLXJvbGVzIjpbInVzZXIiLCJhZG1pbiJdLCJ4LWhhc3VyYS11c2VyLWlkIjoxMjM0fX0.1Dv6B077loe9wGcUFwK_JAHRUtLOZHOEp9EE-OehUzY).
The signature secret to verify this token with the HS256 algorithm is `ultra-secret-very-secret-super-secret-key`.

:::warning Setting audience check

Certain JWT providers (like Firebase) share JWKs between multiple tenants. They use the `aud` claim of JWT to specify
the intended tenant for the JWT. Setting the `audience` field in the Hasura JWT configuration will make sure that the
`aud` claim from the JWT is also checked during verification. Not doing this check will allow JWTs issued for other
tenants to be valid as well.

In these cases, you **MUST** set the `audience` field to appropriate value. **Failing to do so is a major security
vulnerability**. Learn how to set this [here](reference/metadata-reference/auth-config.mdx#authconfig-jwtconfig).

:::

### Step 3. Add permissions to an object in your supergraph

Let's add some example `TypePermissions` so that an admin role can access all fields in the Orders type, but we restrict
a user role from accessing the `deliveryDate` field.

```bash title="Example TypePermissions for Orders type"
---
kind: TypePermissions
version: v1
definition:
  typeName: Orders
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - deliveryDate
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-start
    - role: user
      output:
        allowedFields:
          - createdAt
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-end
```

Let's also add some example `ModelPermissions` so that an admin role can access all rows in the Orders model, but a user
role can only access rows where the userId field matches the user id session variable in the JWT.

```bash title="Example ModelPermissions for Orders model"
---
kind: ModelPermissions
version: v1
definition:
  modelName: Orders
  permissions:
    - role: admin
      select:
        filter: null
        allowSubscriptions: true
# highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: userId
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
# highlight-end
```

:::info Example JWT payload

For these examples we'll set the payload of the JWT to specify a `user` role and a UUID for the user id.

```json title="Example JWT payload which we will send to the Hasura Engine"
{
  "iat": 1735916718,
  "exp": 1796916677,
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "user",
    "x-hasura-allowed-roles": ["user"],
    "x-hasura-user-id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb"
  }
}
```

:::

### Step 4. Rebuild your supergraph

Once you've updated your `AuthConfig` object in `auth-config.hml` and updated your claims, you can rebuild your
supergraph and test it locally.

```bash title="For example, from the root of your project, run:"
ddn supergraph build local
```

### Step 5. Make an authenticated request

In the example above, we're using the `BearerAuthorization` method. As such, as we can make a request to our Hasura DDN
instance by including a header with the key-value of `Authorization: Bearer <our-encoded-token>`. For testing, you can
pass this value in the Hasura DDN console's header section.

<Thumbnail src="/img/auth/console-auth-tab-default-jwt.png" alt="Hasura console authentication using JWT" />

If we run a query for Orders, we can see that we only get the orders which this user has made and are not able to access
the deliveryDate field.

<Thumbnail
  src="/img/auth/console-auth-query-permissions-results.png"
  alt="Hasura console query permissions results using JWT"
/>

### Step 6. Set your API to public

Now that you have implemented JWT authentication, you can set your API to public. See here for more information on
[setting your API to public](/auth/private-vs-public.mdx).

## Next steps

If you're looking for step-by-step help to get started with common authentication providers, check
[this section](/auth/jwt/tutorials/index.mdx) of tutorials.



--- File: ../ddn-docs/docs/business-logic/tutorials/index.mdx ---
# Tutorials

---
sidebar_position: 1
sidebar_label: Tutorials
description:
  "Check out different common and bespoke ways of incorproating custom business logic directly into your API using
  lambda connectors."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
---

# Business Logic Tutorials

In this section, you'll find tutorials that walk you through implementing custom business logic with lambda connectors
step-by-step. Initially, we recommend checking out two very common use cases:

- [Adding custom logic](/business-logic/tutorials/1-add-custom-logic.mdx)
- [Extending an existing model](/business-logic/tutorials/2-extend-a-model.mdx)

From there, check out these advanced use cases:

- [Formatting datetime objects](/business-logic/tutorials/3-format-datetime-objects.mdx)
- [Hashing passwords](/business-logic/tutorials/3-hash-passwords.mdx)
- [Translating content](/business-logic/tutorials/4-translate-content.mdx)
- [Enriching data with an LLM](/business-logic/tutorials/5-enrich-data-with-an-llm.mdx)
- [Validating credentials](/business-logic/tutorials/6-validate-credentials.mdx)
- [HTTP header forwarding](/business-logic/tutorials/7-http-header-forwarding.mdx)



--- File: ../ddn-docs/docs/auth/jwt/jwt-configuration.mdx ---
# ../ddn-docs/docs/auth/jwt/jwt-configuration.mdx

---
description:
  "Comprehensive guide on configuring JSON Web Tokens (JWT) for Hasura DDN, including payload definitions, claim
  descriptions, and configuration examples."
keywords:
  - hasura keywords
  - more
  - JWT
  - JSON Web Token
  - Hasura DDN
  - JWT configuration
  - JWT claims
sidebar_position: 2
---

# JWT Configuration

This section describes the JSON Web Token (JWT) configuration options available in Hasura DDN.

## Payload Definition

Example JSON Web Token (JWT) payload configuration definition:

```yaml title="globals/metadata/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            value: ultra-secret-very-secret-super-secret-key
      audience: ["myapp-1234", "myapp-6789"]
      allowedSkew: 60
      issuer: https://my-auth-server.com
```

As a minimum, either the `claimsConfig`, `tokenLocation`, **and** `key` values **have to be present**.

## Example Decoded Payload

```json
{
  "iat": 1735916718,
  "exp": 1796916677,
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "user",
    "x-hasura-allowed-roles": ["user", "admin"],
    "x-hasura-user-id": "123",
    "x-hasura-org-id": "456",
    "x-hasura-custom": "custom-value"
  }
}
```

## Example Encoded JWT

```text
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzU5MTY3MTgsImV4cCI6MTc5NjkxNjY3NywiY2xhaW1zLmp3dC5oYXN1cmEuaW8iOnsieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoidXNlciIsIngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsidXNlciIsImFkbWluIl0sIngtaGFzdXJhLXVzZXItaWQiOiIxMjMiLCJ4LWhhc3VyYS1vcmctaWQiOiI0NTYiLCJ4LWhhc3VyYS1jdXN0b20iOiJjdXN0b20tdmFsdWUifX0.5bwSMgxsyULY1uhCJxYd-sO35rCdznRCZ4YMLwDD5u8
```

**Note:** `x-hasura-default-role` and `x-hasura-allowed-roles` are mandatory, while the rest of the claims are optional.

[See here for the JWT debugger](https://jwt.io/#debugger-io?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3MzU5MTY3MTgsImV4cCI6MTc5NjkxNjY3NywiY2xhaW1zLmp3dC5oYXN1cmEuaW8iOnsieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoidXNlciIsIngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsidXNlciIsImFkbWluIl0sIngtaGFzdXJhLXVzZXItaWQiOiIxMjMiLCJ4LWhhc3VyYS1vcmctaWQiOiI0NTYiLCJ4LWhhc3VyYS1jdXN0b20iOiJjdXN0b20tdmFsdWUifX0.5bwSMgxsyULY1uhCJxYd-sO35rCdznRCZ4YMLwDD5u8)
of this example JWT token. The signature secret to verify this token with the HS256 algorithm is
`ultra-secret-very-secret-super-secret-key`.

## Hasura JWT format

The `x-hasura-role` value can be sent as a plain **header** in the request to indicate the role which should be used.

When your auth server generates the JWT, the custom claims in the JWT **must contain the following** in a custom
namespace. This namespace can be any string you choose, as long as it matches the `namespace.location` defined in your
AuthConfig. Using `claims.jwt.hasura.io` will match our examples.

1.  A `x-hasura-default-role` field. The role that will be used when the optional `x-hasura-role` _header_ is **not
    passed**.
2.  A `x-hasura-allowed-roles` field. A list of allowed roles for the user i.e. acceptable values of the optional
    `x-hasura-role` _header_.
3.  Add any other optional `x-hasura-*` claim fields (required as per your defined permissions) to the custom namespace.

To summarize, `x-hasura-allowed-roles` session variable contains a list of all the roles that the user can assume and
the `x-hasura-role` header tells the Hasura Engine which role to use for the request, and if that is missing then the
`x-hasura-default-role` session variable will be used.

This setup makes it more convenient for a JWT to only need to be issued once with a list of allowed roles for the user,
and then allow the client to decide which of those roles to actually use for a request. This prevents the user needing
to log in again or unnecessary JWT re-issuance.

If, for example, your app will not need to switch user roles and the user only needs one role, for instance: `user`, you
can just issue a JWT with `x-hasura-default-role` set to `user` and `x-hasura-allowed-roles` set to `["user"]` and not
send the `x-hasura-role` header in the request.

This setup is designed so that there is one authoritative way to construct your JWT token for the Hasura Engine which
can cover a wide range of use cases.

## Hasura JWT Claim Description

### x-hasura-default-role

The `x-hasura-default-role` will be the role that the user falls back to when no `x-hasura-role` value is specified in
the header of the request. Usually, this will be the role with the least privileges and can be overridden by the
`x-hasura-role` header when making a request.

### x-hasura-allowed-roles

The `x-hasura-allowed-roles` list can contain all the roles which a particular user can assume, eg:
`[ "user", "manager", "owner" ]`. Usually, these will have varying degrees of access to your data as specified in
Permissions and by specifying this list it lets the Hasura Engine know that this user can assume any of them.

### x-hasura-\*

The JWT can have other user-defined `x-hasura-*` fields and their values can only be strings (they will be converted to
the right type automatically). You can use these `x-hasura-*` values in your permission rules.

The JWT will normally also contain standard (`sub`, `iat` etc.) and custom (`name`, `admin` etc.) claims depending on
your auth provider.

## JWT Notes

- JWT claim fields eg: `x-hasura-default-role` are case-insensitive.
- Hasura Engine only has access to headers and JWT claims which are prefixed with `x-hasura-`.
- Hasura Engine only has access to JWT claims in namespace defined in the `AuthConfig` object in metadata.
- All `x-hasura-*` values should be of type `String`, they will be converted to the right type automatically.

## Hasura JWT configuration options

### claimsConfig

You can specify where the engine should look for the claims within the decoded token either with one of `namespace` and `locations` options.

#### namespace {#jwt-claims-config-namespace}

The `namespace` option is used when all of the Hasura claims are present in a single object within the decoded JWT. 
Our example uses `claims.jwt.hasura.io` in the [Example Decoded Payload](#example-decoded-payload).

```yaml
claimsConfig:
  namespace:
    claimsFormat: Json
    location: /claims.jwt.hasura.io
```

The `location` field indicates the location of the namespace object that uses [RFC 6901 JSON Pointer](https://datatracker.ietf.org/doc/html/rfc6901) string syntax.

The `claimsFormat` field indicates whether the Hasura-specific claims are a regular JSON object or a stringified JSON. The following possible values are allowed: `Json`, `StringifiedJson`.

This is required because providers like AWS Cognito only allow strings in the JWT claims.
[See #1176](https://github.com/hasura/graphql-engine/issues/1176).

**Example**:

If `claimsFormat` is `Json` then the JWT claims should look like:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "claims.jwt.hasura.io": {
    "x-hasura-allowed-roles": ["editor", "user", "mod"],
    "x-hasura-default-role": "user",
    "x-hasura-user-id": "1234567890",
    "x-hasura-org-id": "123",
    "x-hasura-custom": "custom-value"
  }
}
```

If `claimsFormat` is `StringifiedJson` then the JWT claims should look like:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "claims.jwt.hasura.io": "{\"x-hasura-allowed-roles\":[\"editor\",\"user\",\"mod\"],\"x-hasura-default-role\":\"user\",\"x-hasura-user-id\":\"1234567890\",\"x-hasura-org-id\":\"123\",\"x-hasura-custom\":\"custom-value\"}"
}
```

#### locations {#jwt-claims-config-locations}

This `locations` option can be used when Hasura claims are not all present in the single object, but individual claims are provided a JSON pointer within the decoded JWT. 
In this option, you can indicate:
- a literal value. 
- or a JSON pointer path for individual claims and an optional default value if the claim doesn't exist.

`x-hasura-default-role` and `x-hasura-allowed-roles` claims are required. Other custom claims are optionally configured. 

The literal values should be of type `String`, except for the `x-hasura-allowed-roles` claim which expects a string
array.

**Example: JWT config with JSON path values**

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "user": {
    "id": "ujdh739kd"
  },
  "hasura": {
    "all_roles": ["user", "editor"]
  }
}
```

The mapping for `x-hasura-allowed-roles`, `x-hasura-default-role` and `x-hasura-user-id` session variables can be
specified in the `locations` configuration as follows:

```yaml
claimsConfig: 
  locations:
    x-hasura-default-role: 
      path:
        path: /hasura/all_roles/0
    x-hasura-allowed-roles: 
      path:
        path: /hasura/all_roles
    x-hasura-user-id:
      path:
        path: /user/id
```

**Example: JWT config with JSON path values and default values**

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "hasura": {
    "all_roles": ["user", "editor"]
  }
}
```

```yaml
claimsConfig: 
  locations:
    x-hasura-default-role: 
      path:
        path: /hasura/all_roles/0
    x-hasura-allowed-roles: 
      path:
        path: /hasura/all_roles
    x-hasura-user-id:
      path:
        path: /user/id
        default: ujdh739kd
```

In the above case, since the `/user/id` doesn't exist in the JWT token, the default value of the `x-hasura-user-id` i.e
`ujdh739kd` will be used

**Example: JWT config containing literal values**

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022,
  "user": {
    "id": "ujdh739kd"
  }
}
```

The corresponding JWT config should be:

```yaml
claimsConfig: 
  locations:
    x-hasura-default-role: 
      literal: user
    x-hasura-allowed-roles: 
      literal: ["user", "editor"]
    x-hasura-user-id:
      path:
        path: /user/id
```

In the above example, the `x-hasura-allowed-roles` and `x-hasura-default-role` values are set in the JWT config and the
value of the `x-hasura-user-id` is a JSON path to the value in the JWT token.

### tokenLocation

Indicates the token location where request header to read the JWT from. 

The following are the possible values:

#### BearerAuthorization

In this mode, Hasura expects an `Authorization` header with a `Bearer` token.

```yaml
tokenLocation: 
  type: BearerAuthorization
```

The JWT header should look like:

```none
Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWI...
```

#### Cookie

In the cookie mode, Hasura will try to parse the cookie header with the given cookie name. The value of the cookie
should be the exact JWT.

```yaml
tokenLocation: 
  type: Cookie
  name: cookie_name
```

The JWT header should look like:

```none
Cookie: cookie_name=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWI...
```

#### Header

In the custom header mode, Hasura expects a `header_name` header with the exact JWT token value.

```yaml
tokenLocation: 
  type: Header
  name: header_name
```

The JWT header should look like:

```none
header_name: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWI...
```

### key {#jwt-json-key}

This field specifies the JWT key configuration according to which the incoming JWT will be decoded. 
You can configure either a `fixed` algorithm key or a remote JWK URL.

#### fixed {#jwt-json-key-fixed}

In this option, you must indicate a JWT key and its algorithm so the engine can decode and verify the JWT token. 

```yaml
key:
  fixed:
    algorithm: HS256
    key:
      value: ultra-secret-very-secret-super-secret-key
      # valueFromEnv: AUTH_JWT_KEY
```

The `algorithm` field specifies the cryptographic signing algorithm which is used to sign the JWTs. Valid values are: `HS256`, `HS384`,
`HS512`, `RS256`, `RS384`, `RS512`, `ES256`, `ES384`, `PS256`, `PS384`, `PS512`, `EdDSA`.

The `key` field can be a literal value or an environment variable name. 

- In the case of a symmetric key (i.e. a HMAC-based key), just the key as is. (e.g. -"abcdef..."). The key must be long
  enough for the chosen algorithm, (e.g. for HS256 it must be at least 32 characters long).
- In the case of an asymmetric key (RSA, EdDSA, ECDSA etc.), only the **public** key, in a PEM-encoded string or as an
  X509 certificate.

#### jwkFromUrl {#jwt-json-jwk_url}

An URL where a provider publishes their JWKs (JSON Web Keys - which are used for signing the JWTs). The URL **must**
publish the JWKs in the standard format as described [here](https://tools.ietf.org/html/rfc7517).

For example:

- Auth0 publishes their JWK url at: `https://<YOUR_AUTH0_DOMAIN>.auth0.com`.
- Firebase publishes their JWK url at:
  `https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com`.

```yaml
key: 
  jwkFromUrl: https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com
```

The JWTs must be signed by the JWK published at the given URL. They can be signed by any algorithm that is compatible
with the key (eg. `RS256`, `RS384`, `RS512` algorithms require a JWK with an RSA key).

DDN does not currently support rotating JWKs.

### audience

This is an optional field. Certain providers might set a claim which indicates the intended audience for the JWT. This
can be checked by setting this field.

When this field is set, during the verification process of the JWT, the `aud` claim in the JWT will be checked to see
whether it is equal to the `audience` field given in the configuration. If they are not equal then the JWT will be
rejected.

See the [RFC](https://tools.ietf.org/html/rfc7519#section-4.1.3) for more details.

This field must be a list of strings.

Examples:

```yaml
kind: AuthConfig
version: v3
definition: 
  mode: 
    jwt:
      # ...
      audience: ["myapp-1234", "myapp-6789"]
```

:::danger Audience Security Vulnerability

Certain JWT providers share JWKs between multiple tenants. They use the `aud` claim of the JWT to specify the intended
audience. Setting the `audience` field in the Hasura JWT configuration will make sure that the `aud` claim from the JWT
is also checked during verification. Not doing this check will allow JWTs issued for other tenants to be valid as well.

In these cases, you **MUST** set the `audience` field to the appropriate value. Failing to do so is a **major security
vulnerability**.

:::

### issuer

This is an optional field. It takes a string value.

When this field is set, during the verification process of the JWT, the `iss` claim in the JWT will be checked to see
whether it is equal to the `issuer` field given in the configuration. If they are not equal then the JWT will be
rejected.

See [RFC](https://tools.ietf.org/html/rfc7519#section-4.1.1) for more details.

Examples:

```yaml
kind: AuthConfig
version: v3
definition: 
  mode: 
    jwt:
      # ...
      issuer: https://my-auth-server.com
```

#### Issuer Notes

- Certain providers require you to verify the `iss` claim on the JWT. To do that you can set this field to the
  appropriate value.
- A JWT configuration without an issuer will match any issuer field present in an incoming JWT.
- An incoming JWT without an issuer specified will match a configuration even if it specifies an issuer.

### allowed_skew

`allowedSkew` is an optional field to provide some leeway (to account for clock skews) while comparing the JWT expiry
time. This field expects an integer value which will be the number of seconds of the skew value.

### Hasura JWT Config Examples

#### HMAC-SHA based

Your auth server is using HMAC-SHA algorithms to sign JWTs, and is using a 256-bit key. In this case, the JWT config
will look like:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            value: 3EK6FD+o0+c7tzBNVfjpMkNDi2yARAAKzQlk8O2IKoxQu4nF7EdAh8s3TwpHwrdWT6R
```

The `key` is the actual shared secret, which is used by Hasura and the external auth server.

#### RSA based

If your auth server is using the RSA algorithm to sign JWTs, and is using a 512-bit key, the JWT config only needs to
have the public key.

**Example 1**: public key in PEM format (not OpenSSH format):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: RS512
          key:
            value: '-----BEGIN PUBLIC KEY-----\nMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDdlatRjRjogo3WojgGHFHYLugd\nUWAY9iR3fy4arWNA1KoS8kVw33cJibXr8bvwUAUparCwlvdbH6dvEOfou0/gCFQs\nHUfQrSDv+MuSUMAe8jzKE4qW+jK+xQU9a03GUnKHkkle+Q0pX/g6jXZ7r1/xAK5D\no2kQ+X5xK9cipRgEKwIDAQAB\n-----END PUBLIC KEY-----\n'
```

**Example 2**: public key as X509 certificate:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: RS512
          key:
            value: '-----BEGIN CERTIFICATE-----\nMIIDHDCCAgSgAwIBAgIINw9gva8BPPIwDQYJKoZIhvcNAQEFBQAwMTEvMC0GA1UE\nAxMmc2VjdXJldG9rZW4uc3lzdGVtLmdzZXJ2aWNlYWNjb3VudC5jb20wHhcNMTgQt7dIsMTIU9k1SUrFviZOGnmHWtIAw\nmtYBcM9I0f9/ka45JIRp5Y1NKpAMFSShs7Wv0m1JS1kXQHdJsPSmjmDKcwnBe3R/\nTU3foRRywR/3AJRM15FNjTqvUm7TeaW16LkkRoECAwEAAaM4MDYwDAYDVR0TAQH/\nBAIwADAOBgNVHQ8BAf8EBAMCB4AwFgYDVR0lAQH/BAwwCgYIKwYBBQUHAwIwDQYJ\nKoZIhvcNAQEFBQADggEBADfY2DEmc2gb8/pqMNWHYq/nTYfJPpK4VA9A0lFTNeoq\nzmnbGwhKj24X+Nw8trsvkrKxHvCI1alDgBaCyzjGGvgOrh8X0wLtymp1yj6PWwee\nR2ZPdUaB62TCzO0iRv7W6o39ey+mU/FyYRtxF0ecxG2a0KNsIyFkciXUAeC5UVDo\nBNp678/SDDx9Ltuxc6h56a/hpBGf9Yzhr0RvYy3DmjBs6eopiGFmjnOKNxQrZ5t2\n339JWR+yiGEAtoHqk/fINMf1An6Rung1xYowrm4guhCIVi5unAvQ89fq0I6mzPg6\nLhTpeP0o+mVYrBmtYVpDpv0e71cfYowSJCCkod/9YbY=\n-----END CERTIFICATE-----'
```

**Example 3**: public key published as JWKs:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        jwkFromUrl: https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com
```

#### EdDSA based

If your auth server is using EdDSA to sign JWTs, and is using the Ed25519 variant key, the JWT config only needs to have
the public key.

**Example 1**: public key in PEM format (not OpenSSH format):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: Ed25519
          key:
            value: '-----BEGIN PUBLIC KEY-----\nMCowBQYDK2VwAyEAG9I+toAAJicilbPt36tiC4wi7E1Dp9rMmfnwdKyVXi0=\n-----END PUBLIC KEY-----'
```

**Example 2**: public key as X509 certificate:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: Ed25519
          key:
            value: '-----BEGIN CERTIFICATE REQUEST-----\nMIIBAzCBtgIBADAnMQswCQYDVQQGEwJERTEYMBYGA1UEAwwPd3d3LmV4YW1wbGUu\nY29tMCowBQYDK2VwAyEA/9DV/InajW02Q0tC/tyr9mCSbSnNP1txICXVJrTGKDSg\nXDBaBgkqhkiG9w0BCQ4xTTBLMAsGA1UdDwQEAwIEMDATBgNVHSUEDDAKBggrBgEF\nBQcDATAnBgNVHREEIDAegg93d3cuZXhhbXBsZS5jb22CC2V4YW1wbGUuY29tMAUG\nAytlcANBAKbTqnTyPcf4ZkVuq2tC108pBGY19VgyoI+PP2wD2KaRz4QAO7Bjd+7S\nljyJoN83UDdtdtgb7aFgb611gx9W4go=\n-----END CERTIFICATE REQUEST-----'
```

#### EC based

If your auth server is using ECDSA to sign JWTs, and is using the ES variant with a 256-bit key, the JWT config only
needs to have the public key.

**Example 1**: public key in PEM format (not OpenSSH format):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: ES256
          key:
            value: '-----BEGIN PUBLIC KEY-----\nMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEVs/o5+uQbTjL3chynL4wXgUg2R9\nq9UU8I5mEovUf86QZ7kOBIjJwqnzD1omageEHWwHdBO6B+dFabmdT9POxg==\n-----END PUBLIC KEY-----'
```

**Example 2**: public key as X509 certificate:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: ES256
          key:
            value: '"-----BEGIN CERTIFICATE-----\nMIIBbjCCARWgAwIBAgIUGn02F6Y6s88dDGmIfwiNxWxDjhswCgYIKoZIzj0EAwIw\nDTELMAkGA1UEBhMCSU4wHhcNMjMwNTI0MTAzNTI4WhcNMjgwNTIyMTAzNTI4WjAN\nMQswCQYDVQQGEwJJTjBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABBFbP6OfrkG0\n4y93Icpy+MF4FINkfavVFPCOZhKL1H/OkGe5DgSIycKp8w9aJmoHhB1sB3QTugfn\nRWm5nU/TzsajUzBRMB0GA1UdDgQWBBSaqFjzps1qG+x2DPISjaXTWsTOdDAfBgNV\nHSMEGDAWgBSaqFjzps1qG+x2DPISjaXTWsTOdDAPBgNVHRMBAf8EBTADAQH/MAoG\nCCqGSM49BAMCA0cAMEQCIBDHHWa/uLAVdGFEk82auTmw995+MsRwv52VXLw2Z+ji\nAiAXzOWIcGN8p25uhUN/7v9gEcADGIS4yUiv8gsn/Jk2ow==\n-----END CERTIFICATE-----'
```

**Example 3**: public key published as JWKs:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        jwkFromUrl: https://www.gstatic.com/iap/verify/public_key-jwk
```

## Security considerations

### Setting audience check

Certain JWT providers share JWKs between multiple tenants (like Firebase). They use the `aud` claim of JWT to specify
the intended tenant for the JWT. Setting the `audience` field in the Hasura JWT configuration will make sure that the
`aud` claim from the JWT is also checked during verification. Not doing this check will allow JWTs issued for other
tenants to be valid as well.

In these cases, you **MUST** set the `audience` field to appropriate value. **Failing to do so is a major security
vulnerability**.

## JWT with the WebSocket protocol

When executing a subscription (or query or mutation) over the WebSocket protocol, the authentication step is executed on
`connection_init` when the websocket is connected to Hasura Engine and is valid until the expiry of the JWT when in JWT
mode.

Once authenticated, all operations are allowed without further check, until the authentication expires.

## Popular providers and known issues

### AWS Cognito

AWS Cognito and ELB (Elastic Load Balancer) has a known issue where it adds additional padding (using = characters) to
the JWT token that is generated from Cognito.

This is a known issue and is documented by AWS in
[their docs](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html#user-claims-encoding):

> Standard libraries are not compatible with the padding that is included in the Application Load Balancer
> authentication token in JWT format.

Currently, there is no workaround possible in Hasura. Even if Hasura strips the additional padding the signature
verification of the token would fail (as Hasura had to tamper the token).

### Firebase

Firebase publishes the JWKs at:

[https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com](https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com)

If you are using Firebase and Hasura, use this config:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: Header
        name: Authorization
      key:
        jwkFromUrl: https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com
      issuer: https://securetoken.google.com/<firebase-project-id>
      audience: <firebase-project-id>
```

### Auth0 {#auth0-issues}

Refer to the [Auth0 JWT tutorial](/auth/jwt/tutorials/integrations/1-auth0.mdx) for a detailed guide on integrating
Auth0 with Hasura.

### Clerk

Clerk integrates with Hasura GraphQL Engine using JWTs.

Clerk publishes their JWK under: `https://<YOUR_CLERK_FRONTEND_API>/.well-known/jwks.json`

Refer to the [Clerk JWT tutorial](/auth/jwt/tutorials/integrations/4-clerk.mdx) to set up authenticated requests to
Hasura with Clerk.

### Keycloak

By default, Keycloak uses the `RSA-OAEP` algorithm, which the Hasura DDN engine doesn't support. Remove the algorithm in
the `Realm Settings -> Keys -> Add Providers` tab.



--- File: ../ddn-docs/docs/business-logic/tutorials/1-add-custom-logic.mdx ---
# Add custom logic

---
sidebar_position: 2
sidebar_label: Add custom logic
description:
  "Learn how to add simple custom business logic to your API using lambda connectors and build on this example with your
  own custom logic."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Add Custom Logic

## Introduction

In this tutorial, you'll use a lambda connector to add custom business logic to your supergraph API. The connector and
DDN CLI will automatically determine the argument and return types from your custom logic and add them to your API's
GraphQL schema.

Each connector uses its own conventions to determine whether a function should be exposed as a query or mutation; in
this tutorial, you'll add both.

This tutorial should take about five minutes.

## Step 1. Initialize a new local DDN project

```sh title="Create a new project using the DDN CLI:"
ddn supergraph init lambda-tutorial
```

## Step 2. Initialize the lambda connector

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```bash title="Run the following command:"
ddn connector init my_ts -i
```

- Select `hasura/nodejs` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).

If you open the `app/connector/my_ts` directory, you'll see the `functions.ts` file generated by the CLI; this will be
the entrypoint for your connector.

</TabItem>

<TabItem value="Python" label="Python">

```bash title="Run the following command:"
ddn connector init my_python -i
```

- Select `hasura/python` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).

If you open the `app/connector/my_python` directory, you'll see the `functions.py` file generated by the CLI; this will
be the entrypoint for your connector.

</TabItem>

<TabItem value="Go" label="Go">

```bash title="Run the following command:"
ddn connector init my_go -i
```

- Select `hasura/go` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).

If you open the `app/connector/my_go` directory, you'll see Go files in the `functions` folder; these will serve as the
entrypoint for your connector.

</TabItem>

</Tabs>

## Step 3. Add custom logic

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```bash title="From the connector directory, install the necessary packages:"
cd app/connector/my_ts && npm install
```

```typescript title="Then, replace the contents of functions.ts with the following:"
/**
 * @readonly Exposes the function as an NDC function (the function should only query data without making modifications)
 */
export function hello(name?: string) {
  return `hello ${name ?? "world"}`;
}

/**
 * As this is missing the readonly tag, this will expose the function as an NDC procedure (the function will be exposed as a mutation in the API)
 */
export function encode(username: string) {
  return Buffer.from(username).toString("base64");
}
```

Including the `@readonly` tag ensures `hello()` is exposed as a query in our API. By omitting this from `encode()`,
we're ensuring this second function is exposed as a mutation when we build our API.

Both have typed input arguments and implicitly return strings, which the connector will use to generate the
corresponding GraphQL schema.

</TabItem>

<TabItem value="Python" label="Python">

```python title="Replace the contents of functions.py with the following:"
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector
from pydantic import BaseModel, Field
from hasura_ndc.errors import UnprocessableContent
from typing import Annotated
import base64


connector = FunctionConnector()

@connector.register_query
def hello(name: str) -> str:
    return f"Hello {name}"

@connector.register_mutation
def encode(username: str) -> str:
    return base64.b64encode(username.encode("utf-8")).decode("utf-8")


if __name__ == "__main__":
    start(connector)
```

Using the `@connector.register_query` decorator ensures `hello()` is exposed as a query in our API. By using the
`@connector.register_mutation` decorator with `encode()`, we're ensuring this second function is exposed as a mutation
when we build our API.

Both have typed input arguments and return strings, which the connector will use to generate the corresponding GraphQL
schema.

</TabItem>

<TabItem value="Go" label="Go">

```go title="Add the following to a new file called app/connector/my_go/functions/encode.go:"
package functions

import (
	"context"
	"encoding/base64"

	"hasura-ndc.dev/ndc-go/types"
)

// EncodeUsernameArguments represents the input arguments for encoding a username.
type EncodeUsernameArguments struct {
	Username string `json:"username"`
}

// EncodeUsernameResult represents the result of the Base64 encoding.
type EncodeUsernameResult struct {
	EncodedUsername string `json:"encodedUsername"`
}

// ProcedureEncodeUsername encodes the given username into Base64 format.
func ProcedureEncode(ctx context.Context, state *types.State, arguments *EncodeUsernameArguments) (*EncodeUsernameResult, error) {
	encoded := base64.StdEncoding.EncodeToString([]byte(arguments.Username))
	return &EncodeUsernameResult{
		EncodedUsername: encoded,
	}, nil
}
```

Using the prefix `Procedure` ensures `ProcedureEncode()` is exposed as a mutation in our API. If you open the `hello.go`
file in the `functions` directory, you'll notice the `FunctionHello()` is prefixed differently, identifying it as a
function to be exposed as a query in your API.

Both have typed input arguments and return strings, which the connector will use to generate the corresponding GraphQL
schema.

</TabItem>

</Tabs>

## Step 4. Introspect the source file(s)

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```bash title="Introspect the connector:"
ddn connector introspect my_ts
```

```bash title="Then, we can generate a metadata file for each function using the following command:"
# alternatively, use ddn command add my_ts "*" for bulk adds
ddn command add my_ts hello
ddn command add my_ts encode
```

</TabItem>

<TabItem value="Python" label="Python">

```bash title="Introspect the connector:"
ddn connector introspect my_python
```

```bash title="Then, we can generate a metadata file for each function using the following command:"
# alternatively, use ddn command add my_python "*" for bulk adds
ddn command add my_python hello
ddn command add my_python encode
```

</TabItem>

<TabItem value="Go" label="Go">

```bash title="Introspect the connector:"
ddn connector introspect my_go
```

```bash title="Then, we can generate a metadata file for each function using the following command:"
# alternatively, use ddn command add my_go "*" for bulk adds
ddn command add my_go hello
ddn command add my_go encode
```

</TabItem>

</Tabs>

The commands introspected your connector's entrypoint, identified functions with their argument and return types, and
generated Hasura metadata for each. Look for `Hello.hml` and `Encode.hml` to see the CLI-generated metadata.

## Step 5. Create a new build and test

```bash title="Create a new build:"
ddn supergraph build local
```

```bash title="Start your services:"
ddn run docker-start
```

```bash title="Open your local console:"
ddn console --local
```

```graphql title="Using the GraphiQL explorer, you can now use hello() as a query:"
# Which will return hello, Hasura
query HelloQuery {
  hello(name: "Hasura")
}
```

:::tip Using the Go connector?

The boilerplate `hello()` function requires an argument of `greeting` and for you to include a return type in the query.

:::

```graphql title="And encode() as a mutation:"
# Which will return aGFzdXJh
mutation EncodeMutation {
  encode(username: "hasura")
}
```

## Next steps

Now that you've seen how easy it is to add custom business logic directly to your supergraph API, consider these next
steps:

- [Extend a model](/business-logic/tutorials/2-extend-a-model.mdx)
- [Use env vars](/business-logic/add-env-vars-to-a-lambda.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/2-extend-a-model.mdx ---
# Extend a model

---
sidebar_position: 2
sidebar_label: Extend a model
description:
  "Learn how to extend an existing data source's model to include custom fields returning data from your own custom
  business logic using a lambda connector."
keywords:
  - hasura
  - hasura ddn
  - cicd
  - api deployment
  - business logic
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Extend a Model

## Introduction

In this tutorial, you'll learn how to extend an existing field on a model to enhance its functionality. We'll
demonstrate this using PostgreSQL as the data source, **but the steps apply to any data source supported by Hasura
DDN**. By the end, you'll have created a relationship that integrates custom logic with your API. This process will show
you how to:

- Initialize a DDN project and connect to a data source.
- Add a model to your metadata from a database table.
- Create and implement custom logic in a lambda connector.
- Establish relationships between models and custom commands.

This tutorial should take about ten minutes.

## Step 1. Initialize a new local DDN project

```sh title="Create a new project using the DDN CLI:"
ddn supergraph init lambda-tutorial
```

## Step 2. Prepare the PostgreSQL data

```sh title="In your project directory, run:"
ddn connector init my_pg -i
```

From the dropdown, start typing `PostgreSQL` and hit enter to advance through all the options.

The CLI will output something similar to this:

```plaintext
HINT To access the local Postgres database:
- Run: docker compose -f app/connector/my_pg/compose.postgres-adminer.yaml up -d
- Open Adminer in your browser at http://localhost:5143 and create tables
- To connect to the database using other clients use postgresql://user:password@local.hasura.dev:8105/dev
```

```sh title="Use the hint from the CLI output:"
docker compose -f app/connector/my_pg/compose.postgres-adminer.yaml up -d
```

Run `docker ps` to see on which port Adminer is running. Then, you can navigate to the address below to access it:

```plaintext
http://localhost:<ADMINER_PORT>
```

```sql title="Next, via Adminer select SQL command from the left-hand nav, then enter the following:"
--- Create the table
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  age INT NOT NULL
);

--- Insert some data
INSERT INTO users (name, age) VALUES ('Alice', 25);
INSERT INTO users (name, age) VALUES ('Bob', 30);
INSERT INTO users (name, age) VALUES ('Charlie', 35);
```

You can verify this worked by using Adminer to query all records from the `users` table:

```sql
SELECT * FROM users;
```

```sh title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect my_pg
```

```sh title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add my_pg users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

## Step 3. Initialize the lambda connector

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```bash title="Run the following command:"
ddn connector init my_ts -i
```

- Select `hasura/nodejs` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).

If you open the `app/connector/my_ts` directory, you'll see the `functions.ts` file generated by the CLI; this will be
the entrypoint for your connector.

</TabItem>

<TabItem value="Python" label="Python">

```bash title="Run the following command:"
ddn connector init my_python -i
```

- Select `hasura/python` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).

If you open the `app/connector/my_python` directory, you'll see the `functions.py` file generated by the CLI; this will
be the entrypoint for your connector.

</TabItem>

<TabItem value="Go" label="Go">

```bash title="Run the following command:"
ddn connector init my_go -i
```

- Select `hasura/go` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).

If you open the `app/connector/my_go` directory, you'll see Go files in the `functions` folder; these will serve as the
entrypoint for your connector.

</TabItem>

</Tabs>

## Step 4. Add your custom logic

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```bash title="From the connector directory, install the necessary packages:"
cd app/connector/my_ts && npm install
```

```typescript title="Then, replace the contents of functions.ts with the following:"
/**
 * @readonly
 */
export function shoutName(name: string) {
  return `${name.toUpperCase()}`;
}
```

</TabItem>

<TabItem value="Python" label="Python">

```python title="Replace the contents of functions.py with the following:"
from hasura_ndc import start
from hasura_ndc.function_connector import FunctionConnector
from pydantic import BaseModel, Field
from hasura_ndc.errors import UnprocessableContent
from typing import Annotated


connector = FunctionConnector()

@connector.register_query
def shoutName(name: str) -> str:
    return f"{name}".upper()

if __name__ == "__main__":
    start(connector)
```

</TabItem>

<TabItem value="Go" label="Go">

```go title="Add the following to a new file called app/connector/my_go/functions/shout.go:"
package functions

import (
	"context"
	"strings"

	"hasura-ndc.dev/ndc-go/types"
)

// UppercaseNameArguments represents the input arguments for converting a Name to uppercase.
type UppercaseNameArguments struct {
	Name string `json:"Name"`
}

// FunctionShoutName converts the Name to uppercase and returns it as a string.
func FunctionShoutName(ctx context.Context, state *types.State, arguments *UppercaseNameArguments) (string, error) {
	uppercase := strings.ToUpper(arguments.Name)
	return uppercase, nil
}
```

</TabItem>

</Tabs>

## Step 5. Introspect your lambda connector

<Tabs groupId="source-preference" className="api-tabs">

<TabItem value="TypeScript" label="TypeScript">

```bash title="Introspect the connector:"
ddn connector introspect my_ts
```

```bash title="Then, we can generate a metadata file for each function using the following command:"
# alternatively, use ddn command add my_ts "*" for bulk adds
ddn command add my_ts shoutName
```

</TabItem>

<TabItem value="Python" label="Python">

```bash title="Introspect the connector:"
ddn connector introspect my_python
```

```bash title="Then, we can generate a metadata file for each function using the following command:"
# alternatively, use ddn command add my_python "*" for bulk adds
ddn command add my_python shoutName
```

</TabItem>

<TabItem value="Go" label="Go">

```bash title="Introspect the connector:"
ddn connector introspect my_go
```

```bash title="Then, we can generate a metadata file for each function using the following command:"
# alternatively, use ddn command add my_go "*" for bulk adds
ddn command add my_go shoutName
```

</TabItem>

</Tabs>

## Step 6. Create a relationship

```yaml title="In Users.hml, add the following Relationship object:"
---
kind: Relationship
version: v1
definition:
  name: shoutName # Define a name to expose in the supergraph API
  sourceType: Users # The existing source object type (which also defines the source model Users)
  target:
    command: # The target is a command
      name: ShoutName # The name of the existing command we have defined in metadata
      subgraph: app # The existing subgraph the command is defined in
  mapping:
    - source:
        fieldPath:
          - fieldName: name # The field on the source object type that we want to provide to the target command as an argument
      target:
        argument:
          argumentName: name # The name of the argument on the target command that we want to map to the source field
```

## Step 7. Create a new build and test

```bash title="Create a new build:"
ddn supergraph build local
```

```bash title="Start your services:"
ddn run docker-start
```

```bash title="Open your local console:"
ddn console --local
```

```graphql title="Using the GraphiQL explorer, you can now use shout() as a field on the Users model:"
query UsersWithShoutedName {
  users {
    id
    name
    shoutName
  }
}
```

## Next steps

Now that you know how to extend your existing data sources using custom business logic, check out our advanced use cases
in this section:

- [Formatting datetime objects](/business-logic/tutorials/3-format-datetime-objects.mdx)
- [Hashing passwords](/business-logic/tutorials/3-hash-passwords.mdx)
- [Translating content](/business-logic/tutorials/4-translate-content.mdx)
- [Enriching data with an LLM](/business-logic/tutorials/5-enrich-data-with-an-llm.mdx)
- [Validating credentials](/business-logic/tutorials/6-validate-credentials.mdx)
- [HTTP header forwarding](/business-logic/tutorials/7-http-header-forwarding.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/3-format-datetime-objects.mdx ---
# Format datetime objects

---
sidebar_position: 3
sidebar_label: Format datetime objects
description: "Learn how to easily format datetime objects and send back human-readable formatted data."
keywords:
  - hasura
  - hasura ddn
  - custom business logic
  - recipe
  - guide
seoFrontMatterUpdated: false
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Format Datetime Objects

## Introduction

In this recipe, you'll learn how to convert an existing datetime object from your supergraph into a human-readable
format. This approach is perfect when you want to streamline your frontend by formatting data at the API level, allowing
the UI to easily render the formatted result with minimal effort.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- A [lambda connector](/business-logic/overview.mdx) added to your project.
- A type in your supergraph that is a valid datetime object.

**NB: The type will vary with your database-of-choice, but anything that is
[ISO-8601-compliant](https://www.iso.org/iso-8601-date-and-time-format.html) will generally work for what's listed
below. You can adapt this recipe to fit your individual needs.**

:::

## Recipe

### Step 1. Write the function

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">

    ```typescript title="In your functions.ts file, add the following:"
    /**
    * @readonly
    */
    export function formattedDate(dateString: string): string {
      const date = new Date(dateString);
      return date.toLocaleString("en-US", {
        year: "numeric",
        month: "long",
        day: "2-digit",
        hour: "2-digit",
        minute: "2-digit",
        hour12: true,
      });
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">

    ```python title="In your functions.py file, add the following:"
    from hasura_ndc import start
    from hasura_ndc.function_connector import FunctionConnector
    from datetime import datetime

    connector = FunctionConnector()

    @connector.register_query
    async def formatted_date(dateString: str) -> str:
        date = datetime.fromisoformat(date_string)
        return date.strftime("%B %d, %Y %I:%M %p")

    if __name__ == "__main__":
      start(connector)
    ```

  </TabItem>
  <TabItem value="go" label="Go">

    ```go title="In a Go file inside your functions directory, add the following:"
    package functions

    import (
      "context"
      "fmt"
      "time"

      "hasura-ndc.dev/ndc-go/types"
    )

    // DatetimeArguments defines the input arguments for the function
    type DatetimeArguments struct {
      DateString string `json:"date_string"` // required argument
    }

    // DatetimeResult defines the output result for the function
    type DatetimeResult string

    // FunctionFormattedDate formats a datetime string
    func FunctionFormattedDate(ctx context.Context, state *types.State, arguments *DatetimeArguments) (*DatetimeResult, error) {
      date, err := time.Parse(time.RFC3339, arguments.DateString)
      if err != nil {
        return nil, fmt.Errorf("failed to parse date: %v", err)
      }

      formattedDate := date.Format("January 02, 2006 03:04 PM")
      result := DatetimeResult(formattedDate)
      return &result, nil
    }
    ```

</TabItem>
</Tabs>

### Step 2. Track your function

To add your function, generate the related metadata that will link together any functions in your lambda connector's
source files and your API:

```bash
ddn connector introspect <connector_name>
```

Then, you can generate an `hml` file for the function using the following command:

```bash
ddn command add <connector_name> "*"
```

### Step 3. Create a relationship (optional)

Assuming the input argument's type matches that of a type belonging to one or more of your models, you can create a
relationship to the command. This will enable you to make nested queries that will invoke your custom business logic
using the value of the field from the related model!

Create a relationship in the corresponding model's HML file.

```yaml title="For example, if we have an Orders model:"
---
kind: Relationship
version: v1
definition:
  name: formattedDate
  sourceType: Orders
  target:
    command:
      name: FormattedDate
  mapping:
    - source:
        fieldPath:
          - fieldName: createdAt
      target:
        argument:
          argumentName: dateString
```

### Step 4. Test your function

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

In your project's explorer, you should see the new function exposed as a type and should be able to make a query like
this:

<GraphiQLIDE
  query={`query SimpleFormattedDateQuery {
  formattedDate(dateString: "2023-09-19T10:15:30+02:00")
}`}
  response={`{
  "data": {
    "formattedDate": "September 19, 2023 at 08:15 AM"
  }
}`}
/>

If you created a relationship, you can make a query like this, too:

<GraphiQLIDE
  query={`query OrdersQuery {
  orders {
    id
    status
    formattedDate
  }
}`}
  response={`{
  "data": {
    "orders": [
      {
        "id": "c7406b75-6b24-41e4-9c5b-ff3feada9447",
        "status": "processing",
        "formattedDate": "October 29, 2023 at 05:02 PM"
      },
      {
        "id": "7ff13435-b590-4d6b-957f-f7fd39d4528a",
        "status": "complete",
        "formattedDate": "October 29, 2023 at 05:02 PM"
      },
      {
        "id": "98612470-1feb-4b91-88f7-9289d652ee87",
        "status": "complete",
        "formattedDate": "October 29, 2023 at 05:02 PM"
      },
      {
        "id": "85581445-752a-4aef-9684-b648eb5d5f42",
        "status": "complete",
        "formattedDate": "October 29, 2023 at 05:02 PM"
      },
      {
        "id": "9891596a-a732-4c1c-902c-1a112da48fec",
        "status": "complete",
        "formattedDate": "October 29, 2023 at 05:02 PM"
      }
    ]
  }
}`}
/>

## Wrapping up

In this guide, you learned how to enhance your API and enrich the data it serves for its consumers by incorporating
custom business logic directly into your supergraph. By leveraging lambda connectors with
[relationships](/reference/metadata-reference/relationships.mdx), you can not only add custom business logic, but easily
pass values to it and return this information as part of existing models.

## Learn more about lambda connectors

- [TypeScript](/business-logic/overview.mdx) Node.js connector.
- [Python](/business-logic/overview.mdx) connector.
- [Go](/business-logic/overview.mdx) connector.

## Similar recipes

- [Custom business logic recipes](/business-logic/tutorials/index.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/4-translate-content.mdx ---
# Translate content

---
sidebar_position: 3
sidebar_label: Translate content
description: "Learn how to easily translate content and return multiple languages to your clients."
keywords:
  - hasura
  - hasura ddn
  - custom business logic
  - recipe
  - translation
  - guide
seoFrontMatterUpdated: false
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Translate Content

## Introduction

In this recipe, you'll learn how to translate existing content from your supergraph into another language. This is great
for taking care of translations on the API-end to reach users worldwide. Your supergraph's consumers can choose which
language(s) they want to return, all without worrying about various language configurations on the client-side.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- A [lambda connector](/business-logic/overview.mdx) added to your project.
- A [Google Cloud Translation](https://cloud.google.com/translate) API key.

**NB: This API key is _free_ for the first 500,000 characters each month.**

:::

## Recipe

### Step 1. Write the function

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">

    ```sh title="In your connector's directory, install the Google Cloud Translate package:"
    npm install @google-cloud/translate
    ```

    ```typescript title="In your functions.ts file, add the following:"
    import { v2 } from "@google-cloud/translate";

    // This can also be stored as an environment variable
    // in the connector's .env file.
    const CLOUD_TRANSLATION_API_KEY = "your_cloud_translation_api_key";

    /**
    * @readonly
    */
    export async function translateText(targetLanguage: string, content: string): Promise<string> {
      const translate = new v2.Translate({ key: CLOUD_TRANSLATION_API_KEY });
      const [translation] = await translate.translate(content, targetLanguage);
      return translation;
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">
   
    ```plaintext title="In your connector's directory, add the Google API Python Client package to your requirements.txt:"
    google-api-python-client==v2.146.0
    ```

    ```python title="In your functions.py file, add the following:"
    from hasura_ndc import start
    from hasura_ndc.function_connector import FunctionConnector
    from googleapiclient.discovery import build

    # This can also be stored as an environment variable
    # in the connector's .env file.
    API_KEY = "your_cloud_translation_api_key"

    connector = FunctionConnector()

    @connector.register_query
    def translate_text(target_language: str, content: str) -> str:
        service = build("translate", "v2", developerKey=API_KEY)

        # Make the translation request
        response = service.translations().list(
            target=target_language,
            q=[content]
        ).execute()

        # Return the translated text
        return response['translations'][0]['translatedText']

    if __name__ == "__main__":
        start(connector)
    ```

  </TabItem>
</Tabs>

### Step 2. Track your function

To add your function, generate the related metadata that will link together any functions in your lambda connector's
source files and your API:

```bash
ddn connector introspect <connector_name>
```

Then, you can generate an `hml` file for the function using the following command:

```bash
ddn command add <connector_name> "*"
```

### Step 3. Create a relationship (optional)

Assuming the input argument's type matches that of a type belonging to one or more of your models, you can create a
relationship to the command. This will enable you to make nested queries that will invoke your custom business logic
using the value of the field from the related model!

Create a relationship in the corresponding model's HML file.

```yaml title="For example, if we have a Reviews model:"
---
kind: Relationship
version: v1
definition:
  name: translatedReview
  sourceType: Reviews
  target:
    command:
      name: TranslateText
  mapping:
    - source:
        fieldPath:
          - fieldName: text
      target:
        argument:
          argumentName: content
```

### Step 4. Test your function

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

In your project's explorer, you should see the new function exposed as a type and should be able to make a query like
this:

<GraphiQLIDE
  query={`
query SimpleTranslationQuery {
  translateText(content: "Hello, world!", targetLanguage: "es")
}`}
  response={`{
  "data": {
    "translateText": "Â¡Hola Mundo!"
  }
}`}
/>

If you created a relationship, you can make a query like this, too:

<GraphiQLIDE
  query={`
query ReviewQuery {
  reviews {
    id
    text
    translatedReview(targetLanguage: "es")
  }
}`}
  response={`{
  "data": {
    "reviews": [
      {
        "id": "dc9768a4-673c-11ed-b682-7224baf239e5",
        "text": "Furry logos for the win! So soft, so comfy!",
        "translatedReview": "Â¡Logotipos peludos para triunfar! Â¡Tan suaves y tan cÃ³modos!"
      },
      {
        "id": "fcc86da8-673c-11ed-a17f-7224baf239e5",
        "text": "I love this t-shirt almost as much as I love Hasura.",
        "translatedReview": "Me encanta esta camiseta casi tanto como me encanta Hasura."
      },
      {
        "id": "225c954e-673d-11ed-8105-7224baf239e5",
        "text": "Every time I take a sip I work faster. I don't know if it's the coffee or the mug...",
        "translatedReview": "Cada vez que tomo un sorbo trabajo mÃ¡s rÃ¡pido. No sÃ© si es el cafÃ© o la taza..."
      },
      {
        "id": "3e25c84a-673d-11ed-a82a-7224baf239e5",
        "text": "I think I need more computers because I've run out of sticker space.",
        "translatedReview": "Creo que necesito mÃ¡s computadoras porque me he quedado sin espacio para stickers."
      },
      {
        "id": "587de934-673d-11ed-a7d4-7224baf239e5",
        "text": "I thought I had a big head from using Hasura, but it fits perfectly in this cap.",
        "translatedReview": "PensÃ© que tenÃ­a la cabeza grande por usar Hasura, pero encaja perfectamente en esta gorra."
      },
      {
        "id": "708de0ce-673d-11ed-b2ce-7224baf239e5",
        "text": "Most of my clothes are now Hasura swag. Sorry, not sorry.",
        "translatedReview": "La mayorÃ­a de mi ropa ahora es de Hasura. Lo siento, pero no lo siento."
      },
      {
        "id": "833145c2-673d-11ed-90b8-7224baf239e5",
        "text": "If only all logos were this furry.",
        "translatedReview": "OjalÃ¡ todos los logotipos fueran asÃ­ de peludos."
      },
      {
        "id": "94b4a582-673d-11ed-90b9-7224baf239e5",
        "text": "More cloud. More Hasura.",
        "translatedReview": "MÃ¡s nubes. MÃ¡s Hasura."
      },
      {
        "id": "a6108b8e-673d-11ed-90ba-7224baf239e5",
        "text": "Oh yeah. I ship. ",
        "translatedReview": "Oh, sÃ­. Lo envÃ­o."
      }
    ]
  }
}`}
/>

## Wrapping up

In this guide, you learned how to enhance your API and enrich the data it serves for its consumers by incorporating
custom business logic directly into your supergraph. By leveraging lambda connectors with
[relationships](/reference/metadata-reference/relationships.mdx), you can not only add custom business logic, but easily
pass values to it and return this information as part of existing models.

## Learn more about lambda connectors

- [TypeScript](/business-logic/overview.mdx) Node.js connector.
- [Python](/business-logic/overview.mdx) connector.
- [Go](/business-logic/overview.mdx) connector.

## Similar recipes

- [Custom business logic recipes](/business-logic/tutorials/index.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/3-hash-passwords.mdx ---
# Hash Passwords

---
sidebar_position: 4
sidebar_label: Hash Passwords
description: "Learn how to securely hash passwords and handle them within your API."
keywords:
  - hasura
  - hasura ddn
  - custom business logic
  - password hashing
  - guide
seoFrontMatterUpdated: false
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Hash Passwords

## Introduction

In this recipe, you'll learn how to securely hash passwords to protect user credentials in your application. Password
hashing is an essential part of securing sensitive information before storing it in your database.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- A [lambda connector](/business-logic/overview.mdx) added to your project.
- Familiarity with bcrypt for hashing passwords.

**NB: Bcrypt is a common, well-tested library for password hashing in various languages, and it's widely supported
across many systems.**

:::

## Recipe

### Step 1. Write the function

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">

    ```sh title="In your connector's directory, install the bcrypt package:"
    npm install bcryptjs
    ```

    ```typescript title="In your functions.ts file, add the following:"
    import bcrypt from "bcryptjs";

    export async function hashPassword(password: string): Promise<string> {
      const salt = await bcrypt.genSalt(10);
      const hashedPassword = await bcrypt.hash(password, salt);

      // Add your own logic here to hit your Hasura endpoint and perform an insertion

      return hashedPassword;
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">

    ```plaintext title="In your requirements.txt, add the bcrypt package:"
    bcrypt==4.2.0
    ```

    ```python title="In your functions.py file, add the following:"
    from hasura_ndc import start
    from hasura_ndc.function_connector import FunctionConnector
    import bcrypt

    connector = FunctionConnector()

    @connector.register_mutation
    async def hash_password(password: str) -> str:
        salt = bcrypt.gensalt()
        hashedPassword = bcrypt.hashpw(password.encode("utf-8"), salt).decode("utf-8")

        # Add your own logic here to hit your Hasura endpoint and perform an insertion

        return hashedPassword

    if __name__ == "__main__":
        start(connector)
    ```

  </TabItem>
  <TabItem value="go" label="Go">

    ```sh title="Add the bcrypt package and its dependencies to your connector's go.mod:"
    go get golang.org/x/crypto/bcrypt
    go get golang.org/x/net/idna@v0.26.0
    ```

    ```go title="In a Go file inside your functions directory, add the following:"
    package functions

    import (
      "context"
      "fmt"
      "golang.org/x/crypto/bcrypt"

      "hasura-ndc.dev/ndc-go/types"
    )

    // HashPasswordArguments defines the input arguments for the function
    type HashPasswordArguments struct {
      Password string `json:"password"`
    }

    // HashPasswordResult defines the output result for the function
    type HashPasswordResult string

    // ProcedureHashPassword hashes a password string and returns it as a string result
    func ProcedureHashPassword(ctx context.Context, state *types.State, arguments *HashPasswordArguments) (*HashPasswordResult, error) {
      hashedPassword, err := bcrypt.GenerateFromPassword([]byte(arguments.Password), bcrypt.DefaultCost)
      if err != nil {
        return nil, fmt.Errorf("failed to hash password: %v", err)
      }

      // Add your own logic here to hit your Hasura endpoint and perform an insertion

      result := HashPasswordResult(string(hashedPassword))
      return &result, nil
    }
    ```

</TabItem>
</Tabs>

### Step 2. Track your function

To add your function, generate the related metadata that will link together any functions in your lambda connector's
source files and your API:

```bash
ddn connector introspect <connector_name>
```

Then, you can generate an `hml` file for the function using the following command:

```bash
ddn command add <connector_name> "*"
```

### Step 3. Test your function

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

In your project's explorer, you should see the new function exposed as a type and you should be able to execute a
mutation like this:

<GraphiQLIDE
  query={`mutation HashPassword {
  hashPassword(password: "Thisisthesecure1!")
}`}
  response={`{
  "data": {
    "hashPassword": "$2a$10$0kqTP3HNd72oz8Q/qEA2PeOf.sr8jth/zMuICGmZu1qMCfA5N/b/a"
  }
}`}
/>

## Wrapping up

In this guide, you learned how to enhance your API by securely hashing passwords before storing them in your database.
Your API clients can invoke this mutation and you can handle all of the logic of hashing and inserting the new record
directly from your API.

## Learn more about lambda connectors

- [TypeScript](/business-logic/overview.mdx) Node.js connector.
- [Python](/business-logic/overview.mdx) connector.
- [Go](/business-logic/overview.mdx) connector.

## Similar recipes

- [Custom business logic recipes](/business-logic/tutorials/index.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/6-validate-credentials.mdx ---
# Validate Credentials

---
sidebar_position: 5
sidebar_label: Validate Credentials
description: "Learn how to compare a raw text value to a hashed password as an input to a function."
keywords:
  - hasura
  - hasura ddn
  - custom business logic
  - password comparison
  - guide
seoFrontMatterUpdated: false
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Validate Credentials

## Introduction

In this recipe, you'll learn how to compare a raw text value, such as a user password, with a stored hashed password.
This is critical when authenticating users securely in your application.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- A [lambda connector](/business-logic/overview.mdx) added to your project.
- Familiarity with bcrypt for hashing and comparing passwords.

**NB: The bcrypt library is a secure and widely supported method for password handling across various systems.**

:::

## Recipe

### Step 1. Write the function

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">

    ```sh title="In your connector's directory, install the bcrypt package:"
    npm install bcryptjs
    ```

    ```typescript title="In your functions.ts file, add the following:"
    import bcrypt from "bcryptjs";

    /**
    * @readonly
    */
    export async function comparePassword(password: string, hashedPassword: string): Promise<boolean> {
      return await bcrypt.compare(password, hashedPassword);
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">

    ```plaintext title="In your requirements.txt, add the bcrypt package:"
    bcrypt==4.2.0
    ```

    ```python title="In your functions.py file, add the following:"
    from hasura_ndc import start
    from hasura_ndc.function_connector import FunctionConnector
    import bcrypt

    connector = FunctionConnector()

    @connector.register_query
    async def compare_password(password: str, hashed_password: str) -> bool:
        return bcrypt.checkpw(password.encode("utf-8"), hashed_password.encode("utf-8"))

    if __name__ == "__main__":
        start(connector)
    ```

  </TabItem>
  <TabItem value="go" label="Go">

    ```sh title="Add the bcrypt package and its dependencies to your connector's go.mod:"
    go get golang.org/x/crypto/bcrypt
    go get golang.org/x/net/idna@v0.26.0
    ```

    ```go title="In a Go file inside your functions directory, add the following:"
    package functions

    import (
      "context"
      "golang.org/x/crypto/bcrypt"

      "hasura-ndc.dev/ndc-go/types"
    )

    // ComparePasswordArguments defines the input arguments for the function
    type ComparePasswordArguments struct {
      Password       string `json:"password"`
      HashedPassword string `json:"hashed_password"`
    }

    // ComparePasswordResult defines the output result for the function
    type ComparePasswordResult string

    // FunctionComparePassword compares a password with a hashed password
    func FunctionComparePassword(ctx context.Context, state *types.State, arguments *ComparePasswordArguments) (*ComparePasswordResult, error) {
      err := bcrypt.CompareHashAndPassword([]byte(arguments.HashedPassword), []byte(arguments.Password))
      if err != nil {
        result := ComparePasswordResult("false")
        return &result, nil
      }
      result := ComparePasswordResult("true")
      return &result, nil
    }
    ```

</TabItem>
</Tabs>

### Step 2. Track your function

To add your function, generate the related metadata that will link together any functions in your lambda connector's
source files and your API:

```bash
ddn connector introspect <connector_name>
```

Then, you can generate an `hml` file for the function using the following command:

```bash
ddn command add <connector_name> "*"
```

### Step 3. Create a relationship (optional)

It's a safe assumption that the argument's input type matches that of a `password` field belonging to a User model; you
can create a relationship from the type to the command. This will enable you to make nested queries that will invoke
your custom business logic using the value of the field from the related model!

Create a relationship in the corresponding model's HML file.

```yaml title="For example, if we have a Users model:"
---
kind: Relationship
version: v1
definition:
  name: comparePassword
  sourceType: Users
  target:
    command:
      name: ComparePassword
  mapping:
    - source:
        fieldPath:
          - fieldName: password
      target:
        argument:
          argumentName: hashedPassword
```

### Step 4. Test your function

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

In your project's explorer, you should see the new function exposed as a type and should be able to make a query like
this:

<GraphiQLIDE
  query={`query ComparePassword {
  comparePassword(password: "Thisisthesecure1!", hashedPassword: "$2a$10$0kqTP3HNd72oz8Q/qEA2PeOf.sr8jth/zMuICGmZu1qMCfA5N/b/a")
}`}
  response={`{
  "data": {
    "comparePassword": true
  }
}`}
/>

If you created a relationship, you can make a query like this, too:

<GraphiQLIDE
  query={`query ValidateUser {
  users(where: {email: {_eq: "seandemo@hasura.io"}}) {
    id
    comparePassword(password: "Thisisthesecure1!", hashedPassword: "$2a$10$0kqTP3HNd72oz8Q/qEA2PeOf.sr8jth/zMuICGmZu1qMCfA5N/b/a")
  }
}`}
  response={`{
  "data": {
    "users": [
      {
        "id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
        "comparePassword": true
      }
    ]
  }
}`}
/>

## Wrapping up

In this guide, you learned how to securely compare raw text values to hashed passwords to authenticate users in your
API. By leveraging lambda connectors with [relationships](/reference/metadata-reference/relationships.mdx), you can add
custom business logic to your authentication flows.

## Learn more about lambda connectors

- [TypeScript](/business-logic/overview.mdx) Node.js connector.
- [Python](/business-logic/overview.mdx) connector.
- [Go](/business-logic/overview.mdx) connector.

## Similar recipes

- [Custom business logic recipes](/business-logic/tutorials/index.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/5-enrich-data-with-an-llm.mdx ---
# Enrich data with LLMs

---
sidebar_position: 6
sidebar_label: Enrich data with LLMs
description: "Learn how to make a request to the OpenAI API and handle responses within your API."
keywords:
  - hasura
  - hasura ddn
  - custom business logic
  - openai
  - llm
  - ai
  - guide
seoFrontMatterUpdated: false
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Enrich data with LLMs

## Introduction

In this recipe, you'll learn how to interact with OpenAI's API to send prompts and receive responses. This can be used
to integrate AI-driven features, such as generating text or completing tasks, into your API. In the example below, we'll
hard-code a prompt and have OpenAI apply it to existing content in our supergraph.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- A [lambda connector](/business-logic/overview.mdx) added to your project.
- An OpenAI API key (You can get one by signing up at [OpenAI](https://beta.openai.com/signup)).

:::

## Recipe

### Step 1. Write the function

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">

    ```sh title="In your connector's directory, install the OpenAI package:"
    npm install openai
    ```

    ```typescript title="In your functions.ts file, add the following:"
    // We can store this in the project's .env file and reference it here
    const OPENAI_API_KEY =
      "your_openai_api_key";

    const client = new OpenAI({
      apiKey: OPENAI_API_KEY,
    });

    /**
    * @readonly
    */
    export async function generateSeoDescription(input: string): Promise<string | null> {
      const response = await client.chat.completions.create({
        messages: [
          {
            role: "system",
            content:
              "You are a senior marketing associate. Take the product description provided and improve upon it to rank well with SEO.",
          },
          { role: "user", content: input },
        ],
        model: "gpt-4o",
      });

      return response.choices[0].message.content;
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">

    ```plaintext title="In your connector's directory, add the OpenAI Python client package to your requirements.txt:"
    openai==1.46.1
    ```

    ```python title="In your functions.py file, add the following:"
    from hasura_ndc import start
    from hasura_ndc.function_connector import FunctionConnector
    from openai import OpenAI

    connector = FunctionConnector()

    # We can store this in the project's .env file and referene it here
    OPENAI_API_KEY = "your_openai_api_key"
    client = OpenAI(
        api_key=OPENAI_API_KEY,
    )

    @connector.register_query
    def generate_seo_description(input: str) -> str:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a senior marketing associate. Take the product description provided and improve upon it to rank well with SEO."},
                {"role": "user", "content": input}
            ],
        )
        return response.choices[0].message.content

    if __name__ == "__main__":
        start(connector)
    ```

  </TabItem>
  <TabItem value="go" label="Go">

    ```go title="In a Go file inside your functions directory, add the following:"
    package functions

    import (
      "bytes"
      "context"
      "encoding/json"
      "fmt"
      "io/ioutil"
      "net/http"

      "hasura-ndc.dev/ndc-go/types"
    )

    type SEOArguments struct {
      Input string `json:"input"`
    }

    // OpenAIRequest represents the request payload for OpenAI's Chat API
    type OpenAIRequest struct {
      Model    string    `json:"model"`
      Messages []Message `json:"messages"`
    }

    // Message defines the structure for messages sent to OpenAI
    type Message struct {
      Role    string `json:"role"`
      Content string `json:"content"`
    }

    // OpenAIResponse represents the response from OpenAI
    type OpenAIResponse struct {
      Choices []struct {
        Message struct {
          Content string `json:"content"`
        } `json:"message"`
      } `json:"choices"`
    }

    // GenerateSeoDescription sends a request to OpenAI and generates an SEO-optimized product description
    func FunctionGenerateSeoDescription(ctx context.Context, state *types.State, arguments *SEOArguments) (string, error) {
      // We can store this in the project's .env file and reference it here
      apiKey := "your_openai_api_key"

      // Prepare the request payload
      reqBody, _ := json.Marshal(OpenAIRequest{
        Model: "gpt-4",
        Messages: []Message{
          {Role: "system", Content: "You are a senior marketing associate. Take the product description provided and improve upon it to rank well with SEO."},
          {Role: "user", Content: arguments.Input},
        },
      })

      // Create a new request to the OpenAI Chat API
      req, _ := http.NewRequest("POST", "https://api.openai.com/v1/chat/completions", bytes.NewBuffer(reqBody))
      req.Header.Set("Content-Type", "application/json")
      req.Header.Set("Authorization", "Bearer "+apiKey)

      client := &http.Client{}
      resp, err := client.Do(req)
      if err != nil {
        return "", fmt.Errorf("failed to send request: %v", err)
      }
      defer resp.Body.Close()

      // Parse the response from OpenAI
      body, _ := ioutil.ReadAll(resp.Body)
      var openAIResp OpenAIResponse
      json.Unmarshal(body, &openAIResp)

      return openAIResp.Choices[0].Message.Content, nil
    }
    ```

</TabItem>
</Tabs>

### Step 2. Track your function

To add your function, generate the related metadata that will link together any functions in your lambda connector's
source files and your API:

```bash
ddn connector introspect <connector_name>
```

Then, you can generate an `hml` file for the function using the following command:

```bash
ddn command add <connector_name> "*"
```

### Step 3. Create a relationship (optional)

Assuming the input argument's type matches that of a type belonging to one or more of your models, you can create a
relationship to the command. This will enable you to make nested queries that will invoke your custom business logic
using the value of the field from the related model!

Create a relationship in the corresponding model's HML file.

```yaml title="For example, if we have a Prompts model:"
---
kind: Relationship
version: v1
definition:
  name: optimizedDescription
  sourceType: Products
  target:
    command:
      name: generateSeoDescription
  mapping:
    - source:
        fieldPath:
          - fieldName: description
      target:
        argument:
          argumentName: input
```

### Step 4. Test your function

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

In your project's explorer, you should see the new function exposed as a type and should be able to make a query like
this:

<GraphiQLIDE
  query={`query SimpleOpenAIQuery {
  generateSeoDescription(input: "A cool, red hat.")
}`}
  response={`{
  "data": {
    "generateSeoDescription": "Experience ultimate style and comfort with our vibrant, cherry-red hat. This top-rated, trendy headwear is designed with exceptional quality, ensuring it's not only a fashion statement but also functional. Perfect for all seasons, our stylish red hat will match any outfit and can ignite any look with a pop of color. Ideal for casual wear or special occasions, this must-have accessory is an addition worth making to your wardrobe. Stand out in the crowd and express your personality with our aesthetically pleasing, durable, and unique red hat."
  }
}`}
/>

If you created a relationship, you can make a query like this, too:

<GraphiQLIDE
  query={`query ProductsWithOptimizedDescriptions {
  products {
    id
    description
    optimizedDescription
  }
}`}
  response={`{
  "data": {
    "products": [
      {
        "id": "e0a70b16-65b6-11ed-8788-8fa2504d64a3",
        "description": "6 stickers all about that Hasura life. Featuring a few Hasuras and a lot of attitude.",
        "optimizedDescription": "Experience the spirit of Hasura with our premium pack of 6 high-quality stickers. Each uniquely designed sticker embodies the Hasura lifestyle, featuring dynamically illustrated Hasura icons infused with a whole lot of attitude. Perfect for decorating your laptop, phone case, or workplace to showcase your Hasura passion. Not just a product, it's an expression of your love for Hasura!"
      },
      {
        "id": "fef9c02c-65b6-11ed-be19-2b4fad811971",
        "description": "Keep Hasura on your mind and the sun outta your eyes",
        "optimizedDescription": "Stay ahead with Hasura: Your ultimate solution for deflecting sun rays from your eyes. Our top-ranked, high-quality Hasura shades not only help protect your eyes but also offer a superior blend of style and performance. Don't miss out on the chance to enhance your vision and style quotient with our exceptional Hasura products. Rated as the best in the market, Hasura continues to serve its users with impeccable functionality, ensuring comfort all day long. Make the smart choice - choose Hasura. Stay cool, stay UV-protected, stay trendy with Hasura."
      },
      {
        "id": "3bef8a40-3c33-11ee-bb29-070df467ec94",
        "description": "When you want to keep it simple",
        "optimizedDescription": "Experience the essence of minimalist lifestyle with our Simplify Living Range. Crafted to appeal to those seeking a less cluttered life, our product lineup is designed to enhance aesthetics while maximizing efficiency. Chase the calm and cut the clutter with our Simplify Living Range - optimally designed for a chic yet functional living space."
      },
      {
        "id": "7992fdfa-65b5-11ed-8612-6a8b11ef7372",
        "description": "When you want to keep it simple",
        "optimizedDescription": "Experience the essence of minimalist lifestyle with our Simplify Living Range. Crafted to appeal to those seeking a less cluttered life, our product lineup is designed to enhance aesthetics while maximizing efficiency. Chase the calm and cut the clutter with our Simplify Living Range - optimally designed for a chic yet functional living space."
      },
      {
        "id": "a44eda7c-65b6-11ed-997b-53b5bdb7117e",
        "description": "It's Hasura but on someone else's computer ;)",
        "optimizedDescription": "Experience the power of Hasura like never before on a cloud-based platform. Our innovative product allows you to interact with Hasura in an easily accessible, remote environment. Get all the functionalities and convenience of Hasura right at your fingertips, no matter where you are! This solution is perfect for those who are constantly on the go or prefer the comfort of their own devices. Enjoy all the robust features of the Hasura platform on any computer at any time. Discover how seamless, efficient and productive cloud-based solutions can truly be with this versatile Hasura offering. This is not just Hasura - it's Hasura redefined, revolutionized, and reimagined for your utmost convenience and productivity."
      },
      {
        "id": "8aa93f86-65b6-11ed-901c-f320d4e17bb2",
        "description": "A little darker, a little fun..er",
        "optimizedDescription": "Experience a darker, yet more enjoyable ambiance with our quirky and captivating product. Enjoy the unique blend of mystery and fun that it seamlessly brings into your everyday life. Get lost in its intriguing darkness that's definitely an upgrade from the ordinary - a perfect balance between somber shades and light-hearted joy. Come unearth this rare find, perfect for those who dare to defy the mundane. Enter a world less common, a world darker, yet more exhilarating."
      },
      {
        "id": "cd6be51c-65b6-11ed-a2f4-4b71f0d3d70f",
        "description": "A little reminder every time you take a sip",
        "optimizedDescription": "Experience a delightful reminder with each invigorating sip you take. Discover the joy of our premium-quality, taste-infused beverage that refreshes your senses every time. Our drink is more than just hydration, it's a sensory experience unlike any other. Unleash the potential of a perfect sip with our product and make each moment count. Perfect for individuals who value quality and memorable experiences. Transform your ordinary sip into an extraordinary taste affair."
      }
    ]
  }
}`}
/>

## Wrapping up

In this guide, you learned how to send prompts to OpenAI's API and receive responses in your application. By leveraging
lambda connectors with [relationships](/reference/metadata-reference/relationships.mdx), you can easily incorporate
AI-driven capabilities into your existing supergraph.

## Learn more about lambda connectors

- [TypeScript](/business-logic/overview.mdx) Node.js connector.
- [Python](/business-logic/overview.mdx) connector.
- [Go](/business-logic/overview.mdx) connector.

## Similar recipes

- [Custom business logic recipes](/business-logic/tutorials/index.mdx)



--- File: ../ddn-docs/docs/business-logic/tutorials/7-http-header-forwarding.mdx ---
# HTTP Header Forwarding

---
sidebar_position: 7
sidebar_label: HTTP Header Forwarding
description: "Learn how to configure Hasura DDN to forward HTTP headers to a lambda connector"
keywords:
  - hasura
  - hasura ddn
  - custom business logic
  - http headers
  - guide
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# HTTP Header Forwarding

## Introduction

Hasura DDN can be configured to forward HTTP request headers to functions implemented in a lambda connector. It can also
be configured to respond to HTTP requests involving the lambda connector with HTTP headers returned by functions in the
lambda connector.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- A [lambda connector](/business-logic/overview.mdx) added to your project.

:::

## Recipe - Receiving HTTP request headers

To configure your functions to receive HTTP request headers, perform the following steps:

### Step 1. Add a `headers` function parameter

First, you should modify all functions that you want to receive HTTP request headers and add a `headers` function
parameter, like so:

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">
    ```typescript title="An example hello function in your functions.ts, updated with a headers parameter"
    import * as sdk from "@hasura/ndc-lambda-sdk";

    /** @readonly */
    export function hello(headers: sdk.JSONValue, name?: string): string {
      const headersMap = headers.value as Record<string, string>;
      return `hello ${name ?? "world"}`;
    }
    ```

    The `headers` function parameter will be passed as a JSON object that represents the HTTP request headers. To extract it, we cast the `headers.value` property to the `Record<string, string>` type.

  </TabItem>
  <TabItem value="go" label="Go">
    ```go title="An example hello function in your functions.go, updated with a headers parameter"
    type HelloArguments struct {
      Headers map[string]string `json:"headers,omitempty"`
      Name    string            `json:"name"`
    }

    func FunctionHello(ctx context.Context, state *types.State, arguments *HelloArguments) (string, error) {
      log.Printf("request headers: %v", arguments.Headers)
      name := arguments.Name
      if name == "" {
        name = "world"
      }
      return "hello " + name, nil
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">
    Support coming soon!
  </TabItem>
</Tabs>

:::note

You don't need to call the function parameter `headers`. You can use any name you wish, but it must be the same name
across all functions and it must always be used to receive HTTP headers.

:::

### Step 2. Update the metadata

Next, we need to re-introspect the connector given that we just changed the definition of our functions.

```bash title="Introspect the connector"
ddn connector introspect my_ts
```

Then, we need to open the HML file that contains the `DataConnectorLink` metadata object for the connector (usually
found in `<subgraph name>/metadata/<connector name>.hml`) and edit it.

:::note

From CLI v2.18.0, you can use the codemod `ddn codemod configure-header-forwarding` to configure the `DataConnectorLink`
to forward headers to the connector.

:::

We use
[argument presets](/reference/metadata-reference/data-connector-links.mdx#dataconnectorlink-dataconnectorargumentpreset)
to automatically set our `headers` function parameters with the HTTP request headers. Below is an example where we
forward the `X-Test-Header` header to the connector. **Keep in mind that only headers listed here are forwarded to the
connector.**

```yaml title=my_ts.hml
kind: DataConnectorLink
version: v1
definition:
  name: my_ts
  url:
    readWriteUrls:
      read:
        valueFromEnv: MY_SUBGRAPH_MY_TS_READ_URL
      write:
        valueFromEnv: MY_SUBGRAPH_MY_TS_WRITE_URL
  headers:
    Authorization:
      valueFromEnv: MY_SUBGRAPH_MY_TS_AUTHORIZATION_HEADER
  # highlight-start
  argumentPresets:
    - argument: headers
      value:
        httpHeaders:
          forward:
            - X-Test-Header
          additional: {}
  # highlight-end
  schema: ...
```

### Step 3. Create a new API build and test

Now, we can rebuild the supergraph and test our changes:

```bash title="Run:"
ddn supergraph build local
```

:::warning `headers` already mapped error

If you get a build error saying "the argument headers is mapped to the data connector argument headers which is already
used as an argument preset in the DataConnectorLink" then you need to open HML file containing the `Command` mentioned
in the error and remove the headers argument from the `Command`'s `arguments` list and from the `argumentMapping`, if it
exists.

This is because when you preset arguments in your `DataConnectorLink`, they are automatically set by the DDN engine and
therefore are not defined on the `Command` as arguments that API users can set.

:::

:::tip Start your engines!

Don't forget to start your GraphQL engine using the following command.

```bash title="From the root of your project, run:"
ddn run docker-start
```

This reads the `docker-start` script from the context config at `.hasura/context.yaml` and starts your Hasura engine,
any connectors, and observability tools.

:::

Launch the Hasura console to see and test your GraphQL API using:

```bash title="Run:"
ddn console --local
```

## Recipe - Returning HTTP response headers

To configure your lambda connector to return HTTP response headers, perform the following steps:

### Step 1. Modify the function return type

First, you should add a helper type that you can re-use that will contain your headers as well as the actual value you
want to return from your function:

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">
    ```typescript title="In your functions.ts, add the following"
    type HeadersResponse<T> = {
      headers: sdk.JSONValue
      response: T
    }
    ```
  </TabItem>
    <TabItem value="go" label="Go">
    ```go title="In your functions.go, add the following"
    type HeadersResponse[T any] struct {
      Headers  map[string]string `json:"headers"`
      Response T                 `json:"response"`
    }
    ```
  </TabItem>
  <TabItem value="python" label="Python">
    Support coming soon!
  </TabItem>

</Tabs>

Then, you can modify your function return value to use this type. In the following example, we take the headers received
in the previous section and add an additional header `X-Test-ResponseHeader` to be returned on the response. We then
return that, as well as our response string, inside our new `HeadersResponse` object.

<Tabs className="language-tabs">
  <TabItem value="ts" label="TypeScript">
    ```typescript title="An example hello function in your functions.ts, updated to return a HeadersResponse type"
    /** @readonly */
    export function hello(headers: sdk.JSONValue, name?: string): HeadersResponse<string> {
      const headersMap = headers.value as Record<string, string>;
      headersMap["X-Test-ResponseHeader"] = "I set this in the code";
      return {
        headers: new sdk.JSONValue(headersMap),
        response: `hello ${name ?? "world"}`
      };
    }
    ```
  </TabItem>
    <TabItem value="go" label="Go">
    ```go title="An example hello function in your functions.go, updated to return a HeadersResponse type"
    func FunctionHello(ctx context.Context, state *State, arguments *HelloArguments) (HeadersResponse[string], error) {
      headersMap := arguments.Headers
      if headersMap == nil {
        headersMap = map[string]string{}
      }
      headersMap["X-Test-ResponseHeader"] = "I set this in the code"

      name := arguments.Name
      if name == "" {
        name = "world"
      }

      return HeadersResponse[string]{
        Headers:  headersMap,
        Response: "hello " + name,
      }, nil
    }
    ```

  </TabItem>
  <TabItem value="python" label="Python">
    Support coming soon!
  </TabItem>

</Tabs>

### Step 2. Update the metadata

Next, we need to re-introspect the connector given that we just changed the definition of our functions.

```bash title="Introspect the connector"
ddn connector introspect my_ts
```

Then, we need to open the HML file that contains the `DataConnectorLink` metadata object for the connector (usually
found in `<subgraph name>/metadata/<connector name>.hml`) and edit it.

We will be using the
[`responseHeaders` configuration property](/reference/metadata-reference/data-connector-links.mdx#dataconnectorlink-responseheaders)
to configure which headers returned by our connector functions we want returned as a part of our HTTP response headers.
In the below example, the `X-Test-Header` and `X-Test-ResponseHeader` headers are listed under `forwardHeaders` to
ensure they are added to the HTTP response if they are returned by the connector function. We also set the
`headersField` and `resultField` properties to the two property names we defined on the `HeadersResponse` type we
defined earlier.

```yml title=my_ts.hml
kind: DataConnectorLink
version: v1
definition:
  name: my_ts
  url:
    readWriteUrls:
      read:
        valueFromEnv: MY_SUBGRAPH_MY_TS_READ_URL
      write:
        valueFromEnv: MY_SUBGRAPH_MY_TS_WRITE_URL
  headers:
    Authorization:
      valueFromEnv: MY_SUBGRAPH_MY_TS_AUTHORIZATION_HEADER
  argumentPresets:
    - argument: headers
      value:
        httpHeaders:
          forward:
            - X-Test-Header
          additional: {}
  # highlight-start
  responseHeaders:
    headersField: headers
    resultField: response
    forwardHeaders:
      - X-Test-Header
      - X-Test-ResponseHeader
  # highlight-end
  schema: ...
```

### Step 3. Create a new API build and test

Now, we can rebuild the supergraph and test our changes:

```bash title="Run:"
ddn supergraph build local
```

:::warning type is not defined in the agent schema error

The [`outputType`](/reference/metadata-reference/commands.mdx#command-commandv1) of `Command`s that represent our
functions should be the [OpenDD Scalar Type](/reference/metadata-reference/types.mdx) used to represent type of the
`HeadersResponse.response` property, not the `HeadersResponse` type itself.

So, using our above example, the `Hello` `Command`'s `outputType` should be `String!`, not `HeaderResponseString!`. If
it is incorrectly configured, you may get a build error such as "NDC validation error: type String is not defined in the
agent schema".

:::

:::tip Start your engines!

Don't forget to start your GraphQL engine using the following command.

```bash title="From the root of your project, run:"
ddn run docker-start
```

This reads the `docker-start` script from the context config at `.hasura/context.yaml` and starts your Hasura engine,
any connectors, and observability tools.

:::

Launch the Hasura console to see and test your GraphQL API using:

```bash title="Run:"
ddn console --local
```



--- File: ../ddn-docs/docs/auth/jwt/tutorials/index.mdx ---
# Authentication

---
sidebar_position: 1
sidebar_label: Authentication
description: "Learn how to connect to various authentication providers with Hasura DDN."
keywords:
  - hasura
  - hasura ddn
  - authentication
seoFrontMatterUpdated: false
---

# Authentication

## Introduction

In this section of tutorials, we'll provide you with concise up-to-date descriptions of how to connect your preferred
authentication provider to Hasura DDN.

## Tutorials

- [Auth0](/auth/jwt/tutorials/integrations/1-auth0.mdx)
- [AWS Cognito](/auth/jwt/tutorials/integrations/2-aws-cognito.mdx)
- [Firebase](/auth/jwt/tutorials/integrations/3-firebase.mdx)
- [Clerk](/auth/jwt/tutorials/integrations/4-clerk.mdx)



--- File: ../ddn-docs/docs/auth/jwt/tutorials/setup-test-jwt.mdx ---
# Set up a Test JWT

---
description: "Learn how to set up a test token and jwt for graphql apis"
keywords:
  - hasura
  - authconfig
  - session variables
  - http request headers
  - graphql API security
  - test token
sidebar_position: 4
sidebar_label: Set up a Test JWT
seoFrontMatterUpdated: true
---

# Set up a JWT for Testing

By default, your supergraph uses your Hasura Cloud authentication token, also known as a personal access token (PAT),
for authentication. This is convenient for testing from the console but should not be used in an application or shared
with others.

Instead, to test authentication you need to set up an
[`AuthConfig` object](/reference/metadata-reference/auth-config.mdx) and then generate the corresponding token.

## Step 1: Install the jwt-cli

Install the [`jwt-cli`](https://github.com/mike-engel/jwt-cli), which allows you to generate tokens from the command
line. You can follow their list of installation instructions found
[here](https://github.com/mike-engel/jwt-cli?tab=readme-ov-file#installation).

## Step 2: Generate a random string

Generate a random string that we'll use as the JWT secret key:

```bash title="In your teminal, run the following command"
openssl rand -hex 16
```

Copy the value returned by the terminal.

:::info Creating a random string

If you don't want to use openssl, you can use any other random string generators. The only requirement is that the
string must be at least 32 characters.

:::

## Step 3: Set up your AuthConfig object

Set up an `AuthConfig` object in your project which uses this secret key.

```yaml title="In globals/metadata/auth-config.hml:"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      key:
        fixed:
          algorithm: HS256
          key:
            value: "<insert-the-key-generated-in-previous-step>"
      tokenLocation:
        type: Header
        name: Auth-Token
```

## Step 4: Create a new supergraph build

Create a supergraph build using this `AuthConfig`.

```bash title="From the root of your project, run:"
ddn supergraph build create \
  --description "use jwt-based authconfig" \
  --supergraph supergraph.yaml
```

## Step 5: Generate a JWT

For testing, you can use the `jwt-cli` to encode and generate a new token with the different claims written to match
your testing needs.

```bash title="Run the following with your own values:"
jwt encode --secret="<secret-key>" '{"exp": 1739905122,"iat": 1708369122,"claims.jwt.hasura.io":{"x-hasura-default-role": "admin","x-hasura-allowed-roles":["admin"]}}'
```

In the example above, we're setting the following values:

- The issued (`iat`) time as `Feb. 19 2024, at 18:58:42` as a Unix epoch timestamp.
- The expiration (`exp`) time as `Feb. 18, 2025 at 18:58:42`.
- The default role as `admin`.
- The allowed roles as `admin`.

For more information about the claims Hasura expects, check out [this page](/auth/jwt/jwt-configuration.mdx).

## Step 6: Test your AuthConfig

In the Hasura console, add the JWT generated by the console as the value of a new header called `Auth-token` on the
GraphiQL explorer. You should now be able to execute queries with your custom JWT.

:::info Using environment variables

If you're storing your secret key's value as an environment variable, ensure you've updated the `subgraph.yaml` in the
`globals` subgraph to include this envMapping.

:::



--- File: ../ddn-docs/docs/auth/jwt/tutorials/integrations/index.mdx ---
# Authentication

---
sidebar_position: 1
sidebar_label: Authentication
description: "Learn how to connect to various authentication providers with Hasura DDN."
keywords:
  - hasura
  - hasura ddn
  - authentication
seoFrontMatterUpdated: false
---

# Authentication

## Introduction

In this section of tutorials, we'll provide you with concise up-to-date descriptions of how to connect your preferred
authentication provider to Hasura DDN.

## Tutorials

- [Auth0](/auth/jwt/tutorials/integrations/1-auth0.mdx)
- [AWS Cognito](/auth/jwt/tutorials/integrations/2-aws-cognito.mdx)
- [Firebase](/auth/jwt/tutorials/integrations/3-firebase.mdx)
- [Clerk](/auth/jwt/tutorials/integrations/4-clerk.mdx)



--- File: ../ddn-docs/docs/auth/jwt/tutorials/integrations/1-auth0.mdx ---
# Auth0

---
sidebar_position: 2
sidebar_label: Auth0
description: "Learn how to connect Auth0 to your Hasura DDN supergraph."
keywords:
  - hasura
  - hasura ddn
  - authentication
  - jwt
  - auth0
  - tutorial
  - guide
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# Auth0

## Introduction

In this tutorial, you'll learn how to configure an existing [Auth0 application](https://auth0.com) and generate a JWT
which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- An Auth0 [application](https://manage.auth0.com/dashboard).
- A local application that you're actively developing, built with any language or framework supported by
  [Auth0's SDKs](https://auth0.com/docs/libraries).
- A local Hasura DDN project.

:::

## Tutorial

### Step 1. Create a new Auth0 application

From your Auth0 dashboard, click `Applications` in the sidebar and then click on `Create Application`. Enter a name for
your application, then choose the application type that best suits your needs.

After creating the application, go to `APIs` in the sidebar and create a new API with your GraphQL endpoint as the
identifier.

### Step 2. Create a new Auth0 Action

From your Auth0 dashboard, click `Actions` in the sidebar and choose `Triggers`.

Under `Sign Up & Login`, select the `post-login` trigger, click on the `+` icon, and then choose `Built from Scratch` to
create a new Action.

Enter a name for your Action such as `Hasura JWT Claims` and paste the following code:

```javascript
exports.onExecutePostLogin = async (event, api) => {
  const namespace = "claims.jwt.hasura.io";
  // Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
  // Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
  // Below, we're hard-coding the value for now
  const user_role = "user"; // the role returned from your request â˜ï¸
  api.idToken.setCustomClaim(namespace, {
    "x-hasura-default-role": user_role,
    "x-hasura-allowed-roles": [user_role],
    "x-hasura-user-id": event.user.user_id,
    // Add any other custom claims you wish to include
  });

  // Set the necessary access token claims for Hasura to authenticate the user
  api.accessToken.setCustomClaim(namespace, {
    "x-hasura-default-role": user_role,
    "x-hasura-allowed-roles": [user_role],
    "x-hasura-user-id": event.user.user_id,
  });
};
```

This will add the required Hasura namespace with the keys that Hasura DDN expects when decoding a JWT. You can modify
the keys to suit your Hasura DDN [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

Click `Deploy`.

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura DDN Engine.

:::

### Step 3. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your
[Auth0 JWKs](https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-key-sets):

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      issuer: "<your Auth0 tenant's URL>"
      key:
        jwkFromUrl: "https://<your Auth0 tenant's URL>/.well-known/jwks.json"
      audience: ["<your GraphQL endpoint>"]
      tokenLocation:
        type: Header
        name: Auth-Token
```

Then, create a new build of your supergraph:

```sh
ddn supergraph build local
```

### Step 4. Test your configuration

The easiest way to verify your setup is to generate a new JWT by logging into your client application that uses Auth0.
These values aren't typically displayed to users, so you'll need to log them while in development. You can then add that
value as a header in the console and test any permissions you have in your metadata. If you're unfamiliar with this, we
have a sample repo [here](https://github.com/hasura/ddn-auth-examples/tree/main/auth0).

:::info Auth0 API Debugger

Auth0 does provide an extension: the `Auth0 Authentication API Debugger` for testing configurations. However, custom
claims have been known to cause issues. You can find more information about using their debugger
[here](https://auth0.com/docs/customize/extensions/authentication-api-debugger-extension).

:::

### Step 5. Service account access token (optional)

In certain cases, you may need to generate a service account access token to access your Hasura DDN supergraph when
performing backend operations. You can do this by creating a new Auth0 application named `Service Account` with a type
of `Machine to Machine Applications`.

After creating the application, go to `Triggers`, located underneath `Actions` in the sidebar, and click on the
`credentials-exchange` trigger. Similar to the `post-login` trigger, create a new Action and paste the code below:

```javascript
exports.onExecuteCredentialsExchange = async (event, api) => {
  const namespace = "claims.jwt.hasura.io";

  const service_role = "service_account";

  api.accessToken.setCustomClaim(namespace, {
    "x-hasura-default-role": service_role,
    "x-hasura-allowed-roles": [service_role],
  });
};
```

This will generate a new JWT token with the `service_account` role which can then be used to access your Hasura DDN
supergraph.

You can generate a new access token using the following Python code:

```python
import http.client

conn = http.client.HTTPSConnection("<Auth0 domain>")

payload = "{\"client_id\":\"<client id>\",\"client_secret\":\"<Auth0 Client Secret>\",\"audience\":\"<your GraphQL endpoint>\",\"grant_type\":\"client_credentials\"}"

headers = { 'content-type': "application/json" }

conn.request("POST", "/oauth/token", payload, headers)

res = conn.getresponse()
data = res.read()

print(data.decode("utf-8"))
```

The response will look like:

```json
{
  "access_token": "<service account access token>",
  "token_type": "Bearer"
}
```

You can modify your metadata to allow the `service_account` role to access the necessary models.

:::tip Best Practices for Service Accounts

Instead of granting a service account full admin access, create a custom role with only the necessary permissions. This
approach follows the principle of least privilege, thereby limiting potential impact in case of any errors or
oversights.

:::

## Wrapping up

In this guide, you learned how to integrate Auth0 with Hasura DDN to create a secure and scalable identity management
solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional Auth0 features that can enhance your authentication flows.



--- File: ../ddn-docs/docs/auth/jwt/tutorials/integrations/2-aws-cognito.mdx ---
# AWS Cognito

---
sidebar_position: 3
sidebar_label: AWS Cognito
description: "Learn how to connect AWS Cognito to your Hasura DDN supergraph."
keywords:
  - hasura
  - hasura ddn
  - authentication
  - jwt
  - cognito
  - aws
  - tutorial
  - guide
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# AWS Cognito

## Introduction

In this tutorial, you'll learn how to configure an existing
[AWS Cognito user pool](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools.html) and generate
a JWT which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- An AWS Cognito [user pool](https://manage.auth0.com/dashboard) with a domain configured.
- A local application that can integrate with Cognito for authentication.
- A local Hasura DDN project.

:::

## Tutorial

### Step 1. Create a Lambda trigger for modifying JWT claims

To add the custom claims that Hasura requires, you will need to create an AWS Lambda function and set it as a trigger
for your Cognito user pool.

In the [AWS Lambda console](https://console.aws.amazon.com/lambda/home), create a new Lambda function. Select the
`Author from scratch` option and provide a name, runtime, architecture, and any advanced settings you wish to configure.

After your Lambda is created, you'll be redirected to an editor where you can modify the Lambda's handler. Add the
following code to modify the Cognito JWT and inject the custom Hasura namespace claims:

```javascript
export const handler = async (event) => {
  // Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
  // Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
  // Below, we're hard-coding the value for now
  const user_role = "user"; // the role returned from your request â˜ï¸
  event.response = {
    claimsOverrideDetails: {
      claimsToAddOrOverride: {
        "claims.jwt.hasura.io": JSON.stringify({
          "x-hasura-user-id": event.request.userAttributes.sub,
          "x-hasura-default-role": user_role,
          "x-hasura-allowed-roles": ["user"],
        }),
      },
    },
  };

  return event;
};
```

This will add the required Hasura namespace with the keys that Hasura DDN expects when decoding a JWT. You can modify
the keys to suit your Hasura DDN [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

Click `Deploy`.

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura DDN Engine.

:::

### Step 2. Add the Lambda as an Authentication trigger

From your user pool's dashboard, select the `User pool properties` tab and then click `Add Lambda trigger`. Choose
`Authentication` as the trigger type and choose `Pre token generation trigger`.

Then, select the Lambda you generated in the previous step and click `Add Lambda trigger`.

### Step 3. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your
[Cognito JWKs](https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html),
which you can find on your User pool overview card:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: StringifiedJson
          location: "/claims.jwt.hasura.io"
      key:
        jwkFromUrl: "https://cognito-idp.<your_region>.amazonaws.com/<your_region>_<your_user_pool_id>.well-known/jwks.json"
      tokenLocation:
        type: Header
        name: Auth-Token
```

Then, create a new build of your supergraph:

```sh
ddn supergraph build local
```

### Step 4. Test your configuration

Generate a new JWT by logging into your application. These values aren't typically displayed to users, so you'll need to
log them while in development. You can then add that value as a header in the console and test any permissions you have
in your metadata.

## Wrapping up

In this guide, you learned how to integrate AWS Cognito with Hasura DDN to create a secure and scalable identity
management solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional AWS Cognito features that can enhance your authentication flows.



--- File: ../ddn-docs/docs/auth/jwt/tutorials/integrations/3-firebase.mdx ---
# Firebase

---
sidebar_position: 4
sidebar_label: Firebase
description: "Learn how to connect Firebase to your Hasura DDN supergraph."
keywords:
  - hasura
  - hasura ddn
  - authentication
  - jwt
  - firebase
  - tutorial
  - guide
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# Firebase

## Introduction

In this tutorial, you'll learn how to configure an existing [Firebase project](https://console.firebase.google.com/) and
generate a JWT which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- A [Firebase project](https://console.firebase.google.com/) created, with at least one authentication method enabled.
- A local application that can integrate with Firebase for authentication. We'll provide an example Node.js application
  below.
- A private key from the project's `Service Accounts` section via the
  [Firebase console](https://console.firebase.google.com/project/_/settings/serviceaccounts/adminsdk).
- A local Hasura DDN project.

:::

## Tutorial

### Step 1. Add Firebase to your local application

#### Step 1.1 Add the firebase-admin package

Firebase works with a number of [languages and frameworks](https://firebase.google.com/docs/admin/setup#add-sdk). In the
example(s) we show below, we'll use a Node.js application.

```sh
npm i firebase-admin
```

#### Step 1.2 Initialize firebase-admin

Then, initialize the module in your application:

```javascript
const admin = require("firebase-admin");

// service_account.json points to the private key from the prerequisites
admin.initializeApp({ credential: admin.credential.cert(require("./service_account.json")) });
```

#### Step 1.3 Add the custom claims

```javascript
// Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
// Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
// Below, we're hard-coding the value for now
const user_role = "user"; // the role returned from your request â˜ï¸
const customClaims = {
  "claims.jwt.hasura.io": {
    "x-hasura-default-role": user_role,
    "x-hasura-allowed-roles": ["user"],
    "x-hasura-user-id": decodedToken.uid,
  },
};

// Set custom claims for the user based on their uid
await admin.auth().setCustomUserClaims(decodedToken.uid, customClaims);
```

This will add the required Hasura namespace with the keys that Hasura DDN expects when decoding a JWT. You can modify
the keys to suit your Hasura DDN [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura DDN Engine. :::

### Step 2. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your
[Firebase JWKs](https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com) and audience:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      audience: ["your-firebase-project-name"]
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      key:
        jwkFromUrl: "https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com"
      tokenLocation:
        type: Header
        name: Auth-Token
```

:::info Firebase JWKs

Firebase uses the same set of JWKs for all applications. It distinguishes between different apps by specifying the
audience (`aud`) claim in the JWT. Make sure to set the audience field to match your Firebase project name in the
AuthConfig object to ensure proper validation.

:::

Then, create a new build of your supergraph:

```sh
ddn supergraph build local
```

### Step 3. Test your configuration

Generate a new JWT by logging into your application. These values aren't typically displayed to users, so you'll need to
log them while in development. You can then add that value as a header in the console and test any permissions you have
in your metadata.

<details>
  <summary>Here's the complete sample Node.js server.</summary>

```javascript
const express = require("express");
const admin = require("firebase-admin");
const bodyParser = require("body-parser");
const axios = require("axios");

// Initialize Firebase Admin SDK
admin.initializeApp({
  credential: admin.credential.cert(require("./service_account.json")),
});

const app = express();
app.use(bodyParser.json());

// Firebase API key from your Firebase project settings
const FIREBASE_API_KEY = "your API key found on the Firebase project's console";

// Route to handle user login with email and password
app.post("/login", async (req, res) => {
  const { email, password } = req.body;

  if (!email || !password) {
    return res.status(400).json({ message: "Email and password are required" });
  }

  try {
    // Call Firebase REST API to sign in the user with email and password
    const response = await axios.post(
      `https://identitytoolkit.googleapis.com/v1/accounts:signInWithPassword?key=${FIREBASE_API_KEY}`,
      {
        email,
        password,
        returnSecureToken: true,
      }
    );

    const { idToken } = response.data;

    // Verify the token using Firebase Admin SDK
    const decodedToken = await admin.auth().verifyIdToken(idToken);

    // Here, you'll need to fetch the user's role from Hasura DDN using an admin-level authenticated request
    // Learn more here: https://hasura.io/docs/3.0/auth/authentication/jwt/special-roles
    // Below, we're hard-coding the value for now
    const user_role = "user"; // the role returned from your request â˜ï¸
    const customClaims = {
      "claims.jwt.hasura.io": {
        "x-hasura-default-role": user_role,
        "x-hasura-allowed-roles": ["user"],
        "x-hasura-user-id": decodedToken.uid,
      },
    };

    // Set custom claims for the user based on their uid
    await admin.auth().setCustomUserClaims(decodedToken.uid, customClaims);

    // Send the updated JWT back in the response
    res.status(200).json({
      idToken,
    });
  } catch (error) {
    console.error("Error logging in:", error.response?.data || error.message);
    res.status(401).json({ message: "Invalid credentials", error: error.response?.data || error.message });
  }
});

app.listen(4000, () => {
  console.log("Server running on port 4000");
});
```

</details>

## Wrapping up

In this guide, you learned how to integrate Firebase with Hasura DDN to create a secure and scalable identity management
solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional Firebase features that can enhance your authentication flows.



--- File: ../ddn-docs/docs/auth/jwt/tutorials/integrations/4-clerk.mdx ---
# Clerk

---
sidebar_position: 5
sidebar_label: Clerk
description: "Learn how to connect Clerk to your Hasura DDN supergraph."
keywords:
  - hasura
  - hasura ddn
  - authentication
  - jwt
  - clerk
  - guide
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# Clerk

## Introduction

In this tutorial, you'll learn how to configure an existing [Clerk application](https://clerk.com/) and generate a JWT
which you can pass in the header of your requests to Hasura. After setting up your
[AuthConfig](/reference/metadata-reference/auth-config.mdx) object to use JWT mode, this will allow you to validate
users' identities and create [permission rules](/reference/metadata-reference/permissions.mdx) which can limit access to
underlying data served by Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- A [Clerk application](https://clerk.com/docs/quickstarts/setup-clerk).
- A local application that can integrate with Clerk for authentication.
- A local Hasura DDN project.

### Step 1. Create a JWT template

From your [Clerk application's dashboard](https://dashboard.clerk.com/), click `JWT templates` in the sidenav and create
a new blank template. You can name this whatever you wish along with configuring properties like the token's lifetime,
clock skew, etc.

In the claims editor, add the following:

```javascript
{
	"claims.jwt.hasura.io": {
		"x-hasura-user-id": "{{user.id}}",
		"x-hasura-default-role": "user",
		"x-hasura-allowed-roles": [
			"user"
		]
	}
}
```

This will add the required Hasura namespace with the keys that Hasura DDN expects when decoding a JWT. You can modify
the keys to suit your Hasura DDN [roles](/reference/metadata-reference/permissions.mdx#typepermissions).

You can also see that we're hard-coding the user's role. You can dynamically set a user's role using Clerk's metadata to
set a `role` field on the `User` object whenever a user registers. You can learn more
[here](https://clerk.com/docs/users/metadata).

This enables you to then pass the value of `{{user.publicMetadata.role}}` in the custom claims of the JWT.

:::tip Custom claims

You can create any custom keys you wish and reference them in your permissions using session variables. Above,
`x-hasura-user-id` is simply an example. Any claim prefixed with `x-hasura-` is accessible to the Hasura DDN Engine.

:::

### Step 2. Update your AuthConfig

Update your AuthConfig object to use JWT mode and your Clerk JWKs, which rely on your Clerk domain name found in the
sidenav under `Domains`:

```yaml
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: "/claims.jwt.hasura.io"
      key:
        jwkFromUrl: "https://<your-clerk-domain>/.well-known/jwks.json"
      tokenLocation:
        type: Header
        name: Auth-Token
```

Then, create a new build of your supergraph:

```sh
ddn supergraph build local
```

### Step 3. Test your configuration

Generate a new JWT by logging into your application. These values aren't typically displayed to users, so you'll need to
log them while in development. You can then add that value as a header in the console and test any permissions you have
in your metadata.

Clerk's [various SDKs](https://clerk.com/docs/references/overview) make this easy, as you can pass the name of a JWT
template and get back the encoded token.

```javascript title="As an example, using the JavaScript SDK:"
const jwt = await session.getToken({ template: "hasura" });
```

## Wrapping up

In this guide, you learned how to integrate Clerk with Hasura DDN to create a secure and scalable identity management
solution using JWTs. By leveraging custom claims in conjunction with
[permissions](/reference/metadata-reference/permissions.mdx), you can define precise access-control rules, ensuring that
your application remains secure and meets your users' needs.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

If you encounter issues or need further customization, consider reviewing our related documentation or exploring
additional Clerk features that can enhance your authentication flows.



--- File: ../ddn-docs/docs/auth/webhook/webhook-mode.mdx ---
# ../ddn-docs/docs/auth/webhook/webhook-mode.mdx

---
description:
  "Learn how to set up your Hasura DDN project with Webhook mode to authenticate requests and apply access-control
  rules."
keywords:
  - hasura webhook mode
  - hasura engine authentication
  - graphql request authentication
  - webhook authentication
  - webhook configuration
  - api authorization
  - hasura api requests
  - session variables
  - hasura user-role
sidebar_position: 2
---

import Thumbnail from "@site/src/components/Thumbnail";

# Webhook Mode

## Introduction

You can enable your Hasura DDN instance to use an auth webhook in just a few steps.

You will need to provide a URL that Hasura will call with the original request headers, and it should return a body with
the session variables after the request is authenticated.

<Thumbnail src="/img/auth/auth-webhook-overview-diagram.png" alt="Authentication using webhooks" width="1000px" />

## Session variable requirements

The only session variable required is `x-hasura-role` appearing in the response body.

In contrast to JWT mode, you do not have to pass `x-hasura-allowed-roles` or `x-hasura-default-role` session variables
and a `x-hasura-role` header will no be checked.

Session variable keys are case-insensitive. Values are case-sensitive.

## Enabling Webhook authentication

## Step 1. Update your AuthConfig {#update-authconfig}

Hasura utilizes an [AuthConfig](/reference/metadata-reference/auth-config.mdx) object that allows you to define the
configuration for your authentication service. In a standard setup the `auth-config.hml` file can be found in your
`globals` directory.

:::tip Hasura DDN VS Code extension

You can use [Hasura's VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) to
scaffold out your `AuthConfig` object by typing `AuthConfig` and selecting this object from the list of available
options. As you navigate through the skeleton, you can type `CTRL+SPACEBAR` at any point to reveal options for the
different key-value pairs.

:::

In the example below, we're demonstrating a sample authentication webhook.

```yaml title="globals/metadata/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    webhook:
      url:
        valueFromEnv: AUTH_WEBHOOK_URL
      method: POST
      customHeadersConfig:
        body:
          headers:
            forward:
              - authorization
              - content-type
        headers:
          additional:
            user-agent: "Hasura DDN"
```

```yaml title="Example .env file"
AUTH_WEBHOOK_URL=http://auth_hook:3050/validate-request
```

### GET vs POST

For a POST request, the headers received by the DDN engine on the query can be forwarded to the webhook in a JSON object
in the body of the request under a `headers` key.

For a GET request, the headers received by the DDN engine on the query can be forwarded to the webhook as actual headers
on the request and the body will be empty.

What we've provided above is a sample configuration as a POST request but it can be configured as a GET request. See the
reference for `AuthHookConfig` [here](/reference/metadata-reference/auth-config.mdx#authconfig-authhookconfigv3). For
the sample configuration above, the webhook will receive the `Authorization` and `Content-Type` headers in the body of
the request:

```json title="Example body of a POST request"
{
  "headers": {
    "Authorization": "Bearer some-token",
    "Content-Type": "application/json"
  }
}
```

Also, the webhook will receive the `user-agent` header as an actual header on the request.

:::tip Environment variables

You can use environment variables to dynamically and securely add the webhook URL to your AuthConfig. You can add these
values to your root-level `.env` and then map them in the `globals` subgraph.yaml file. Alternatively, you can include
raw strings here using `value` instead of `valueFromEnv`.

:::

## Step 2. Shaping the webhook request and response

### Request

Below is an example of the header object your webhook might receive in the body of a POST request:

```json title="Example header object"
{
  "headers": {
    "Authorization": "Bearer some-token",
    "Content-Type": "application/json"
  }
}
```

Headers are forwarded to the auth webhook from the client on each request received by the Hasura engine either as
headers for a GET request or as a JSON object in the body of a POST request under a `headers` key.

:::tip Custom Headers Configuration

You can configure the headers that are sent to the webhook in the optional `customHeadersConfig` field. If the field is
not provided, the default behavior is to forward all headers (after filtering out commonly used headers) received by the
DDN engine on the query to the webhook.

```yaml title="Ignored headers list"
Accept
Accept-Datetime
Accept-Encoding
Accept-Language
Cache-Control
Connection
Content-Length
Content-MD5
Content-Type
DNT
Host
Origin
Referer
User-Agent
```

When using `customHeadersConfig`, you can explicitly forward any headers, including those that are ignored by default.
This gives you full control over which headers are sent to the webhook.

It is recommended to use the `customHeadersConfig` field to configure only the headers that are required by the webhook
and not forward any other headers. This will help in reducing the size of the request and improve the performance of the
webhook.

:::

In this example, we're passing an encoded JWT in the `Authorization` header, however webhook mode is flexible and you
can pass any headers you wish.

:::tip Forward all headers

If you want to forward all headers received by the DDN engine on the query to the webhook, you can set `forward: "*"`.
This will forward all headers received by the DDN engine on the query to the webhook, excluding the ignored headers.

:::

### Token Parsing

In this example, the webhook is then responsible for validating and parsing the token passed in the header. It will need
to:

- **Extract the Token:** Retrieve the Authorization header from the incoming request and extract the token.

- **Validate the Token:** Use a library or your own logic to validate the token. This involves verifying the token's
  signature with the secret key.

- **Extract Claims:** Decode the token to extract the claims.

### Response

Based on the validation result, the webhook will need to respond with either a `200` status code (for a valid token) or
a `401` status code (for an invalid or missing token).

You should respond with session variables beginning with `X-Hasura-*` in an object in the **body** of your response. The
value of each session variable can be any JSON value. These will be available to your
[permissions](/reference/metadata-reference/permissions.mdx) in Hasura.

You will, at least, need to set the `X-Hasura-Role` session variable to let the Hasura DDN know which role to use for
this request. Unlike [JWT auth mode](auth/jwt/jwt-mode.mdx), you do not have to pass `X-Hasura-Allowed-Roles` or
`X-Hasura-Default-Role` session variables.

In the example below the `X-Hasura-Is-Owner` and `X-Hasura-Custom` are examples of custom session variables which can be
used to enforce permissions in your supergraph.

```json title="Example response from your webhook to Hasura DDN"
HTTP/1.1 200 OK
Content-Type: application/json

{
    "X-Hasura-Role": "user",
    "X-Hasura-User-Id": 25,
    "X-Hasura-Is-Owner": "true",
    "X-Hasura-Custom": "custom value"
}
```

## Step 3. Define permissions

Let's add some example `TypePermissions` so that an admin role can access all fields in the Orders type, but we restrict
a user role from accessing the `deliveryDate` field.

```bash title="Example TypePermissions for Orders type"
---
kind: TypePermissions
version: v1
definition:
  typeName: Orders
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - deliveryDate
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-start
    - role: user
      output:
        allowedFields:
          - createdAt
          - id
          - isReviewed
          - productId
          - status
          - updatedAt
          - userId
  # highlight-end
```

Let's also add some example `ModelPermissions` so that an admin role can access all rows in the Orders model, but a user
role can only access rows where the userId field matches the user id session variable in the JWT.

```bash title="Example ModelPermissions for Orders model"
---
kind: ModelPermissions
version: v1
definition:
  modelName: Orders
  permissions:
    - role: admin
      select:
        filter: null
        allowSubscriptions: true
# highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: userId
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
# highlight-end
```

## Step 4. Rebuild your supergraph

```bash title="For example, from the root of your project, run:"
ddn supergraph build local
```

## Step 5. Make an authenticated request

Here we're making a request to our Hasura DDN instance which will be validated by our webhook which returns a payload of
session variables.

```json title="Example response from our webhook to Hasura DDN"
{
  "x-hasura-user-id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
  "x-hasura-role": "user"
}
```

If we run a query for Orders, we can see that we only get the orders which this user has made and are not able to access
the deliveryDate field.

<Thumbnail
  src="/img/auth/console-auth-webhook-query-permissions-results.png"
  alt="Hasura console authentication using JWT"
/>

### Step 6. Set your API to public

Now that you have implemented webhook authentication, you can set your API to public. See here for more information on
[setting your API to public](/auth/private-vs-public.mdx).



--- File: ../ddn-docs/docs/auth/webhook/index.mdx ---
# Webhook

---
description:
  "Get comprehensive information on how to configure Hasura to use webhook mode to authenticate incoming requests.
  Understand the process and requirements, and how to control the Hasura Engine's response to user authentication."
keywords:
  - hasura webhook mode
  - hasura engine authentication
  - graphql request authentication
  - webhook authentication
  - webhook configuration
  - api authorization
  - hasura api requests
  - session variables
  - hasura user-role
sidebar_position: 4
sidebar_label: Webhook
---

# Authentication Using a Webhook

## Introduction

You can configure the Hasura DDN to use webhook mode in order to authenticate incoming requests.

This requires specifying a URL - which Hasura calls with the original request headers - that then returns a body
containing the session variables after authenticating the request.

## Webhook mode setup

- [How to set up webhook mode](/auth/webhook/webhook-mode.mdx)
- [Webhook admin and unauthenticated requests](/auth/webhook/tutorials/special-roles.mdx)



--- File: ../ddn-docs/docs/auth/webhook/tutorials/index.mdx ---
# Tutorials

---
sidebar_label: Tutorials
description: "Learn how to connect your preferred authentication provider to Hasura DDN using webhook mode."
keywords:
  - hasura authentication
  - webhook tutorials
  - connect provider
sidebar_position: 0
---

# Webhook Tutorials

## Introduction

This section shows you how to handle admin-level and unauthenticated requests when using webhook mode in Hasura DDN.

## Tutorials

- [Admin and unauthenticated requests](/auth/webhook/tutorials/special-roles.mdx)



--- File: ../ddn-docs/docs/auth/webhook/tutorials/special-roles.mdx ---
# Admin and Unauthenticated Requests

---
description: "Learn how to make admin-level and unauthenticated requests to a Hasura DDN instance with a webhook."
sidebar_label: Admin and Unauthenticated Requests
sidebar_position: 3
keywords:
  - hasura webhook mode
  - hasura engine authentication
  - graphql request authentication
  - webhook authentication
  - webhook configuration
  - api authorization
  - hasura api requests
  - session variables
  - hasura user-role
---

# Admin and Unauthenticated Requests

Hasura DDN projects enable an `admin` role by default which has access to all Models and Types in your supergraph.

## Making admin-level requests

To make an admin-level request, after [updating up your AuthConfig](/auth/webhook/webhook-mode.mdx#update-authconfig),
shape your webhook's response as follows:

```json title="Example response from your webhook to Hasura DDN"
HTTP/1.1 200 OK
Content-Type: application/json

{
    "X-Hasura-Role": "admin",
}
```

Any request that triggers this response from your webhook will be treated as an `admin` request.

:::info Your JWT claims should be unique for each role

When designing or implementing an auth server, it is crucial to generate JWTs with different claims for each user role
so that each token enables the appropriate data access permissions for that user.

:::

## Making unauthenticated requests

To make an unauthenticated request (i.e., one that is publicly accessible without any authentication), you'll need to do
a few things.

### Step 1. Create the claims

In your authentication server, you can provide a response that identifies the user's role as `public`. This can be any
role name you wish, so long as it's not a role (such as `admin`) that already exists.

```json title="Example response from your webhook to Hasura DDN"
HTTP/1.1 200 OK
Content-Type: application/json

{
    "X-Hasura-Role": "public",
}
```

### Step 2. Update ModelPermissions

For whatever [models](/reference/metadata-reference/models.mdx) you'd like to publicly expose, add a
[`ModelPermissions`](/reference/metadata-reference/permissions.mdx#modelpermissions-modelpermissions) rule for the
public role.

```yaml title="Example ModelPermission for an Events Model"
kind: ModelPermissions
version: v1
definition:
  modelName: Events
  permissions:
    - role: admin
      select:
        filter: null
    #highlight-start
    - role: public
      select:
        filter: null
    #highlight-end
```

### Step 3. Update TypePermissions

Then, determine which [types](/reference/metadata-reference/types.mdx) you'd like to publicly expose by updating
[TypePermissions](/reference/metadata-reference/permissions.mdx#typepermissions-typepermissions). Hasura DDN gives you
the ability to granularly determine which fields from each Model are available to each role.

```yaml title="Example TypePermissions for an Events Model"
kind: TypePermissions
version: v1
definition:
  typeName: Events
  permissions:
    - role: admin
      output:
        allowedFields:
          - id
          - owner_id
          - created_at
          - updated_at
          - is_live
          - title
          - date
          - description
    #highlight-start
    - role: public
      output:
        allowedFields:
          - id
          - is_live
          - title
          - date
          - description
    #highlight-end
```

### Step 4. Rebuild your supergraph

Once you've updated your metadata files, you can rebuild your supergraph and test it locally.

```bash title="For example, from the root of your project, run:"
ddn supergraph build local
```

### Step 5. Make an unauthenticated request

Now you can make an unauthenticated request to your API. The request response body will include
`{"x-hasura-role": "public"}` and as such the engine will limit access to the fields that are returned to the ones
specified in the `TypePermissions` for that role.



--- File: ../ddn-docs/docs/auth/permissions/index.mdx ---
# Permissions

---
description: "Detailed guide on configuring access control rules and permissions for data security."
keywords:
  - access control
  - data security
  - permissions
  - role-based access
  - API field access
  - model data access
  - command execution
sidebar_label: Permissions
---

# Permissions

Access control rules, authorization or "permissions" are essential for securing your data and ensuring that only
authorized users can access it.

Permissions allow you to declaratively define fine-grained rules in metadata which can determine exactly what data each
user can access.

The following types of permissions can be defined:

- To define what **data** within a model are allowed to be accessed by a role, configure the appropriate
  [`ModelPermissions`](/auth/permissions/model-permissions.mdx)
- To define which **fields** are accessible by a role in the API, configure the appropriate
  [`TypePermissions`](/auth/permissions/type-permissions.mdx)
- To define whether the command is **executable** by a role, configure the appropriate
  [`CommandPermissions`](/auth/permissions/command-permissions.mdx)



--- File: ../ddn-docs/docs/auth/permissions/model-permissions.mdx ---
# Model Permissions

---
description: "Guide on setting up model permissions in your supergraph"
keywords:
  - model permissions
  - supergraph
  - data access control
sidebar_position: 1
sidebar_label: Model Permissions
---

# Model Permissions

To limit what **data** in a model is available to a role in your supergraph, you define a `ModelPermissions` object with
a `filter` expression.

By default, whenever a new model is created in your supergraph, all records are only accessible to the `admin` role. You
can think of these as permissions on rows in a typical relational database table.

You can restrict access to certain data by adding a new item to the `permissions` array in the `ModelPermissions`
object. Each item in the array should have a `role` field and a `select` field. The `select` field should contain a
`filter` expression that determines which rows are accessible to the role when selecting from the model.

Most commonly, you'll use session variables â€” accessed by Hasura Engine via your configured
[authentication mechanism](/auth/overview.mdx) in a JWT or body of a webhook response â€” to restrict access to rows based
on the user's role, identity or other criteria.

This filter expression can reference

- The fields in your Model
- Logical operators: `and`, `or` and `not`
- `fieldIsNull` predicate
- `fieldComparison` predicate
- Relationship predicates
- `null`

To make a new `ModelPermission` or role available in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="Allow admin to access all rows in the Articles model but allow user to access rows where the author_id field matches the user id session variable. Basically, their own articles."
---
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  # highlight-start
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          fieldComparison:
            field: author_id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
  # highlight-end
```

## Reference

See the [ModelPermissions](/reference/metadata-reference/permissions.mdx) reference for more information.



--- File: ../ddn-docs/docs/auth/permissions/type-permissions.mdx ---
# Type Permissions

---
sidebar_position: 2
description: "Guide on defining Type Permissions for API fields in a supergraph."
keywords:
  - type permissions
  - supergraph roles
  - API field access
sidebar_label: Type Permissions
---

# Type Permissions

To make API **fields** available to a role in your supergraph, you define a `TypePermissions` object.

You can think of TypePermissions as being similar to column-level permissions in a relational database. Just as you can
restrict access to specific columns in a table based on the user's role, TypePermissions allow you to control access to
specific fields in a type within your supergraph.

By default, whenever a new type is created in your supergraph, each field is defined as being only accessible to the
`admin` role.

To add a new role, add a new item to the `permissions` array in the TypePermissions object.

Each item in the array should have a `role` field and an `output` field. The `output` field should contain an
`allowedFields` array, which lists the fields that are accessible to the role when the type is used in an output
context.

To make a new `TypePermission` object or role available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

## Example

```yaml title="Allow admin to access all fields in the article type, disallow user from accessing the author_id field."
---
kind: TypePermissions
version: v1
definition:
  typeName: article
  permissions:
    # highlight-start
    - role: admin
      output:
        allowedFields:
          - article_id
          - author_id
          - title
    - role: user
      output:
        allowedFields:
          - article_id
          - title
  # highlight-end
```

## Reference

See the [TypePermissions](/reference/metadata-reference/permissions.mdx) reference for more information.



--- File: ../ddn-docs/docs/auth/permissions/command-permissions.mdx ---
# Command Permissions

---
sidebar_position: 3
description:
  "Guide on configuring command permissions in your supergraph, including role-based access and argument presets."
keywords:
  - command permissions
  - role-based access
  - supergraph
  - argument presets
sidebar_label: Command Permissions
---

# Command Permissions

To limit what **commands** are available to a role in your supergraph, you define a `CommandPermissions` object.

By default, whenever a new command is created in your supergraph, it is only executable by the `admin` role.

You can enable or restrict access to commands by adding a new item to the `permissions` array in the
`CommandPermissions` object. Each item in the array should have a `role` field and an `allowExecution` field. The
`allowExecution` field should be set to `true` if the command is executable by the role.

You can also use argument presets to pass actual logical expressions to your data sources to control how they do things.

For example, a data connector might expose a `Command` called `delete_user_by_id` with two arguments - `user_id` and
`pre_check`. `user_id` is the primary key of the user you'd like to remove, and `pre_check` lets you provide a custom
boolean expression.

```yaml
kind: CommandPermissions
version: v1
definition:
  commandName: delete_user_by_id
  # highlight-start
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: pre_check
          value:
            booleanExpression:
              fieldComparison:
                field: is_invincible
                operator: _eq
                value:
                  literal: false
  # highlight-end
```

Now, when `admin` role runs this command, once again, they can do what they want, and provide their own `pre_check` if
they want.

The `user` role however, is able to pass a `user_id` argument, but the `pre_check` expression is passed to the data
connector which will only let them delete the row if the row's `is_invincible` value is set to `false`.

To make a execution of a command available to a role in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="Allow admin to execute the get_article_by_id command, restrict user to execute the get_article_by_id command with an id argument preset of 100."
---
kind: CommandPermissions
version: v1
definition:
  commandName: get_article_by_id
  # highlight-start
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: id
          value:
            literal: 100
  # highlight-end
```

## Reference

See the [CommandPermissions](/reference/metadata-reference/permissions.mdx) reference for more information.



--- File: ../ddn-docs/docs/auth/permissions/tutorials/index.mdx ---
# Authorization

---
sidebar_position: 1
sidebar_label: Authorization
description: "Learn how to create various access-control rules for your data sources with Hasura DDN."
keywords:
  - hasura
  - hasura ddn
  - authorization
  - access control
  - rules
seoFrontMatterUpdated: false
---

# Permissions Tutorials

## Introduction

In this section of tutorials, we'll provide step-by-step guides for common patterns in controlling users' access to data
via your supergraph.

If you're unfamiliar with how Hasura DDN handles authorization, [check out the docs](/auth/overview.mdx) before diving
deeper into one of these tutorials.

## Recipes

- [Limit data to users](/auth/permissions/tutorials/1-simple-user-permissions.mdx)
- [Public access](/auth/permissions/tutorials/2-public-access-role.mdx)
- [Service accounts](/auth/permissions/tutorials/4-service-account.mdx)
- [Role-based command execution](/auth/permissions/tutorials/5-restrict-command-execution-with-role-based-permissions.mdx)



--- File: ../ddn-docs/docs/auth/permissions/tutorials/1-simple-user-permissions.mdx ---
# Limit data to users

---
sidebar_position: 2
sidebar_label: Limit data to users
description: "Learn how to return only data belonging to a user."
keywords:
  - hasura
  - hasura ddn
  - authorization
  - user access
  - tutorial
  - guide
seoFrontMatterUpdated: false
---

# Limit Data to Users

## Introduction

In this tutorial, you'll learn how to configure [permissions](/reference/metadata-reference/permissions.mdx) to limit
users to accessing only their data.

This can be done by passing a value in the header of each request to your supergraph; Hasura will then use that value to
return only the data which matches conditions you specify.

:::info Prerequisites

Before continuing, ensure you have:

- A local Hasura DDN project.
- Either JWT or Webhook mode enabled in your [AuthConfig](/reference/metadata-reference/auth-config.mdx).

:::

## Tutorial

### Step 1. Create your ModelPermissions {#step-one}

To create a new role, such as `user`, simply add the role to the list of `permissions` for the model to which you wish
to limit access. Then, set up your access control rules.

In the example below, we'll allow users with the role of `user` to access only their own rows from a `Users` model by
checking for a header value matching their `id`:

```yaml title="For example, in a Users.hml"
---
kind: ModelPermissions
version: v1
definition:
  modelName: Users
  permissions:
    - role: admin
      select:
        filter: null
        #highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
        #highlight-end
```

You can modify this to meet your own data modeling by ensuring that the `field` is a column or type that can be compared
to the value of the session variable you send in the header of your request. In this example, we're using
`x-hasura-user-id`, but you can use any `x-hasura-` value you wish.

:::info Authentication tutorials

We have tutorials for popular authentication providers available [here](/auth/jwt/tutorials/integrations/index.mdx)!

:::

### Step 2. Create your TypePermissions

By adding ModelPermissions, we've made the model available to the new role. However, this role is not yet able to access
any of the fields from the model. We can do that by adding the new role to the list of `permissions` and including which
fields are accessible to it.

```yaml title="For example, in a Users.hml"
---
kind: TypePermissions
version: v1
definition:
  typeName: Users
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - email
          - favoriteArtist
          - id
          - isEmailVerified
          - lastSeen
          - name
          - password
          - updatedAt
    #highlight-start
    - role: user
      output:
        allowedFields:
          - createdAt
          - email
          - favoriteArtist
          - id
          - isEmailVerified
          - lastSeen
          - name
          - password
          - updatedAt
    #highlight-end
```

If you want to restrict which fields the new role can access, simply omit them from the list of `allowedFields`.

### Step 3. Test your permissions

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

Then, in a request, pass a header with the [session variable](#step-one) you identified earlier according to your
authentication configuration. You should see a schema limited to whatever ModelPermissions you defined for your new role
and â€” when executing a query â€” only see data meeting the filtering rule you included in the first step.

## Wrapping up

In this guide, you learned how to limit a user to see only their own data from a single type. However, you can use this
same tutorial and apply it to a variety of scenarios.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

## Learn more about permissions and auth

- [Permissions](/auth/permissions/index.mdx) with Hasura DDN
- [Auth](/auth/overview.mdx) with Hasura DDN

## Similar tutorials

- [Authorization tutorials](/auth/permissions/tutorials/index.mdx)



--- File: ../ddn-docs/docs/auth/permissions/tutorials/2-public-access-role.mdx ---
# Public access

---
sidebar_position: 3
sidebar_label: Public access
description: "Learn how to create a public-access role that can view data without authentication."
keywords:
  - hasura
  - hasura ddn
  - authorization
  - public
  - open
  - tutorial
  - guide
seoFrontMatterUpdated: false
---

# Public Access

## Introduction

In this tutorial, you'll learn how to configure [permissions](/reference/metadata-reference/permissions.mdx) to allow
for unauthenticated access to data in your supergraph. This can be done by creating a role and setting the `filter`
field to `null`.

:::warning A word of caution

Any requests made to your supergraph with the configuration demonstrated below will have unauthenticated access to
whatever resources you allow. Use with caution!

:::

:::info Prerequisites

Before continuing, ensure you have:

- A local Hasura DDN project.
- Either JWT or Webhook mode enabled in your [AuthConfig](/reference/metadata-reference/auth-config.mdx).

:::

## Tutorial

### Step 1. Create the claims

In your authentication server, you can provide a claims map that identifies the default role as `public`. This can be
any name you wish, so long as it's not a role (such as `admin`) that already exists.

```json title="E.g., a JWT claims configuration in an authentication service"

  "claims.jwt.hasura.io": {
    "x-hasura-default-role": "public",
    "x-hasura-allowed-roles": ["public"],
  }
```

### Step 2. Update ModelPermissions {#step-two}

For whatever [models](/reference/metadata-reference/models.mdx) you'd like to publicly expose, add a
[ModelPermissions](/reference/metadata-reference/permissions.mdx#modelpermissions-modelpermissions) rule for the public
role.

```yaml title="Example ModelPermission for an Events Model"
kind: ModelPermissions
version: v1
definition:
  modelName: Events
  permissions:
    - role: admin
      select:
        filter: null
    #highlight-start
    - role: public
      select:
        filter: null
    #highlight-end
```

### Step 3. Update TypePermissions

Then, determine which [types](/reference/metadata-reference/types.mdx) you'd like to publicly expose by updating
[TypePermissions](/reference/metadata-reference/permissions.mdx#typepermissions-typepermissions). Hasura DDN gives you
the ability to granularly determine which fields from each Model are available to each role.

```yaml title="Example TypePermissions for an Events Model"
kind: TypePermissions
version: v1
definition:
  typeName: Events
  permissions:
    - role: admin
      output:
        allowedFields:
          - id
          - owner_id
          - created_at
          - updated_at
          - is_live
          - title
          - date
          - description
    #highlight-start
    - role: public
      output:
        allowedFields:
          - id
          - is_live
          - title
          - date
          - description
    #highlight-end
```

### Step 4. Test your permissions

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

Then, in a request, pass a header with the [role](#step-two) you identified earlier according to your authentication
configuration. You should see a schema limited to whatever ModelPermissions you defined for your new role and â€” when
executing a query â€” only see data meeting the filtering rule you included in the first step.

## Wrapping up

In this guide, you learned how to expose data in your supergraph to users without any authentication. This is valuable
for any public-facing resources clients may need to access.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

## Learn more about permissions and auth

- [Permissions](/auth/permissions/index.mdx) with Hasura DDN
- [Auth](/auth/overview.mdx) with Hasura DDN

## Similar tutorials

- [Authorization tutorials](/auth/permissions/tutorials/index.mdx)



--- File: ../ddn-docs/docs/auth/permissions/tutorials/4-service-account.mdx ---
# Service accounts

---
sidebar_position: 4
sidebar_label: Service accounts
description: "Learn how to creata a public-access role that can view data without authentication."
keywords:
  - hasura
  - hasura ddn
  - authorization
  - service account
  - admin-secret
  - x-hasura-admin-secret
  - open
  - recipe
  - guide
seoFrontMatterUpdated: false
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Service Accounts

## Introduction

In this tutorial, you'll learn how to configure a JWT or webhook to allow for admin-level access to data in your
supergraph. This can be done by passing hard-coded session variables that match the `admin` role in Hasura DDN.

:::info Prerequisites

Before continuing, ensure you have:

- A local Hasura DDN project.
- Either JWT or Webhook mode enabled in your [AuthConfig](/reference/metadata-reference/auth-config.mdx).

:::

## Tutorial

### Step 1. Create a custom claim

<Tabs className="auth-tabs">
  <TabItem value="jwt" label="JWT">
To make an admin-level request, shape your claims as follows:

```json
  "https://hasura.io/jwt/claims": {
    "x-hasura-default-role": "admin",
    "x-hasura-allowed-roles": ["admin"],
  }
```

When the token is minted, it will include the hard-coded values and can be passed to act as an admin-level request to
your supergraph.

:::info Your JWT claims should be unique for each role

When designing or implementing an auth server, it is best practice to generate JWTs with different claims for each user
role so that each token enables the appropriate data access permissions for that user.

If you're unsure about setting up JWTs with Hasura, check out our
[tutorials](/auth/jwt/tutorials/integrations/index.mdx) for popular providers.

:::

  </TabItem>
  <TabItem value="webhook" label="Webhook">
To make an admin-level request, shape the response provided by your webhook as follows:

```json
HTTP/1.1 200 OK
Content-Type: application/json

{
    "X-Hasura-Role": "admin",
}
```

  </TabItem>
</Tabs>

### Step 2. Test your permissions

Create a new build of your supergraph:

```sh
ddn supergraph build local
```

Then, in a request, pass a header according to your authentication configuration. You should see all types and fields
available to the `admin` role.

## Wrapping up

In this guide, you learned how to expose all data in your supergraph to the `admin` role. While this is done by default,
you'll need to generate a JWT or include the session variables in your webhook response that will allow the request to
act as a service account.

As you continue building out your supergraph, keep in mind that authentication and authorization are crucial components.
Always validate your configuration and regularly test your setup to ensure it functions as expected across different
roles and environments.

## Learn more about permissions and auth

- [Permissions](/auth/permissions/index.mdx) with Hasura DDN
- [Auth](/auth/overview.mdx) with Hasura DDN

## Similar tutorials

- [Authorization tutorials](/auth/permissions/tutorials/index.mdx)



--- File: ../ddn-docs/docs/auth/permissions/tutorials/5-restrict-command-execution-with-role-based-permissions.mdx ---
# Restrict command execution with role-based permissions

---
sidebar_position: 5
sidebar_label: Restrict command execution with role-based permissions
description: "Learn how to restrict command execution with role-based permissions."
keywords:
  - hasura
  - hasura ddn
  - authorization
  - role-based execution
seoFrontMatterUpdated: false
---

# Restrict Command Execution with Role-based Permissions

## Introduction

Often, you'll want to limit a user's ability to execute certain [commands](/data-modeling/command.mdx) â€” which power
mutations in your GraphQL API â€” based on some related data. In the example below, we'll build on the
[tutorial found in our PostgreSQL getting-started section](/how-to-build-with-ddn/with-postgresql.mdx) and **restrict
users to only being able to update posts of which they're the author**.

:::info Prerequisites

Before continuing, ensure you have:

- A local Hasura DDN project.
- Either JWT or Webhook mode enabled in your [AuthConfig](/reference/metadata-reference/auth-config.mdx).

:::

## Tutorial

## Step 1. Add an `author` role to your CommandPermissions object

```yaml title="Locate your UpdatePostsById.hml file and update it to the following:" {9-23}
---
kind: CommandPermissions
version: v1
definition:
  commandName: UpdatePostsById
  permissions:
    - role: admin
      allowExecution: true
    - role: author
      allowExecution: true
      argumentPresets:
        - argument: preCheck
          value:
            booleanExpression:
              relationship:
                # Here, `user` refers to the pre-generated relationship's name
                name: user
                predicate:
                  fieldComparison:
                    field: id
                    operator: _eq
                    value:
                      sessionVariable: x-hasura-user-id
```

This role grants the `author` role permission to execute the `UpdatePostsById` command, but only if the user who
authored the post has an `id` that matches the `x-hasura-user-id` session variable in the request header. This ensures
users can only update posts they have authored.

## Step 2. Add TypePermissions to your response type

The new `author` role will need access to return types for the `UpdatePostsByIdResponse` type.

```yaml title="Find the UpdatePostsByIdResponse TypePermissions object and add the following:" {11-14}
kind: TypePermissions
version: v1
definition:
  typeName: UpdatePostsByIdResponse
  permissions:
    - role: admin
      output:
        allowedFields:
          - affectedRows
          - returning
    - role: author
      output:
        allowedFields:
          - affectedRows
```

In the configuration above, we're only allowing a user with the role of `author` to access the number of affected rows.
Alternatively, you could include `returning` in the `output` array and _then_ set ModelPermissions **and**
TypePermissions for the `author` role on the `Posts` type to allow for any or specific fields to be returned.

## Step 3. Create a new build and test {#build-and-test}

```yaml title="To test this, if you don't have JWT or Webhook mode enabled, we recommend replacing your AuthConfig with:"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: author
      sessionVariables: { "x-hasura-user-id": 1 }
```

This will set your `x-hasura-role` session variable as `author` and the `x-hasura-user-id` as `1`, enabling you to
impersonate Alice.

```bash title="Create a new build and start your services:"
ddn supergraph build local && ddn run docker-start
```

```graphql title="Then, run the following query:"
mutation UPDATE_POST_TITLE {
  updatePostsById(keyId: "1", updateColumns: { title: { set: "This is not Alice's first post" } }) {
    affectedRows
  }
}
```

```json title="As Alice is the owner of the post with the ID of 1, you should then see the following response:"
{
  "data": {
    "updatePostsById": {
      "affectedRows": 1
    }
  }
}
```

```graphql title="Alternatively, if we run the following â€” which is on a post Alice does not own â€” we'll see a different return value:"
mutation UPDATE_POST_TITLE {
  updatePostsById(keyId: "4", updateColumns: { title: { set: "Malicious Actions in the API" } }) {
    affectedRows
  }
}
```

```json title="Being 0, which is the number of rows affected:"
{
  "data": {
    "updatePostsById": {
      "affectedRows": 0
    }
  }
}
```

## Wrapping up

In this tutorial, we've demonstrated the minimum sets of permissions necessary to enforce role-based execution of
commands. While the example illustrates limiting users to updating their own posts, the principles can be applied to any
scenario by which you want to limit command execution â€” and mutations â€” based on relationships.

## Learn more about permissions and auth

- [Permissions](/auth/permissions/index.mdx) with Hasura DDN
- [Auth](/auth/overview.mdx) with Hasura DDN



--- File: ../ddn-docs/docs/plugins/overview.mdx ---
# Basics

---
title: Basics
sidebar_position: 1
description: "Learn about Hasura's powerful plugins architecture."
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
seoFrontMatterUpdated: false
---

# Engine Plugins

## Introduction

Hasura's engine plugins architecture allows you to integrate HTTP hooks at various stages of the API execution pipeline.
This functionality enables you to embed custom logic that reacts to specific events, effectively extending the core
capabilities of your API. These hooks can be triggered either before or after the execution of a query or mutation,
providing a powerful mechanism to implement user-defined workflows and event-driven customizations.

## Types of plugins

### Pre-Parse plugins

Pre-parse plugins are executed at the very beginning of the execution pipeline. They allow you to insert custom logic
**before** the query-parsing step occurs. This enables you to pre-process, or manipulate incoming queries, and modify
the behavior of your API at an early stage.

### Pre-Response plugins

Pre-response plugins are triggered at the final stage of the execution pipeline, **after the query has been executed but
before the response is sent to the client**. These plugins allow you to execute logic based on the response such as
calling third-party services before the client receives the response.

### Pre-Route plugins

Pre-route plugins are executed at the very beginning of request handling, **before predefined endpoints are processed**. They allow you to insert custom HTTP handlers into your API, enabling features like REST-style GraphQL endpoints, internal tools, or documentation interfaces such as a GraphQL schema visualizer or Swagger UI for a JSON API.

## Find out more

- [Learn more about how plugins work](/plugins/introduction.mdx)
- [Allowlist](/plugins/allowlist/index.mdx)
- [Caching](/plugins/caching/index.mdx)
- [RESTified Endpoints](/plugins/restified-endpoints/index.mdx)



--- File: ../ddn-docs/docs/project-configuration/overview.mdx ---
# Basics

---
sidebar_position: 1
sidebar_label: Basics
description: "Learn how metadata is configured in Hasura DDN with projects and subgraphs and how to create builds."
keywords:
  - hasura
  - hasura ddn
  - project configuration
  - hasura projects
  - hasura builds
  - hasura env vars
  - hasura subgraphs
---

# Projects

## Introduction

A **project** in Hasura DDN is the foundation for building and managing your API. It contains a structured collection of
metadata files that define the behavior, relationships, and permissions for your API. These metadata files are organized
into **subgraphs**, each representing a distinct data domain.

Projects are designed to support both local development and cloud deployment. During development, you work with a local
version of your project, which is linked to a cloud project through a
[context file](/project-configuration/project-management/manage-contexts.mdx). This linkage enables seamless
development, testing, and deployment workflows. You can also define multiple contexts (e.g., `staging`) to manage
different environments, with unique configurations for environment variables and cloud resources.

## Data domains

A **data domain** in Hasura DDN represents a distinct area of responsibility or focus within your project, typically
aligned with a specific team or business function. These domains are managed as **subgraphs**, which are collections of
metadata files that describe the relationships, permissions, and structure for the data within that domain.

Organizing your project into subgraphs provides several benefits:

- **Team Ownership**: Each team can focus on their own data domain without interfering with others, making collaboration
  simpler and reducing bottlenecks.
- **Clear Boundaries**: Subgraphs establish clear boundaries between domains, making it easier to define and enforce
  data access permissions and relationships.
- **Scalability**: By breaking your project into manageable pieces, you can scale your API incrementally as your
  organization and data requirements grow.
- **Flexibility**: Subgraphs can be independently updated and extended, allowing you to adapt your project to changing
  needs without disrupting the overall API.

This structure ensures that your project remains organized, collaborative, and adaptable, enabling you to build and
maintain robust APIs efficiently.

:::info Multi-repo projects

Hasura DDN also supports multi-repository setups, giving teams greater autonomy in their development process. With this
setup:

- Each team can maintain their **subgraph** in a separate repository while still contributing to the shared
  **supergraph**.
- Teams can develop, test, and deploy their subgraphs independently, enabling faster iteration and minimizing
  dependencies on other teams.
- The supergraph integrates these subgraphs into a single API, ensuring that the overall API remains consistent and
  cohesive.

This approach is ideal for large organizations where multiple teams work on distinct data domains but need to
collaborate through a unified API. For more information, check out
[this section](/project-configuration/subgraphs/index.mdx) of the docs.

:::

## Find out more

- [Tutorials](/project-configuration/tutorials/index.mdx)
- [Learn more about the supergraph concept](/project-configuration/supergraph.mdx)
- [Learn more about provisioning subgraphs](/project-configuration/subgraphs/index.mdx)
- [Learn how to manage a project across environments](/project-configuration/project-management/index.mdx)
- [Learn how to upgrade a legacy configuration](/project-configuration/upgrading-project-config/index.mdx)



--- File: ../ddn-docs/docs/plugins/introduction.mdx ---
# How plugins work

---
sidebar_position: 2
sidebar_label: How plugins work
description:
  "Explore the Engine Plugins. Understand the architecture of engine plugins and how they can be used to add new
  functionalities on top of DDN"
keywords:
  - hasura plugins
  - plugins
  - engine plugins
  - custom execution
seoFrontMatterUpdated: false
---

import Thumbnail from "@site/src/components/Thumbnail";

# How Plugins Work

## Introduction

Engine plugins are HTTP servers that run alongside a Hasura DDN instance and can be written in any language capable of
running an HTTP server.

They are configured in DDN using metadata. The engine sends HTTP requests to the plugin at the specified execution step,
the plugin processes the request, and then sends a response back to the engine, which continues execution based on the
plugin's response.

Plugins can be applied at the following steps:

| Execution Step   | Description                                                                                                                                          | Example Usage                                                                |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| **Pre-Parse**    | The first step in the execution pipeline, where custom logic can be applied before the query is parsed and its internal representation is generated. | Add an allowlist layer to restrict access to specific queries and mutations. |
| **Pre-Response** | The final step in the execution pipeline, where custom logic can be added after the query is executed but before the response is sent to the client. | Trigger Slack notifications after a mutation is executed.                    |
| **Pre-Route**    | The first step in the routing pipeline, where custom logic can be applied to the requests on other than pre-defined endpoints.                       | Add a custom endpoint to DDN.                                                |

## Architecture

```mermaid
---
title: Engine Plugins Execution Pipeline
---
graph RL
    Client[Client] -->|"Request (/graphql)"| Authentication
    Authentication --> Query_Parsing_and_Planning["Query parsing and planning"]
    Query_Parsing_and_Planning --> Execute_Query["Execute query"]
    Execute_Query <-->|Fetch Data| Data_Connector["Data connector"]
    Execute_Query --> Post_Processing["Post-processing"]
    Post_Processing --> Pre_Response_Hooks["Pre-response hooks"]
    Post_Processing -->|Response| Client

    subgraph DDN Engine
        Authentication
        Query_Parsing_and_Planning
        Execute_Query
        Post_Processing
    end

    Pre_Parse_Hooks["Pre-parse hooks"] <--> Query_Parsing_and_Planning
```

```mermaid
---
title: Pre-route plugin
---
graph RL
    Client[Client] -->|"Request (/*)"| Route_Handler["Route handler"]
    Route_Handler -->|Response| Client

    subgraph DDN Engine
        Route_Handler
    end

    Pre_Route_Hooks["Pre-route hooks"] <-->|Handle Request| Route_Handler["Route handler"]

```

## Plugin Configuration

Engine plugins are configured in DDN using metadata. The metadata specifies the URL of the engine plugin and the
execution step at which the plugin should be called. The configuration also can control the request that is sent to the
engine plugin.

```yaml title="Here is an example of a plugin configuration in DDN metadata:"
kind: LifecyclePluginHook
version: v1
definition:
  name: cloudflare allowlist
  url:
    valueFromEnv: ALLOW_LIST_URL
  pre: parse
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: "your-strong-m-auth-key"
      session: {}
      rawRequest:
        query: {}
        variables: {}
```

In this example, the plugin is configured to run at the `pre-parse` execution step. The plugin is called
`cloudflare allowlist`. The URL of the plugin is read from the `ALLOW_LIST_URL` environment variable. The plugin is
configured to add a `hasura-m-auth` header to the request with the value `your-strong-m-auth-key`.

Additionally, the request sent to the plugin includes the query and variables from the incoming GraphQL request, as well
as the session information from the incoming request.

## Pre-Parse Plugin

The `pre-parse` plugin is triggered at the first step in the execution pipeline, before the query is parsed. Use this
step to add custom logic before parsing begins.

For pre-parse plugin configuration
[click here](reference/metadata-reference/engine-plugins.mdx#lifecyclepluginhook-lifecyclepreparsepluginhook).

### Pre-Parse Plugin Request

```json title="A sample request that is sent to the pre-parse plugin is as follows:"
{
  "rawRequest": {
    "query": "query MyQuery { getAuthorById(author_id: 10) { first_name } }",
    "variables": {},
    "operationName": "MyQuery"
  },
  "session": {
    "role": "user",
    "variables": {
      "x-hasura-role": "user",
      "x-hasura-user-id": "123"
    }
  }
}
```

:::info Customize the request

The request sent to the plugin can be customized based on the plugin's
[configuration](reference/metadata-reference/engine-plugins.mdx#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest).

:::

### Pre-Parse Plugin Response

The `pre-parse` plugin has the ability to control the execution pipeline by returning a specific response to DDN. The
response determines whether execution continues, halts, or returns an error. The possible response types are:

| Response Type    | HTTP Status Code | Response Body | Description                                                                                    |
| ---------------- | ---------------- | ------------- | ---------------------------------------------------------------------------------------------- |
| `Continue`       | `204`            | None          | Execution continues without interruption.                                                      |
| `Response`       | `200`            | Response body | Execution halts, and the provided response is returned to the client.                          |
| `User Error`     | `400`            | Error object  | Execution halts, and the provided error is returned to the client as a user error.             |
| `Internal Error` | `500`            | Error object  | Execution halts, and the provided error is returned to the client as an internal server error. |

:::warning Response Validation

DDN does not validate the plugin's response. It is the plugin's responsibility to ensure the response is valid and
aligns with its intended logic.

:::

### Use Cases

The `pre-parse` plugin can be used to add a multitude of functionalities to DDN. Some use cases are:

- **Allowlist**: Add an allowlist layer to restrict access to specific queries and mutations based on the incoming
  request and session information.
- **Basic Rate Limiting**: Implement rate limiting to restrict the total number of requests that can be made to DDN in a
  given time period.
- **Custom Query Validation**: Add custom query validation logic to ensure that the incoming query is valid based on
  custom business logic.
- **Cache Get**: Implement a cache get layer to fetch the response from the cache before executing the query.

### Multiple Pre-Parse Plugins

You can configure multiple pre-parse plugins in DDN metadata. These plugins execute in the order they are listed in the
metadata configuration.

If a plugin returns a `Response`, `User Error`, or `Internal Error`, the execution stops immediately, and the response
is sent back to the client. Any plugins defined after that will not run.

Letâ€™s consider an example where the engine uses two pre-parse plugins: `Pre-parse Hook 1` and `Pre-parse Hook 2`.

```mermaid
sequenceDiagram
    participant Client
    participant Initial as "Initial Request"
    participant "Pre-parse Hook 1"
    participant "Pre-parse Hook 2"
    participant "Query Parsing and Planning"

    Client ->> Initial: Request to engine
    Initial ->> "Pre-parse Hook 1": Request
    "Pre-parse Hook 1" ->> Initial: Response
    Initial ->> "Pre-parse Hook 2": Request
    "Pre-parse Hook 2" ->> Initial: Response
    Initial ->> "Query Parsing and Planning": Continue
    "Query Parsing and Planning" ->> Client: Response
```

Hereâ€™s how the process works:

#### Case 1: Continue

If `Pre-parse Hook 1` returns a `Continue` response (HTTP status code 204), the engine proceeds to send the request to
`Pre-parse Hook 2`. This continues until all configured pre-parse plugins have been executed.

:::tip Continue response body

The `Continue` response body is ignored by DDN. Plugins returning a `Continue` response can safely leave the body empty.

:::

#### Case 2: Response, User Error, or Internal Error

If `Pre-parse Hook 1` returns any of the following:

- `Response` (HTTP status code 200)
- `User Error` (HTTP status code 400)
- `Internal Error` (HTTP status code 500)

The engine stops further execution and sends the response to the client and other plugins will not be called.

```mermaid
sequenceDiagram
    participant Client
    participant Initial as "Initial Request"
    participant PreParseHook1 as "Pre-parse Hook 1"
    participant PreParseHook2 as "Pre-parse Hook 2"
    participant QueryParsingAndPlanning as "Query Parsing and Planning"

    Client->>Initial: Send Request
    Initial->>PreParseHook1: Forward to Pre-parse Hook 1
    PreParseHook1-->>Initial: Response from Hook 1
    Initial-->>Client: Return Response (Query not executed)
```

:::info Will subsequent pre-response plugins execute?

Yes, even if a pre-parse plugin returns a `Response`, `User Error`, or `Internal Error`, the subsequent `pre-response`
plugins will still execute.

:::

If all pre-parse plugins return a `Continue` response (HTTP status code 204), the engine completes execution and sends
its generated response to the client.

:::info Can pre-parse plugins modify the request?

No, pre-parse plugins cannot alter the request itself. They only influence the execution pipeline by returning specific
responses.

:::

## Pre-Response Plugin

The `pre-response` plugin is triggered at the final step in the execution pipeline after the query is executed. Use this
step to add webhooks after the query is executed. Please note that the `pre-response` plugin cannot control or change
the execution pipeline.

For pre-response plugin configuration
[click here](reference/metadata-reference/engine-plugins.mdx#lifecyclepluginhook-lifecyclepreresponsepluginhook).

### Pre-Response Plugin Request

```json title="A sample request that is sent to the pre-response plugin is as follows:"
{
  "response": {
    "data": {
      "getAuthorById": {
        "first_name": "John"
      }
    }
  },
  "session": {
    "role": "user",
    "variables": {
      "x-hasura-role": "user",
      "x-hasura-user-id": "123"
    }
  },
  "rawRequest": {
    "query": "query MyQuery { getAuthorById(author_id: 10) { first_name } }",
    "variables": {},
    "operationName": "MyQuery"
  }
}
```

:::info Customize the request

The request sent to the plugin can be customized based on the plugin's
[configuration](reference/metadata-reference/engine-plugins.mdx#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest).

:::

### Pre-Response Plugin Response

The `pre-response` plugin cannot control the execution pipeline. The response from the plugin is ignored by DDN.

### Use Cases

The `pre-response` plugin can be used to add a number of functionalities to DDN. Some use cases also make use of the
pre-parse plugin. They are:

- **Slack Notifications**: Trigger Slack notifications after a query/mutation is executed.
- **Cache Set**: Implement a cache set layer to store the response in the cache.
- **Cache Invalidation**: Implement a cache invalidation layer to invalidate the cache based on the incoming request and
  session information.
- **Audit Logs**: Add audit logs to track the queries and mutations executed by the users.

### Multiple Pre-Response Plugins

Multiple `pre-response` plugins can be configured in DDN metadata.

The engine sends requests to all configured `pre-response` plugins in parallel. It does not wait for responses from the
plugins and immediately sends the response generated by the engine to the client.

## Pre-Route Plugin

The `pre-route` plugin is triggered at the first step in the routing stage, before the request is routed to the handler.
Use this step to add custom HTTP handlers to DDN. Please note that the pre-route plugin can only handle requests that do
not match DDN's pre-defined endpoints (`/graphql`, `/v1/sql`, `/v1/rest`, `/v1/explain`, `/healthz` and `/metrics`).

[See the reference here](reference/metadata-reference/engine-plugins.mdx#lifecyclepluginhook-lifecyclepreroutepluginhook)
for pre-route plugin metadata configuration.

### Pre-Route Plugin Request

A sample request that is sent to the `pre-route` plugin is as follows:

```json
{
  "path": "/v1/rest/users/5",
  "method": "POST",
  "query": "limit=10&offset=0"
  "body": {
    "name_like": "%foo%"
  }
}
```

:::info Customize the request

The request sent to the plugin can be customized based on the plugin's
[configuration](reference/metadata-reference/engine-plugins.mdx#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequest).

:::

### Pre-Route Plugin Response

The `pre-route` plugin have absolute control over the execution pipeline for the configured path. The plugin can return
any of the following responses to DDN:

| Response Type    | HTTP Status Code | Response Body | Description                                                 |
| ---------------- | ---------------- | ------------- | ----------------------------------------------------------- |
| `Success`        | `200`            | Response body | Return the response to the client.                          |
| `User Error`     | `400`            | Error object  | Stop the execution and return the error to the client.      |
| `Internal Error` | `500`            | Error object  | Stop the execution and return internal error to the client. |

### Use Cases

The `pre-route` plugin can be used to add custom endpoints to DDN which are not part of the pre-defined DDN endpoints.
Some use cases are:

- **RESTified Endpoints**: Turn GraphQL queries into REST endpoints.
- **Internal tools**: Add internal tools like graphql schema visualizers like graphql-voyager, Swagger UI for JSON API,
  etc.

### Multiple Pre-Route Plugins

Multiple `pre-route` plugins can be configured in DDN metadata to handle requests to different paths. However, if more
than one plugin matches the request path, the first plugin in the list is executed and the subsequent plugins are
ignored.

So, while defining multiple `pre-route` plugins, make sure that the more specific paths are defined first.

Let's take an example where the engine is configured with two `pre-route` plugins: `Pre-route hook 1` (for path
`/v1/api/users/admin`) and `Pre-route hook 2` (for path `/v1/api/users/*`). We want to handle the request to GET admin
user details (a more specific path) using `Pre-route hook 1` and the request to GET user with an ID (a less specific
path) using `Pre-route hook 2`. For this, we need to define the `Pre-route hook 1` first in the metadata.

```mermaid
sequenceDiagram
    participant Client
    participant RouteHandler as "Route Handler"
    participant PreRouteHook1 as "Pre-route Hook 1"
    participant PreRouteHook2 as "Pre-route Hook 2"

    Client->>RouteHandler: Send Request
    alt Path matches /v1/api/users/admin
        RouteHandler->>PreRouteHook1: Forward to Pre-route Hook 1
        PreRouteHook1-->>RouteHandler: Response from Hook 1
        RouteHandler-->>Client: Return Response
    else Path matches /v1/api/users/*
        RouteHandler->>PreRouteHook2: Forward to Pre-route Hook 2
        PreRouteHook2-->>RouteHandler: Response from Hook 2
        RouteHandler-->>Client: Return Response
    end
```

In this example, the engine is configured with two `pre-route` plugins. The engine sends the request to either
`Pre-route hook 1` or `Pre-route hook 2` based on the path. If the path matches `/v1/api/users/admin`, the engine sends
the request to `Pre-route hook 1`. If the path matches `/v1/api/users/*`, the engine sends the request to
`Pre-route hook 2`.



--- File: ../ddn-docs/docs/project-configuration/supergraph.mdx ---
# Supergraph

---
sidebar_position: 3
sidebar_label: Supergraph
description: "Learn what a supergraph is, what components make it up, and how it works."
keywords:
  - hasura
  - hasura ddn
  - project
  - supergraph
---

# Supergraph

## Introduction

A supergraph is essentially the API for your project. It provides a unified interface for interacting with your data and
serves as the backbone for building applications that rely on multiple data sources.

## Components

The supergraph is made up of several key components that work together to deliver a seamless API experience:

- **Subgraphs**: These represent different data domains and can include multiple data connectors to bring in data from
  various sources.
- **Builds**: A supergraph consists of immutable builds. At any given time, one build is applied, and it can be easily
  rolled back to a previous state if necessary.

## How it all works

The supergraph is defined using Hasura Metadata Language, which the engine uses to generate and serve the API. This
metadata acts as the blueprint for everything, including:

- Defining roles and permissions to control access.
- Configuring authentication mechanisms.
- Defining relationships across and between data sources.
- Specifying the GraphQL fields and types exposed by the API.

By centralizing these configurations, the supergraph ensures consistency and simplifies collaboration across teams.

## Next steps

- [Learn more about subgraphs](/project-configuration/subgraphs/index.mdx)



--- File: ../ddn-docs/docs/project-configuration/tutorials/index.mdx ---
# Tutorials

---
sidebar_position: 1
sidebar_label: Tutorials
description: "Learn how to configure and manage your projects with step-by-step tutorials."
keywords:
  - hasura
  - hasura ddn
  - projects
  - tutorials
---

# Tutorials

## Introduction

This section provides tutorials to guide you through essential project configuration tasks. You'll learn how to manage
multiple environments, structure your project around subgraphs, and distribute your project across separate repositories
to enable independent ownership and development.

## Available tutorials

- [Manage multiple environments](/project-configuration/tutorials/manage-multiple-environments.mdx)
- [Work with multiple subgraphs](/project-configuration/tutorials/work-with-multiple-subgraphs.mdx)
- [Work with multiple subgraphs across separate repositories](/project-configuration/tutorials/work-with-multiple-repositories.mdx)



--- File: ../ddn-docs/docs/project-configuration/tutorials/manage-multiple-environments.mdx ---
# Manage mulitple environments

---
sidebar_position: 2
sidebar_label: Manage mulitple environments
description: "Learn how to leverage contexts to manage multiple environments from a single source of metadata."
keywords:
  - hasura
  - hasura ddn
  - environments
  - context
  - tutorial
---

# Manage Multiple Environments

## Introduction

Managing multiple contexts in Hasura DDN allows you to replicate the functionality of traditional environments like
`staging` and `production`, but with greater flexibility. Instead of rigidly-defined environments, Hasura DDN uses
contexts to store key details such as project configurations, metadata, and environment variables. This approach lets
you switch between different setups without disrupting your workflow or end users.

In this tutorial, you'll learn how to:

- Set up multiple contexts to test and collaborate on your project
- Use the CLI to manage your project's contexts
- Build and deploy projects with shared metadata across different contexts

This tutorial should take less than twenty minutes.

## Setup

### Step 1. Initialize a new local project

```sh title="Create a new local project:"
ddn supergraph init environments-example && cd environments-example
```

This will scaffold out the necessary files for a Hasura DDN project in a new `environments-example` directory.

### Step 2. Add a data source and seed data

In this tutorial, we'll use the PostgreSQL connector and our sample PostgreSQL database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```sh title="In your project directory, run the following, choose hasura/postrges, and pass the connection URI above when prompted:"
ddn connector init my_pg -i
```

### Step 3. Generate the Hasura metadata

```sh title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect my_pg
```

After running this, you should see a representation of your database's schema in the
`app/connector/my_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add my_pg users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 4. Create a new local build and test the API

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```sh title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query {
  users {
    id
    name
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "users": [
      {
        "id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
        "name": "Sean"
      },
      {
        "id": "82001336-65b7-11ed-b905-7fa26a16d198",
        "name": "Rob"
      },
      {
        "id": "86d5fba0-65b7-11ed-b906-afb985970e2e",
        "name": "Marion"
      },
      {
        "id": "8dea1160-65b7-11ed-b907-e3c5123cb650",
        "name": "Sandeep"
      },
      {
        "id": "9bd9d300-65b7-11ed-b908-571fef22d2ba",
        "name": "Abby"
      }
    ]
  }
}
```

## Create a new context

### Step 5. Create a new project context and switch to it

```sh title="From the project directory, run:"
ddn context create-context staging
```

Verify this by opening the `.hasura/context.yaml` file. You'll see a `contexts` array with two entries: `default` and
your newly-created `staging`.

:::tip What can be stored in context?

Contexts store a series of key-value pairs that make it easy to switch between different setups. You can learn more
[here](/project-configuration/project-management/manage-contexts.mdx).

:::

### Step 6. Update the context values

```sh title="First, set the supergraph configuration:"
ddn context set supergraph "supergraph.yaml"
```

```sh title="Then, define which subgraphs to include:"
ddn context set subgraph "app/subgraph.yaml"
```

```sh title="Finally, stub out a local .env.staging file and add it to your staging context:"
touch .env.staging && ddn context set localEnvFile ".env.staging"
```

You'll see each of these key-value pairs added to your `staging` context.

:::tip Customize these values

The examples provided here are starting points and can be adapted to suit your project's requirements. Ideally, the
values included in your `.env.staging` file will reference resources such as testing database instances. Keep the keys
consistent with those in your production `.env` files, but assign different values tailored for staging or testing
environments.

:::

### Step 7. Create a new staging cloud project

```sh title="Using your new context, create a cloud project:"
ddn project init --env-file-name ".env.staging.cloud"
```

The CLI will add the project's name and your `cloudEnvFile` to your `staging` context.

### Step 8. Create a new build on your staging project

```sh
ddn supergraph build create
```

The CLI will output information about the build, including a console URL which you can open in your browser. Your local
metadata was used to create this API build on your `staging` project.

### Step 9. Create a new production cloud project

```sh title="First, switch contexts:"
ddn context set-current-context default
```

```sh title="Then, create a new project:"
ddn project init
```

In your `context.yaml`, you'll now see a `project` and `cloudEnvFile` value for your `default` context. We're
considering this our `production` instance.

### Step 10. Create a new build on your production project

```sh title="Since our current context is default, we can now use the same metadata to create a productioun build:"
ddn supergraph build create
```

Just as with our `staging` project, you can navigate to the console URL output by the CLI and explore your `production`
build, which should be identical to your `staging` build.

## Next steps

Now that you know how contexts can help you manage environments, see how easy it is to
[set up CI/CD](/deployment/hasura-ddn/ci-cd.mdx) using the CLI and contexts.



--- File: ../ddn-docs/docs/project-configuration/tutorials/work-with-multiple-subgraphs.mdx ---
# Work with multiple subgraphs

---
sidebar_position: 3
sidebar_label: Work with multiple subgraphs
description:
  "Learn how to organize your supergraph into multiple subgraphs, each of which can be owned by separate teams."
keywords:
  - hasura
  - hasura ddn
  - multiple subgraphs
  - tutorial
---

# Work with Multiple Subgraphs

## Introduction

Learn how to manage multiple subgraphs in Hasura DDN to streamline team ownership, enforce clear data boundaries, and
scale your API easily. This tutorial will show you how to structure your project for flexibility and collaboration,
ensuring efficient and adaptable API development.

In this tutorial, you'll learn how to:

- How to organize your project into subgraphs
- How to create a subgraph as part of your local project
- How to create relationships across subgraphs
- How to create a subgraph on a cloud project

We'll demonstrate this by creating a supergraph with two subgraphs: one owned by a `customers` team and another owned by
the `billing` team.

This tutorial should take less than twenty minutes.

## Setup

### Step 1. Initialize a new local project

```sh title="Create the project directory and move into it:"
ddn supergraph init subgraph-example && cd subgraph-example
```

### Step 2. Add a data source and seed data

In this tutorial, we'll use the PostgreSQL connector and our sample PostgreSQL database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```sh title="In your project directory, run the following, choose hasura/postrges, and pass the connection URI above when prompted:"
ddn connector init customers_pg -i
```

### Step 3. Generate the Hasura metadata

```sh title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect customers_pg
```

After running this, you should see a representation of your database's schema in the
`app/connector/customers_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add customers_pg users
```

Open the `app/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this Hasura
Metadata Language file to represent the `users` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 4. Create a new local build and test the API

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```sh title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query GET_USERS {
  users {
    id
    name
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "users": [
      {
        "id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
        "name": "Sean"
      },
      {
        "id": "82001336-65b7-11ed-b905-7fa26a16d198",
        "name": "Rob"
      },
      {
        "id": "86d5fba0-65b7-11ed-b906-afb985970e2e",
        "name": "Marion"
      },
      {
        "id": "8dea1160-65b7-11ed-b907-e3c5123cb650",
        "name": "Sandeep"
      },
      {
        "id": "9bd9d300-65b7-11ed-b908-571fef22d2ba",
        "name": "Abby"
      }
    ]
  }
}
```

## Add a subgraph

### Step 5. Create a new `billing` subgraph

```sh title="Use the DDN CLI to create the subgraph in your local metadata:"
ddn subgraph init billing --graphql-type-name-prefix billing
```

You'll see a new `billing` directory added to your local project. The CLI pre-configured this with all the necessary
files and subdirectories to contain data connectors and their metadata.

Additionally, by adding the `--graphql-type-name-prefix` flag, we're ensuring any types generated by the CLI will not
conflict with existing types from our other subgraph. If we had concerns about conflicts of root-level GraphQL fields,
we could also add the flag `--graphql-root-field-prefix`.

```sh title="Then, add it to your supergraph.yaml config:"
ddn subgraph add billing --subgraph ./billing/subgraph.yaml --target-supergraph ./supergraph.yaml
```

```yaml title="You can verify this by opening the supergraph.yaml in the root of your project. You should see the following:"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    - app/subgraph.yaml
    # highlight-start
    - billing/subgraph.yaml
    # highlight-end
```

### Step 6. Switch contexts {#switch-contexts}

```sh title="Switch contexts to the billing subgraph:"
ddn context set subgraph billing/subgraph.yaml
```

This will simplify our subsequent CLI commands as the CLI will now know that â€” whenever the `--subgraph` flag is
required â€” we're referencing the `billing` subgraph.

### Step 7. Add a data source and seed data

As before, we'll use the PostgreSQL connector with our sample database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```sh title="In your project directory, run:"
ddn connector init billing_pg -i
```

:::info Notice where this connector was added

Since you created the `billing` subgraph and switched contexts in [Step 6](#switch-contexts), the CLI added the
connector in the `billing` subgraph directory.

:::

### Step 8. Generate the Hasura metadata

```sh title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect billing_pg
```

After running this, you should see a representation of the database's schema in the
`billing/connector/billing_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add billing_pg orders
```

Open the `billing/metadata` directory and you'll find a newly-generated file: `PaymentInformation.hml`. The DDN CLI will
use this Hasura Metadata Language file to represent the `payment_information` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

### Step 9. Create a new local build and test the API

```sh title="To create a local build, run:"
ddn supergraph build local
```

```sh title="Kill your local services from their terminal tab:"
CTRL+C
```

```sh title="Restart your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

You should see both root-level fields for `users` and `orders` in your GraphQL API. Additionally, if you navigate to the
`Explorer` tab in the left-hand navigation, you should see your supergraph's data domains organized into two separate
subgraphs.

### Step 10. Create a relationship across subgraphs

As our models are in different subgraphs, we'll need to explicitly define a
[relationship](/reference/metadata-reference/relationships.mdx) between these using Hasura metadata.

```yaml title="Add the following to your Users.hml file:"
---
kind: Relationship
version: v1
definition:
  name: orders
  sourceType: Users
  target:
    model:
      #highlight-start
      subgraph: billing
      #highlight-end
      name: Orders
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

By calling out the `billing` subgraph in the relationship object, the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can help you with
validating your configuration.

```sh title="Then, create a new build:"
ddn supergraph build local
```

```sh title="And kill your services with CTRL+C before restarting them:"
ddn run docker-start
```

```graphql title="You can now query across subgraphs:"
query GET_USERS_AND_ORDERS {
  users {
    id
    name
    orders {
      id
      createdAt
      status
    }
  }
}
```

```json title="And get a response like:"
{
  "data": {
    "users": [
      {
        "id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
        "name": "Sean",
        "orders": [
          {
            "id": "7ff13435-b590-4d6b-957f-f7fd39d4528a",
            "createdAt": "2023-10-29T17:02:50.958076+00:00",
            "status": "complete"
          }
        ]
      },
      {
        "id": "82001336-65b7-11ed-b905-7fa26a16d198",
        "name": "Rob",
        "orders": [
          {
            "id": "9891596a-a732-4c1c-902c-1a112da48fec",
            "createdAt": "2023-10-29T17:02:51.150084+00:00",
            "status": "complete"
          }
        ]
      },
      {
        "id": "86d5fba0-65b7-11ed-b906-afb985970e2e",
        "name": "Marion",
        "orders": [
          {
            "id": "85581445-752a-4aef-9684-b648eb5d5f42",
            "createdAt": "2023-10-29T17:02:51.085229+00:00",
            "status": "complete"
          }
        ]
      },
      {
        "id": "8dea1160-65b7-11ed-b907-e3c5123cb650",
        "name": "Sandeep",
        "orders": [
          {
            "id": "c7406b75-6b24-41e4-9c5b-ff3feada9447",
            "createdAt": "2023-10-29T17:02:50.889261+00:00",
            "status": "processing"
          }
        ]
      },
      {
        "id": "9bd9d300-65b7-11ed-b908-571fef22d2ba",
        "name": "Abby",
        "orders": [
          {
            "id": "98612470-1feb-4b91-88f7-9289d652ee87",
            "createdAt": "2023-10-29T17:02:51.021317+00:00",
            "status": "complete"
          }
        ]
      }
    ]
  }
}
```

### Step 11. Create a cloud project

```sh title="Using your default context, create a new cloud project:"
ddn project init
```

Your `.hasura/context.yaml` will be updated to reflect the new project name. The `default` context of this project is
now linked to the cloud project you just created.

Additionally, the CLI will output information about each subgraph that it generated in your cloud project, including
`billing`.

### Step 12. Create a new build on your production project

```sh title="Finally, create a new build on your cloud project:"
ddn supergraph build create
```

The CLI will return a console URL which you can navigate to; within the `Explorer` tab, you'll find your project's
subgraphs, including `billing`.

:::info Adding subgraphs after project initialization

When you **initialize a cloud project**, the subgraphs in your local metadata will automatically be generated in your
cloud project. **If you add subgraphs after initialization, you'll have to manually add the subgraphs to the cloud
project as well.** Learn more [here](project-configuration/subgraphs/create-a-subgraph.mdx#cloud).

:::

## Next steps

In the example above, you learned the steps to organize your project into multiple subgraphs for clearer ownership
between teams. To take this a step further, many teams prefer to implement multi-repository setups wherein a single
subgraph can be added to an existing or private team repository. Learn more
[here](/project-configuration/tutorials/work-with-multiple-repositories.mdx).



--- File: ../ddn-docs/docs/project-configuration/tutorials/work-with-multiple-repositories.mdx ---
# Work with multiple repositories

---
sidebar_position: 4
sidebar_label: Work with multiple repositories
description: "Learn how to split your Hasura DDN across multiple repositories."
keywords:
  - hasura
  - hasura ddn
  - multiple subgraphs
  - multiple repositories
  - multi-repo
  - tutorial
toc_max_heading_level: 4
---

# Work with Multiple Repositories

## Introduction

Managing multiple repositories in Hasura DDN lets you distribute development across independent subgraphs located in
separate repositories while maintaining a unified supergraph. This approach provides flexibility for teams to work
autonomously in their respective data domains, while contributing to a single coordinated API.

By organizing your project into multiple repositories, you can iterate and deploy changes efficiently without impacting
other subgraphs, ensuring a smooth and collaborative development experience.

You'll learn:

- How to organize your project into subgraphs
- How to provision a "parent" repo
- How to set up your cloud project for multi-subgraph development
- How to create independent subgraph repositories
- How to create, test, and deploy your supergraph as subgraphs develop independently
- How to create relationships across independent subgraphs

This tutorial takes about thirty minutes.

:::warning DDN Advanced Plan required

In order to utilize multi-repository collaboration, you must have an active
[DDN Advanced Plan](https://hasura.io/pricing).

:::

## Create the initial project

Begin by creating a "parent project" that will serve as the coordinated supergraph for your independent subgraph
repositories. The person creating this will assume the role of **supergraph admin** and have full control over
provisioning subgraphs, inviting collaborators, and managing the API as a whole.

### Step 1. Initialize a new local project

```sh title="Create a new local project and initialize a git repository:"
ddn supergraph init parent-project && cd parent-project && git init
```

This will scaffold out the local configuration for a DDN project and initialize a git repository.

### Step 2. Create a cloud project

```sh title="From the local project directory, create a new cloud project:"
ddn project init
```

In `.hasura/context.yaml`, you'll see a new `project` key-value pair with the name of the project returned from the CLI.

### Step 3. Create a commit

```sh title="Create an initial commit with your local project mapped to the cloud project via context:"
git add . && git commit -m "Initial commit"
```

### Step 4. Provision subgraphs

We'll add two subgraphs to this supergraph: `customers` and `billing`.

```sh title="Create the subgraphs on the cloud project:"
ddn project subgraph create customers && ddn project subgraph create billing
```

### Step 5. Create a supergraph build

```sh title="Create an initial supergraph build:"
ddn supergraph build create
```

This will serve as the foundation for your first **subgraph** build to expand upon.

### Step 6. Invite collaborators

Head to the project's console at [console.hasura.io](https://console.hasura.io) and navigate to
`Setetings/Collaborators`. Then, invite collaborators based on their role:

| Role               | Description                                                                                                    |
| ------------------ | -------------------------------------------------------------------------------------------------------------- |
| Subgraph Admin     | Users with permissions to create builds and deploy them to the parent project's endpoint.                      |
| Subgraph Developer | Developers responsible for implementing features and making iterative changes to the subgraph's functionality. |

:::info Learn more about collaborators

Learn more about collaborators and roles [here](/project-configuration/project-management/manage-collaborators.mdx).

:::

Each collaborator will receive an email inviting them to your project.

## Add independent subgraphs to the project

Each subgraph will live in its own repository. While this can be an existing repository, we're going to demonstrate
initializing a new repository below.

### Customers

#### Step 1. Create a new repo for the `customers` subgraph

```sh title="Initialize a new local project and git repo:"
ddn supergraph init customer-team --create-subgraph customers && cd customer-team && git init
```

This will scaffold out the necessary project structure along with initializing a git repository for version control.

#### Step 2. Map the local project to the existing cloud project

```sh title="Use the --with-project flag to map the local directory to the parent cloud project, replacing the name with your parent project's name:"
ddn project init --with-project <project-name>
```

You'll see the `project` key-value pair updated in your `.hasura/context.yaml` file.

#### Step 3. Set the subgraph context

```sh title="Change the context to your subgraph's configuration file:"
ddn context set subgraph ./customers/subgraph.yaml
```

This will set your unique subgraph in your `.hasura/context.yaml` file and make the CLI commands below more concise.

#### Step 4. Add prefixing

```sh title="Next, open the customers/subgraph.yaml file and add the following:"
kind: Subgraph
version: v2
definition:
  name: customers
  generator:
    rootPath: .
    namingConvention: graphql
    #highlight-start
    graphqlRootFieldPrefix: customers_
    graphqlTypeNamePrefix: customers_
    #highlight-end
  includePaths:
    - metadata
```

These will prevent collisions of GraphQL root fields and GraphQL types when your supergraph is built.

#### Step 5. Add a data source and generate your first local build

In this tutorial, we'll use the PostgreSQL connector and our sample PostgreSQL database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```sh title="In your project directory, run the following, choose hasura/postrges, and pass the connection URI above when prompted:"
ddn connector init customers_pg -i
```

##### Generate the Hasura metadata

```sh title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect customers_pg
```

After running this, you should see a representation of your database's schema in the
`customers/connector/customers_pg/configuration.json` file; you can view this using `cat` or open the file in your
editor.

```sh title="Now, track the table from your PostgreSQL database as a model in your DDN metadata:"
ddn models add customers_pg users
```

Open the `customers/metadata` directory and you'll find a newly-generated file: `Users.hml`. The DDN CLI will use this
Hasura Metadata Language file to represent the `users` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

##### Create a new local build and test the API

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```sh title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query GET_USERS {
  customers_users {
    id
    name
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "customers_users": [
      {
        "id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
        "name": "Sean"
      },
      {
        "id": "82001336-65b7-11ed-b905-7fa26a16d198",
        "name": "Rob"
      },
      {
        "id": "86d5fba0-65b7-11ed-b906-afb985970e2e",
        "name": "Marion"
      },
      {
        "id": "8dea1160-65b7-11ed-b907-e3c5123cb650",
        "name": "Sandeep"
      },
      {
        "id": "9bd9d300-65b7-11ed-b908-571fef22d2ba",
        "name": "Abby"
      }
    ]
  }
}
```

#### Step 6. Create a subgraph build

Now that we've tested our local API and are happy with its state, we can create a build of our independent subgraph on
our cloud project.

```sh title="Create a subgraph build on your DDN project:"
ddn subgraph build create
```

The CLI will return the subgraph's build version; you'll need this value in the next step.

#### Step 7. Create a supergraph build

```sh title="Begin by listing the available supergraph builds:"
ddn supergraph build get
```

Grab the most recent build's version and use it â€” along with the subgraph build version â€”Â in the following command.

```sh title="Finally, create a supergraph build incorporating your latest subgraph build:"
ddn supergraph build create --subgraph-version customers:<build-version> --base-supergraph-version <supergraph-build-id>
```

The CLI will return a build URL for the console so you can explore your build and test your subgraph changes with others
before applying the build to the supergraph. In order to apply the build, you must be a **subgraph admin** or higher:

```sh
ddn supergraph build apply <supergraph-build-version>
```

:::warning Make sure to stop all running services

As a multi-repository setup is intended to be utilized by multiple persons across teams, you're not likely to have
multiple instances of your Hasura DDN Engine and other services running at the same time on your machine. Ensure you've
killed all active services before proceeding to the next step for the `billing` subgraph.

:::

### Billing

The steps below will allow you to incorporate your independent `billing` subgraph into your deployed supergraph API.

#### Step 1. Create a new repo for the `billing` subgraph

```sh title="Initialize a new local project and git repo:"
ddn supergraph init billing-team --create-subgraph billing && cd billing-team && git init
```

This will scaffold out the necessary project structure along with initializing a git repository for version control.

#### Step 2. Map the local project to the existing cloud project

```sh title="Use the --with-project flag to map the local directory to the parent cloud project, replacing the name with your parent project's name:"
ddn project init --with-project <project-name>
```

You'll see the `project` key-value pair updated in your `.hasura/context.yaml` file.

#### Step 3. Set the subgraph context

```sh title="Change the context to your subgraph's configuration file:"
ddn context set subgraph ./billing/subgraph.yaml
```

This will set your unique subgraph in your `.hasura/context.yaml` file and make the CLI commands below more concise.

#### Step 4. Add prefixing

```sh title="Next, open the billing/subgraph.yaml file and add the following:"
kind: Subgraph
version: v2
definition:
  name: billing
  generator:
    rootPath: .
    namingConvention: graphql
    #highlight-start
    graphqlRootFieldPrefix: billing_
    graphqlTypeNamePrefix: billing_
    #highlight-end
  includePaths:
    - metadata
```

Like before, this will ensure the types an fields are namespaced to your subgraph.

#### Step 5. Add a data source and generate your first local build

As before, we'll use the PostgreSQL connector with our sample database:

```plaintext
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

```sh title="In your project directory, run:"
ddn connector init billing_pg -i
```

##### Generate the Hasura metadata

```sh title="Next, use the CLI to introspect the PostgreSQL database:"
ddn connector introspect billing_pg
```

After running this, you should see a representation of your database's schema in the
`billing/connector/billing_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Now, track the table from the PostgreSQL database as a model in your DDN metadata:"
ddn models add billing_pg orders
```

Open the `billing/metadata` directory and you'll find a newly-generated file: `Orders.hml`. The DDN CLI will use this
Hasura Metadata Language file to represent the `orders` table from PostgreSQL in your API as a
[model](/reference/metadata-reference/models.mdx).

##### Create a new local build and test the API

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```sh title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query GET_ORDERS {
  billing_orders {
    id
    createdAt
    status
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "billing_orders": [
      {
        "id": "c7406b75-6b24-41e4-9c5b-ff3feada9447",
        "createdAt": "2023-10-29T17:02:50.889261+00:00",
        "status": "processing"
      },
      {
        "id": "7ff13435-b590-4d6b-957f-f7fd39d4528a",
        "createdAt": "2023-10-29T17:02:50.958076+00:00",
        "status": "complete"
      },
      {
        "id": "98612470-1feb-4b91-88f7-9289d652ee87",
        "createdAt": "2023-10-29T17:02:51.021317+00:00",
        "status": "complete"
      },
      {
        "id": "85581445-752a-4aef-9684-b648eb5d5f42",
        "createdAt": "2023-10-29T17:02:51.085229+00:00",
        "status": "complete"
      },
      {
        "id": "9891596a-a732-4c1c-902c-1a112da48fec",
        "createdAt": "2023-10-29T17:02:51.150084+00:00",
        "status": "complete"
      }
    ]
  }
}
```

#### Step 6. Create a subgraph build

```sh title="Once your local metadata is in its desired state, create a subgraph build on your DDN project:"
ddn subgraph build create
```

The CLI will return the subgraph's build version; you'll need this value in the next step.

#### Step 7. Create a supergraph build

```sh title="Begin by listing the available supergraph builds:"
ddn supergraph build get
```

Grab the most-recent build's version and use it â€” along with the subgraph build version â€”Â in the following command.

```sh title="Finally, create a supergraph build incorporating your latest subgraph build:"
ddn supergraph build create --subgraph-version billing:<build-version> --base-supergraph-version <supergraph-build-id>
```

The CLI will return a build URL for the console so you can explore your build and test your subgraph changes with others
before applying the build to the supergraph. In order to apply the build, you must be a **subgraph admin** or higher:

```sh
ddn supergraph build apply <supergraph-build-version>
```

:::warning Make sure to stop all running services

As a multi-repository setup is intended to be utilized by multiple persons across teams, you're not likely to have
multiple instances of your Hasura DDN Engine and other services running at the same time on your machine. Ensure you've
killed all active services before proceeding to the next step.

:::

## Add a cross-subgraph relationship

**We can create relationships across subgraphs that are testable on a Hasura Cloud project**. As your locally-running
engine will only have access to your local subgraphs and their accompanying metadata, you'll need to define
relationships using your knowledge of other subgraphs and then test them using a supergraph build in your hosted
environment.

```yaml title="Add the following relationship object to your Users.hml file in your customer-team repo:"
---
kind: Relationship
version: v1
definition:
  name: orders
  sourceType: Users
  target:
    model:
      subgraph: billing
      name: Orders
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

```sh title="Create a new subgraph build:"
ddn subgraph build create
```

```sh title="Get the list of supergraph builds:"
ddn supergraph build get
```

```sh title="Then, create a supergraph build incorporating your latest subgraph build:"
ddn supergraph build create --subgraph-version customers:<build-version> --base-supergraph-version <supergraph-build-id>
```

```graphql title="Now, you can test your nested query across subgraphs:"
query GET_USERS_AND_ORDERS {
  customers_users {
    id
    name
    orders {
      id
      createdAt
      status
    }
  }
}
```

```json title="With a repsonse like this:"
{
  "data": {
    "customers_users": [
      {
        "id": "7cf0a66c-65b7-11ed-b904-fb49f034fbbb",
        "name": "Sean",
        "orders": [
          {
            "id": "7ff13435-b590-4d6b-957f-f7fd39d4528a",
            "createdAt": "2023-10-29T17:02:50.958076+00:00",
            "status": "complete"
          }
        ]
      },
      {
        "id": "82001336-65b7-11ed-b905-7fa26a16d198",
        "name": "Rob",
        "orders": [
          {
            "id": "9891596a-a732-4c1c-902c-1a112da48fec",
            "createdAt": "2023-10-29T17:02:51.150084+00:00",
            "status": "complete"
          }
        ]
      },
      {
        "id": "86d5fba0-65b7-11ed-b906-afb985970e2e",
        "name": "Marion",
        "orders": [
          {
            "id": "85581445-752a-4aef-9684-b648eb5d5f42",
            "createdAt": "2023-10-29T17:02:51.085229+00:00",
            "status": "complete"
          }
        ]
      },
      {
        "id": "8dea1160-65b7-11ed-b907-e3c5123cb650",
        "name": "Sandeep",
        "orders": [
          {
            "id": "c7406b75-6b24-41e4-9c5b-ff3feada9447",
            "createdAt": "2023-10-29T17:02:50.889261+00:00",
            "status": "processing"
          }
        ]
      },
      {
        "id": "9bd9d300-65b7-11ed-b908-571fef22d2ba",
        "name": "Abby",
        "orders": [
          {
            "id": "98612470-1feb-4b91-88f7-9289d652ee87",
            "createdAt": "2023-10-29T17:02:51.021317+00:00",
            "status": "complete"
          }
        ]
      }
    ]
  }
}
```

## Recap

By organizing your project into multiple repositories, you can create a flexible and collaborative workflow for subgraph
development in Hasura DDN. Starting with a parent project, you learned how to provision subgraphs, invite collaborators,
and manage builds to integrate subgraph changes into a unified supergraph. This structure ensures teams can work
independently while maintaining seamless integration and coordination across the entire API.



--- File: ../ddn-docs/docs/plugins/tutorials/index.mdx ---
# Tutorials

---
title: Tutorials
description: "Learn how to implement engine plugins using these step-by-step guides."
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
  - tutorial
---

# Tutorials

## Introduction

This section contains tutorials to get you more familiar with developing and deploying your own engine plugins. **Do
note: each engine plugin built by Hasura has its own dedicated guide found within its section in these plugins docs.**

## Available tutorials

- [Build a simple plugin using TypeScript and Express](plugins/tutorials/simple-engine-plugins-ts.mdx)



--- File: ../ddn-docs/docs/plugins/tutorials/simple-engine-plugins-ts.mdx ---
# Create a Simple Engine Plugin

---
title: Create a Simple Engine Plugin
description: "Learn how to implement engine plugins using this step-by-step guide."
sidebar_label: Simple engine plugins with TS and Express
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
  - tutorial
---

# Create a Simple Engine Plugin: TypeScript and Express

## Introduction

In this tutorial, weâ€™ll create a pair of simple engine plugins using TypeScript and Express:

- A **pre-parse plugin** to log incoming session information before the query is executed.
- A **pre-pesponse plugin** to log responses before they are sent to clients.

By the end of this guide, you'll understand how to configure, deploy, and test plugins that extend Hasura DDN's core
capabilities, unlocking new possibilities for your API workflows.

This tutorial should take less than thirty minutes.

## Setup

### Step 1. Create the project structure

```sh title="Begin by creating a new directory:"
mkdir plugin-tutorial && cd plugin-tutorial
```

### Step 2. Initialize your local Hasura DDN project

```sh title="Within this directory, initialize your local Hasura DDN project:"
ddn supergraph init ddn && cd ddn
```

This will create a `ddn` directory â€” which will house your local DDN metadata â€” with all the necessary files and
directories scaffolded out.

### Step 3. Initialize a data connector and seed the database

```sh title="In your project directory, run:"
ddn connector init my_pg -i
```

From the dropdown, start typing `PostgreSQL` and hit enter to advance through all the options.

The CLI will output something similar to this:

```plaintext
HINT To access the local Postgres database:
- Run: docker compose -f app/connector/my_pg/compose.postgres-adminer.yaml up -d
- Open Adminer in your browser at http://localhost:5143 and create tables
- To connect to the database using other clients use postgresql://user:password@local.hasura.dev:8105/dev
```

```sh title="Use the hint from the CLI output:"
docker compose -f app/connector/my_pg/compose.postgres-adminer.yaml up -d
```

Run `docker ps` to see on which port Adminer is running. Then, you can then navigate to the address below to access it:

```plaintext
http://localhost:<ADMINER_PORT>
```

```sql title="Next, via Adminer select SQL command from the left-hand nav, then enter the following:"
--- Create the customers table
CREATE TABLE customers (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  email TEXT UNIQUE NOT NULL
);

--- Create the orders table with a default value for order_date
CREATE TABLE orders (
  id SERIAL PRIMARY KEY,
  customer_id INT NOT NULL REFERENCES customers(id),
  order_date DATE NOT NULL DEFAULT CURRENT_DATE,
  total_amount DECIMAL(10, 2) NOT NULL
);

--- Insert some data into customers
INSERT INTO customers (name, email) VALUES ('Alice', 'alice@example.com');
INSERT INTO customers (name, email) VALUES ('Bob', 'bob@example.com');
INSERT INTO customers (name, email) VALUES ('Charlie', 'charlie@example.com');

--- Insert some data into orders (order_date will use the default value if not provided)
INSERT INTO orders (customer_id, total_amount) VALUES (1, 99.99);
INSERT INTO orders (customer_id, total_amount) VALUES (2, 49.50);
INSERT INTO orders (customer_id, total_amount) VALUES (3, 75.00);
```

You'll see the execution return `OK` for each table creation and row insert.

### Step 4. Generate the Hasura metadata

```sh title="Next, use the CLI to introspect your PostgreSQL database:"
ddn connector introspect my_pg
```

After running this, you should see a representation of your database's schema in the
`app/connector/my_pg/configuration.json` file; you can view this using `cat` or open the file in your editor.

```sh title="Now, track the entities from your PostgreSQL database in your DDN metadata:"
ddn models add my_pg "*" && ddn commands add my_pg "*" && ddn relationships add my_pg "*"
```

Open the `app/metadata` directory and you'll find newly-generated files representing your API. The DDN CLI will use
these Hasura Metadata Language files to represent the tables, their operations, and relationships from PostgreSQL in
your API.

### Step 5. Create and test a local build

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

```sh title="Start your local Hasura DDN Engine and PostgreSQL connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query GET_CUSTOMERS_AND_ORDERS {
  customers {
    id
    name
    email
    orders {
      id
      orderDate
      totalAmount
    }
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "customers": [
      {
        "id": 1,
        "name": "Alice",
        "email": "alice@example.com",
        "orders": [
          {
            "id": 1,
            "orderDate": "2025-01-14",
            "totalAmount": "99.99"
          }
        ]
      },
      {
        "id": 2,
        "name": "Bob",
        "email": "bob@example.com",
        "orders": [
          {
            "id": 2,
            "orderDate": "2025-01-14",
            "totalAmount": "49.50"
          }
        ]
      },
      {
        "id": 3,
        "name": "Charlie",
        "email": "charlie@example.com",
        "orders": [
          {
            "id": 3,
            "orderDate": "2025-01-14",
            "totalAmount": "75.00"
          }
        ]
      }
    ]
  }
}
```

With our DDN project set up with a data source, we can now create the plugins.

## Create the plugins

### Step 6. Create the project directory

```sh title="From the root of your plugin-example directory, initialize a new Node.js project as a sibling of the ddn directory:"
mkdir simple-plugin && cd simple-plugin && npm init -y
```

```sh title="Your top-level project structure should look like this:"
â”œâ”€â”€ ddn
â””â”€â”€ simple-plugin
```

### Step 7. Install dependencies

```sh title="Execute the following from your simple-plugin directory:"
npm install express && npm install --save-dev typescript @types/node @types/express ts-node-dev
```

### Step 8. Initialize your tsconfig

```json title="Create a tsconfig.json in the simple-plugin directory:"
{
  "compilerOptions": {
    "target": "ES6",
    "module": "commonjs",
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true
  }
}
```

### Step 9. Create the `index.ts` and `types.ts` files

```sh title="Create the index.ts file"
mkdir src && touch src/index.ts && touch src/types.ts
```

```ts title="Then, populate index.ts with this:"
import express, { Request, Response, NextFunction } from "express";
import preparse from "./routes/pre-parse";
import preresponse from "./routes/pre-response";

const app = express();
const PORT = process.env.PORT || 4000;

// Middleware to check for hasura-m-auth header
const checkHasuraMAuth = (req: Request, res: Response, next: NextFunction): void => {
  const authHeader = req.header("hasura-m-auth");
  if (authHeader === "super-secret-key") {
    next();
  } else {
    res.status(401).json({ error: "Unauthorized: Invalid or missing header" });
  }
};

// Middleware
app.use(express.json());
app.use(checkHasuraMAuth);

// Apply the middleware selectively for specific routes
app.use("/pre-parse", preparse);
app.use("/pre-response", preresponse);

app.listen(PORT, () => {
  console.log(`Server is running on http://localhost:${PORT}`);
});
```

Don't worry about the warnings; we'll take care of adding the routes in the next step.

```ts title="Add types.ts with the following:"
export type PreParseRequest = {
  session: {
    role: string;
    variables: Record<string, any>;
  };
  rawRequest: {
    query: string;
    variables: Record<string, any> | null;
    operationName: string | null;
  };
};

export type PreResponseRequest = {
  response: {
    data: unknown;
  };
  session: {
    role: string;
    variables: Record<string, any> | null;
  };
  rawRequest: {
    query: string;
    variables: Record<string, any> | null;
    operationName: string | null;
  };
};
```

These types will serve as generic type definitions for the requests our webhook can expect from Hasura DDN.

### Step 10. Create the routes

```sh title="Create a routes directory and files for each route handler:"
mkdir src/routes && touch src/routes/pre-parse.ts && touch src/routes/pre-response.ts
```

```ts title="Add the following to the pre-parse.ts file:"
import { Router, Request, Response } from "express";
import { PreParseRequest } from "../types";

const router = Router();

router.post("/", (req: Request<any, any, PreParseRequest>, res: Response): void => {
  console.log(
    `This is running before the request is parsed! The user making this request is of role ${req.body.session.role}.`
  );
  res.status(204).send();
});

export default router;
```

```ts title="And this to the pre-response.ts file:"
import { Router, Request, Response } from "express";
import { PreResponseRequest } from "../types";

const router = Router();

router.post("/", (req: Request<unknown, unknown, PreResponseRequest>, res: Response): void => {
  const { response } = req.body;
  console.log(
    `\n\nThis is running after the request is logged via the webhook before the response is sent to the client! \n\n
    Response Data: ${JSON.stringify(response.data, null, 2)}`
  );

  res.status(204).send();
});

export default router;
```

These two handlers will be responsible for processing and responding to requests to the webhook from Hasura DDN. You can
learn more about the contract between Hasura DDN and a plugin [here](/plugins/introduction.mdx).

### Step 11. Update the `package.json`

```json title="Update the package.json to include these scripts:"
{
  "name": "simple-plugin",
  "version": "1.0.0",
  "main": "index.js",
  // highlight-start
  "scripts": {
    "start": "node dist/index.js",
    "dev": "ts-node-dev --respawn --transpile-only src/index.ts",
    "build": "tsc"
  },
  // highlight-end
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "dependencies": {
    "express": "^4.21.2"
  },
  "devDependencies": {
    "@types/express": "^5.0.0",
    "@types/node": "^22.10.6",
    "ts-node-dev": "^2.0.0",
    "typescript": "^5.7.3"
  }
}
```

### Step 12. Run the server

```sh title="From the simple-plugin directory, run:"
npm run dev
```

### Step 13. Update the Hasura metadata

Open your `ddn/globals/metadata` directory and create a new file called `plugins-config.hml`.

```yaml title="Populate the file with the following:"
kind: LifecyclePluginHook
version: v1
definition:
  name: Pre-Parse Plugin
  url:
    valueFromEnv: PRE_PARSE_URL
  pre: parse
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: "super-secret-key"
      session: {}
      rawRequest:
        query: {}
        variables: {}

---
kind: LifecyclePluginHook
version: v1
definition:
  name: Pre-Response Plugin
  url:
    valueFromEnv: PRE_RESPONSE_URL
  pre: response
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: "super-secret-key"
      session: {}
      rawRequest:
        query: {}
        variables: {}
```

These two metadata objects serve as the configuration for our pre-parse and pre-response plugins respectively. They'll
tell the Hasura DDN Engine to utilize these URLs whenever a query hits the API. You can learn more about them
[here](/reference/metadata-reference/engine-plugins.mdx).

### Step 14. Add the environment variables

Since we've used the keys `PRE_PARSE_URL` and `PRE_RESPONSE_URL`, we'll need to map these values in our `.env` file in
the root of our project.

```yaml title="Add the following key-value pairs to your globals/metadata/subgraph.yaml file:"
kind: Subgraph
version: v2
definition:
  name: globals
  generator:
    rootPath: .
  includePaths:
    - metadata
  #highlight-start
  envMapping:
    PRE_PARSE_URL:
      fromEnv: PRE_PARSE_URL
    PRE_RESPONSE_URL:
      fromEnv: PRE_RESPONSE_URL
  #highlight-end
```

```plaintext title="And these key-value pairs to your ddn directory's .env"
PRE_PARSE_URL="http://local.hasura.dev:4000/pre-parse"
PRE_RESPONSE_URL="http://local.hasura.dev:4000/pre-response"
```

### Step 15. Create a new build and test

```sh title="Use the DDN CLI to create a new build:"
ddn supergraph build local
```

```sh title="Then, kill your local services using CTRL+C before starting them back up:"
ddn run docker-start
```

```graphql title="Finally, exeucte this query:"
query GET_CUSTOMERS_AND_ORDERS {
  customers {
    id
    name
    email
    orders {
      id
      orderDate
      totalAmount
    }
  }
}
```

```plaintext title="You'll see similar values logged to your server's terminal tab at the appropriate point in the query's execution:"
This is running before the request is parsed! The user making this request is of role admin.


This is running after the request is logged via the webhook before the response is sent to the client!


    Response Data: {
  "customers": [
    {
      "id": 1,
      "name": "Alice",
      "email": "alice@example.com",
      "orders": [
        {
          "id": 1,
          "orderDate": "2025-01-14",
          "totalAmount": "99.99"
        }
      ]
    },
    {
      "id": 2,
      "name": "Bob",
      "email": "bob@example.com",
      "orders": [
        {
          "id": 2,
          "orderDate": "2025-01-14",
          "totalAmount": "49.50"
        }
      ]
    },
    {
      "id": 3,
      "name": "Charlie",
      "email": "charlie@example.com",
      "orders": [
        {
          "id": 3,
          "orderDate": "2025-01-14",
          "totalAmount": "75.00"
        }
      ]
    }
  ]
}
```

## Next steps

While this works locally, it's just as easy to also deploy this anywhere an Express application can be hosted. You can
also use a separate context configuration for local vs. cloud development to test, collaborate, and deploy your DDN
application â€”Â along with your plugins â€” whenever you wish. Learn more [here](/project-configuration/overview.mdx).

Check out the other plugins we have available out-of-the-box in this directory, too!



--- File: ../ddn-docs/docs/plugins/allowlist/index.mdx ---
# Allowlist Plugin

---
title: Allowlist Plugin
sidebar_position: 1
description: "Learn more about the Allowlist Plugin for Hasura DDN."
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
---

# Allowlist Plugin

## Introduction

The [Allowlist Plugin](https://github.com/hasura/engine-plugin-allowlist) provides a mechanism to restrict access to
your supergraph by defining specific queries or mutations that are allowed. This adds an extra layer of security by
ensuring that only predefined operations can be executed.

The plugin integrates with Hasura DDN as a **pre-parse plugin** and can be deployed as an HTTP service using tools like
Cloudflare Workers or similar services.

Key benefits of the allowlist plugin include:

- **Enhanced security:** Restrict operations to a predefined set.
- **Flexibility:** Supports dynamic configuration through environment variables.
- **Integration:** Works as a pre-parse plugin, ensuring only allowed queries or mutations are processed.

## Next steps

To get started with configuring and deploying the Allowlist Plugin, refer to the [guide](/plugins/allowlist/how-to.mdx),
which walks you through the process of setting up, configuring, and deploying the plugin.



--- File: ../ddn-docs/docs/project-configuration/subgraphs/create-a-subgraph.mdx ---
# Create a subgraph

---
sidebar_position: 1
sidebar_label: Create a subgraph
description: "Learn how to create a new local or a new cloud subgraph."
keywords:
  - hasura
  - hasura ddn
  - subgraph
  - create subgraph
---

# How to Create a Subgraph

## Introduction

You can easily create subgraphs using the CLI and â€” for cloud projects â€” the Hasura DDN console. By default, the CLI
will generate an `app` subgraph when a new project is initialized.

## In your local metadata {#local}

```sh title="Within a local project directory, run the following:"
ddn subgraph init <subgraph-name>
```

You can verify this by identifying the new subdirectory in your project with the subgraph's name.

```sh title="Then, add the subgraph to your supergraph's config file:"
ddn subgraph add <subgraph-name> --subgraph ./<subgraph-name>/subgraph.yaml --target-supergraph ./supergraph.yaml
```

```sh title="This will add the new subgraph to your supergraph.yaml:"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    - app/subgraph.yaml
    # highlight-start
    - <subgraph-name>/subgraph.yaml
    # highlight-end
```

## On a cloud project {#cloud}

By default, when a cloud project is initialized, any subgraphs present in your supergraph configuration will be
initialized as well. However, to explicitly add a new subgraph to a cloud project â€” such as when iterating on an
existing project â€” follow these steps:

### Using the CLI

```sh title="The folowing will create a new subgraph on the current context's cloud project:"
ddn project subgraph create <subgraph-name>
```

### Using the console

Under `Settings` > `Subgraphs` click `+ Create New Subgraph`.

:::info Subgraph mapping

Be careful of mismatching subgraphs between your local metadata and a cloud project: if you create a subgraph locally
and add it to your supergraph configuration without also creating the subgraph on your cloud project, cloud builds will
fail.

To resolve this, ensure that when you create a local subgraph in your metadata, you also create the companion subgraph
on the cloud project.

:::

## Next steps

- [Check out an end-to-end tutorial for working with multiple subgraphs](/project-configuration/tutorials/work-with-multiple-subgraphs.mdx)
- [Learn how to work with multiple subgraphs in a project](/project-configuration/subgraphs/working-with-multiple-subgraphs.mdx)



--- File: ../ddn-docs/docs/project-configuration/subgraphs/index.mdx ---
# ../ddn-docs/docs/project-configuration/subgraphs/index.mdx

---
sidebar_position: 1
description: "Learn what a subgraph is, what components make it up, and how it works."
keywords:
  - hasura
  - hasura ddn
  - project
  - subgraph
---

# Subgraphs

## Introduction

A subgraph is a focused component of your overall supergraph, typically organized around specific data domains or
business functions.

Subgraphs are often team-specific, aligning with the responsibilities and expertise of individual teams. This structure
not only enables efficient development but also makes it easier to onboard new teams as your unified supergraph evolves.
By defining clear boundaries and responsibilities, subgraphs allow new teams to integrate seamlessly without impacting
existing functionality, fostering a scalable and collaborative development environment.

## Organization

Subgraphs provide flexibility and modularity in API development. A single project can include one or more subgraphs,
depending on its complexity and the distribution of responsibilities among teams. Subgraphs are designed to be iterated
on and developed independently, allowing teams to work at their own pace and with their own preferred tools and
languages without interfering with the broader system.

In setups with multiple repositories, subgraphs also enhance governance by limiting access. Individual developers or
teams are only granted permissions to their specific subgraph, ensuring they cannot modify or disrupt other parts of the
API. This separation not only protects the integrity of the overall system but also enables streamlined collaboration
across teams by maintaining a well-defined scope of access.

## Next steps

- [Learn how to create a subgraph](/project-configuration/subgraphs/create-a-subgraph.mdx)
- [Learn how to establish relationships across subgraphs to unify your data](/project-configuration/subgraphs/working-with-multiple-subgraphs.mdx)
- [Learn how to split subgraphs across repositories to enable decentralized development](/project-configuration/subgraphs/working-with-multiple-repositories.mdx)



--- File: ../ddn-docs/docs/plugins/allowlist/how-to.mdx ---
# How to Configure the Allowlist Plugin

---
title: How to Configure the Allowlist Plugin
sidebar_position: 2
description: "Learn how to configure the Allowlist Plugin for Hasura DDN."
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
  - how-to
  - guide
---

# How to Configure the Allowlist Plugin

## Introduction

The [allowlist plugin](https://github.com/hasura/engine-plugin-allowlist) adds an allowlist layer on top of your
supergraph to restrict access to only specific queries or mutations.

:::info We're using Cloudflare Wrangler

In this example, we're using Cloudflare Wrangler to deploy our plugin as a Cloudflare Worker. However, you can use any
other tool or service that hosts HTTPS services you wish. You can get started with Wrangler
[here](https://developers.cloudflare.com/workers/wrangler/install-and-update/).

:::

## Step 1. Create a new Worker project

Create a new Cloudflare Worker project using the `create-cloudflare` command with the
[`allowlist` plugin template](https://github.com/hasura/engine-plugin-allowlist):

```bash
npm create cloudflare@latest allowlist-plugin -- --template https://github.com/hasura/engine-plugin-allowlist
```

## Step 2. Install the dependencies

Navigate to the new directory and install the dependencies.

```bash
cd allowlist-plugin
npm install
```

Also, start the local development server.

```bash
npm start
```

## Step 3. Add the plugin configuration

We'll let the engine know about the plugin and to execute it as a pre-parse plugin by creating a new metadata file. In
your `global` subgraph's metadata directory, create a new file named `allow-list.hml` and add the following
configuration.

```yaml
kind: LifecyclePluginHook
version: v1
definition:
  name: cloudflare allowlist
  url:
    valueFromEnv: ALLOW_LIST_URL
  pre: parse
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            valueFromEnv: M_AUTH_KEY
      session: {}
      rawRequest:
        query: {}
        variables: {}
```

:::info Using environment variables

We've used `valueFromEnv` so that we can dynamically and securely add values from our environment variables. You can add
these values to your root-level `.env` and then map them in the `globals` subgraph.yaml file. Alternatively, you can
include raw strings here using `value` instead of `valueFromEnv` and passing the keys.

:::

Next, update the `subgraph.yaml` file to include the metadata file and the environment variables.

```yaml
kind: Subgraph
version: v2
definition:
  name: globals
  ...
  includePaths:
    ...
    - allowlist-plugin.hml
  envMapping:
    ALLOW_LIST_URL:
      fromEnv: ALLOW_LIST_URL
    M_AUTH_KEY:
      fromEnv: M_AUTH_KEY
```

Finally, we need to add the environment variables to the `.env` file.

```bash
ALLOW_LIST_URL="http://local.hasura.dev:8787"
M_AUTH_KEY="your-strong-m-auth-key"
```

:::tip M-Auth Key

The `hasura-m-auth` header is a custom header that is used to authenticate the requests to the allowlist plugin. You can
use any strong key here to authenticate the plugin. DDN will automatically add this header to the requests to the
plugin. Also, make sure to update the `src/config.ts` file (in step 5) with the same key.

:::

## Step 4. Create a new build for local development

Create a new supergraph build.

```bash
ddn supergraph build local
```

Start the console for the local supergraph.

```bash
ddn console --local
```

You can now test the plugin by running queries or mutations that are not in the allowlist. The plugin will restrict
access to only the queries or mutations you've defined.

## Step 5. Update the plugin config

Update the `src/config.ts` file with the queries and mutations that you want to allow, using a strong m-auth key.

```typescript
export const Config = {
  headers: {
    "hasura-m-auth": "your-strong-m-auth-key",
  },
  allowlist: [
    ...,
    "query MyQuery {\n  getAuthorById(author_id: 10) {\n    first_name\n    id\n    last_name\n  }\n}",
  ],
};
```

:::info Hot reloading

The local wrangler development server will automatically reload the plugin when you make changes to the code.

:::

## Step 6. Configure the plugin variables

:::info Setup tracing

To enable tracing for the plugin, you need to update the `wrangler.toml` file with the required configurations. If you
don't want to enable tracing for the plugin, you can skip this step.

:::

In `allowlist-plugin` directory, update the `wrangler.toml` file with the required configurations.

```toml
...
[vars]
OTEL_EXPORTER_OTLP_ENDPOINT = "https://gateway.otlp.hasura.io:443/v1/traces"
OTEL_EXPORTER_PAT = "<PAT>"
```

Replace `<PAT>` with the Personal Access Token (PAT) for the Hasura Cloud account. You can generate this using the
`ddn auth print-access-token` command.

## Step 7. Deploy the plugin

For your plugin to be reachable by your hosted supergraph, we'll need to deploy using Cloudflare Wrangler. The `deploy`
command included in your plugin's `package.json` will do this automatically for you and return the hosted service's URL.

**Note**: Please also update the `wrangler.toml` with your cloud PAT for the tracing to work.

```bash
npm run deploy
```

This will deploy the plugin to Cloudflare Workers and return the URL of the hosted service. Next, update the .env.cloud
file with the URL.

```bash
ALLOW_LIST_URL="https://<your-deployed-plugin>.workers.dev"
M_AUTH_KEY="your-strong-m-auth-key"
```

## Step 8. Create a new build

Create a new supergraph build.

```bash
ddn supergraph build create
```

The engine will execute the plugin before each request using the queries or mutations you defined.



--- File: ../ddn-docs/docs/project-configuration/subgraphs/working-with-multiple-subgraphs.mdx ---
# Work with multiple subgraphs

---
sidebar_position: 2
sidebar_label: Work with multiple subgraphs
description: "Learn how to use contexts to switch between subgraphs for easy CLI management."
keywords:
  - hasura
  - hasura ddn
  - subgraphs
  - multiple subgraphs
  - subgraph relationship
  - CLI
  - context
---

# How to Work with Multiple Subgraphs

## Introduction

In your daily development, you'll often encounter scenarios where your project requires multiple subgraphs to represent
distinct domains. Whether you're working in a single repository or planning to split subgraphs across multiple
repositories for team independence, understanding how to manage multiple subgraphs is crucial.

We'll walk you through the best practices for working with multiple subgraphs, setting their context, managing
namespacing, and establishing relationships across them to create a unified supergraph.

## Subgraph Namespacing

Each subgraph in your project operates in its own namespace, which means internal metadata objects (like models,
relationships, and permissions) cannot conflict with other subgraphs. However, the GraphQL API is where all subgraphs
come together, and conflicts can occur with root field and type names.

To prevent naming conflicts, you can:

1. Use subgraph prefixing (recommended for multi-team setups)
2. Manually ensure unique names across subgraphs
3. Use explicit type names in your schema

For more information about prefixing, see the
[subgraph prefixing guide](/project-configuration/subgraphs/subgraph-prefixing.mdx).

## Set subgraph context

If you're working in a project with multiple subgraphs, there will be the need to switch between subgraphs when
executing commands with the DDN CLI. While the CLI supports a `--subgraph` flag, it is much more convenient to set your
subgraph in context.

```sh title="Switch between context using the path to the subgraph's configuration file:"
ddn context set subgraph ./<subgraph-name>/subgraph.yaml
```

```sh title="You can verify this by checking the current context:"
ddn context get subgraph
```

Which should return the path you entered. This will ensure any commands you run â€” such as adding a new data connector â€”
are run in the appropriate subgraph.

## Relationships across subgraphs

Relationships across subgraphs work in nearly the exact same way as relationships within a single subgraph; the only
caveat is to add the `subgraph` name of the target type.

```yaml title="In the example below, we're creating a simple relationship between models, with the target model belonging to the billing subgraph:" {8}
kind: Relationship
version: v1
definition:
  name: orders
  sourceType: Users
  target:
    model:
      subgraph: billing
      name: Orders
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

### Single-repo relationships

In a single-repo setup, relationships are straightforward to manage. All subgraphs are in the same repository and the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can be used to assist
with authoring relationships, providing auto-complete and validation.

### Cross-repo relationships {#cross-repo-relationships}

:::info Advanced plan

You will need a project on the [DDN Advanced plan](https://hasura.io/pricing) to use multi-repo federation and
cross-repo relationships.

:::

In a [multi-repo setup](/project-configuration/subgraphs/working-with-multiple-repositories.mdx), while working with an
independent subgraph in its own repository, you may want to relate objects that reside in different repositories, some
of which you may not have access to.

In these cases the Hasura VS Code extension cannot validate the entirety of the `Relationship` object and you will
manually author cross-repo relationships and ensure that the field mappings are correct. However, upon creating a
supergraph build, all cross-subgraph metadata is validated to prevent mistakes from being deployed to the final API.

You can still easily use the Hasura DDN console to explore the supergraph and test relationships across subgraphs once
you have created a build.

If you perform a _local_ supergraph build using the CLI (ie. `ddn supergraph build local`), cross-repo relationships
will be ignored and will not be validated. If you run the build locally you will only see the subgraphs in that
repository, and any relationships to subgraphs from other repositories will be missing.

### Example

Let's say you have a supergraph with two subgraphs, each managed in different repositories: `users` and `products`.

The `users` subgraph in repo 'A' has a `User` type with a field called `user_favorite_product_id`.

The `products` subgraph in repo 'B' has a `Product` type with a field called `id`.

To create a relationship between these two types in different repositories, you would create a `Relationship` object in
the `users` subgraph metadata as normal.

The LSP is able to understand that the `Product` type is in a different subgraph to which it does not have access and
will not give a warning on the foreign type.

```yaml
kind: Relationship
version: v1
definition:
  name: favorite_product
  sourceType: User
  target:
    model:
      name: Product
      subgraph: products
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: user_favorite_product_id
      target:
        modelField:
          - fieldName: id
```

This `Relationship` object defines a relationship called `favorite_product` from the `User` type to the `Product` type.
The `mapping` field specifies how the `user_favorite_product_id` field in the `User` type maps to the `id` field in the
`Product` type.

After defining the cross-repo relationship, it's important to note that you won't be able to test this locally. To see
the relationship in action, you'll need to follow these steps:

1. Create a new supergraph build on DDN using the `ddn supergraph build create` command. (Subgraph builds do not get an
   API, so supergraph builds are required to test.)
2. You can then use the Hasura DDN console to explore and test the relationship across subgraphs.
3. If you have admin permissions, you can apply the subgraph to the supergraph with the `ddn subgraph apply` command.

Remember, cross-repo relationships only come into effect when the subgraphs are combined in the DDN environment. Local
development and testing are limited to the scope of your current repository.

With this relationship defined, you can now query the `favorite_product` field on the `User` type to retrieve the
related `Product`.

```graphql
query {
  users {
    id
    name
    favorite_product {
      id
      name
    }
  }
}
```

## Next steps

- [Learn how to split subgraphs across repositories](/project-configuration/subgraphs/working-with-multiple-repositories.mdx)

:::info Multi-Repository Development

For larger teams, you can split subgraphs across multiple repositories to enable independent development lifecycles.
This advanced feature requires the DDN Advanced plan. Learn more in our guide about
[splitting subgraphs across repositories](/project-configuration/subgraphs/working-with-multiple-repositories.mdx).

:::



--- File: ../ddn-docs/docs/project-configuration/subgraphs/working-with-multiple-repositories.mdx ---
# Split Subgraphs across repositories

---
sidebar_position: 3
sidebar_label: Split Subgraphs across repositories
description: "Learn how to split subgraphs across multiple repositories."
keywords:
  - hasura
  - hasura ddn
  - subgraphs
  - multiple subgraphs
  - parent repo
  - multiple repositories
  - mutli-repo
  - ddn advanced
  - advanced
  - CLI
  - context
---

import Roles from "@site/docs/project-configuration/_roles.mdx";
import Permissions from "@site/docs/project-configuration/_permissions.mdx";

# How to Split Subgraphs across Repositories

## Introduction

For larger teams and larger projects, it can make sense to split your subgraphs into multiple repositories contributing
to the supergraph in the "parent" project. This approach enables teams to have independent software development
lifecycles (SDLC) and CI/CD pipelines, providing better governance and control over their specific domains.

In this setup, one or more subgraphs are defined in each repository and depending on the role of the user on the cloud
project, can be built and applied to the supergraph independently. Each subgraph typically represents a distinct data
domain (e.g., users, orders, products) and can be managed by different teams.

:::tip Key Concepts

- Supergraph and subgraph builds are immutable and have unique IDs
- A subgraph must exist in the DDN project before inviting collaborators to it
- Each subgraph is namespaced and internal metadata objects cannot conflict with other subgraphs
- The GraphQL API is where subgraphs meet and conflicts can occur with root field and type names
- [Subgraph prefixing](/project-configuration/subgraphs/subgraph-prefixing.mdx) can automatically remedy naming
  conflicts

:::

Deployment with a multi-repository setup is effectively the same as
[deploying from a single repository](/deployment/hasura-ddn/index.mdx), but with the considerations of managing your
context to reference the shared project, and also being aware of the differences in abilities of the users building and
deploying.

See the tables [below](#invite-collaborators) for the roles and permissions of users on a Hasura DDN project.

:::warning DDN Advanced Plan required

In order to utilize multi-repository collaboration, you must have an active
[DDN Advanced Plan](https://hasura.io/pricing).

:::

## How it works

### Parent Project:

The project `Owner` or `Admin` will need to:

1. Initialize a new local parent project
2. Initialize the corresponding cloud parent project
3. Initialize git for the parent project
4. Provision subgraphs on the cloud parent project
5. Create a base build of the cloud parent project
6. Invite collaborators to the cloud parent project

### Independent Subgraphs:

The invited user will need to:

1. Create a new supergraph with a new subgraph and initialize a new repository
2. Initialize their cloud project as the parent project and link the new subgraph
3. Set the subgraph context
4. Create a subgraph build
5. Integrate the subgraph build into the parent project
6. Apply the build to be the official supergraph API (`Owner`, `Admin`, `Subgraph Admin` roles)

## Create the Initial Parent Project

Begin by creating a "parent" project that will serve as the coordinated supergraph for your independent subgraph
repositories. This central project will manage provisioning subgraphs, inviting collaborators, and maintaining the
overall API.

### Step 1. Initialize a new local project

This will serve as the parent project.

```sh
ddn supergraph init <parent-project> && cd <parent-project> && git init
```

This will scaffold the local configuration for your DDN project and initialize a Git repository.

### Step 2. Initialize the cloud project

This is based on the local parent project.

```sh
ddn project init
```

In your configuration file (e.g., `.hasura/context.yaml`), you'll see a new `project` entry with the name of the project
returned by the CLI.

### Step 3. Create an initial commit

```sh
git add . && git commit -m "Initial commit"
```

Push this repository to your preferred hosting service to share it with collaborators.

### Step 4. Provision subgraphs on the cloud parent project

If you know the subgraphs to include, you can provision them using the DDN CLI. Replace `<subgraph-name>` with the
desired name:

```sh
ddn project subgraph create <subgraph-name>
```

You are also able to add a subgraph on the console in the `Share > Invite a user > Granular access` section.

:::info On-Demand Subgraphs

Subgraphs can be added as needed when collaborators are onboarded.

:::

### Step 5. Create a base build of the cloud parent project

```sh
ddn supergraph build create
```

This initial build serves as the foundation for future subgraph builds.

### Step 6. Invite collaborators to the cloud parent project {#invite-collaborators}

Navigate to your project's console and invite collaborators based on their roles:

<Roles />

The following are the detailed permissions for the above roles:

<Permissions />

Each collaborator will receive an invitation to join the project and can proceed to add their subgraphs.

## Independent Subgraphs

Each independent subgraph contributing to the parent project resides in its own separate repository and has it's own
local supergraph. The repository could be an existing or newly initialized one.

### Step 1. Create a new supergraph with a new subgraph and initialize a repository

The invited user will need to create a new supergraph on their machine with a new subgraph and initialize a repository.

They will use the same name for the subgraph as was created in the parent project.

```sh
ddn supergraph init <subgraph-name-as-supergraph-name> --create-subgraph <subgraph-name> && cd <subgraph-name> && git init
```

This scaffolds the necessary structure and initializes a Git repository.

### Step 2. Initialize the cloud project as the parent project and link the new subgraph

This will initialize the cloud project as the parent project.

```sh
ddn project init --with-project <parent-project-name>
```

Your configuration file will now link the local subgraph to the parent project.

:::info Local Development

You can add data sources and develop locally at this point. Check out relevant tutorials for adding sources.

:::

### Step 3. Set the subgraph context

```sh
ddn context set subgraph ./<subgraph-name>/subgraph.yaml
```

This sets the subgraph as the default for future CLI commands.

### Step 4. Create a subgraph build

```sh
ddn subgraph build create
```

The CLI returns a build version for later integration into the parent supergraph.

### Step 5. Integrate the subgraph build into the parent project

```sh
ddn supergraph build get
```

Use the latest build version from the parent supergraph and the subgraph build version in the following command:

```sh
ddn supergraph build create --subgraph-version <subgraph-name:build-version> --base-supergraph-version <parent-build-id>
```

This integrates the subgraph changes into an existing build of the parent supergraph.

### Step 6. Apply the build

```sh
ddn supergraph build apply <supergraph-build-version>
```

This finalizes the integration, making the changes available to all collaborators. This command is only available to
`Owner` and `Admin` roles. A `Subgraph Admin` can run:

```sh title="Apply a single subgraph. Unavailable for Subgraph Developer role"
ddn subgraph build apply <subgraph-build-version>
```

## Build Summary

### Supergraph and all subgraphs

Even though all subgraphs are not in the same repo, the `ddn supergraph build create` command will build the supergraph
including all subgraphs which are on DDN cloud.

Once your [context is set to the shared project](/project-configuration/project-management/manage-environments.mdx), an
`Owner` or an `Admin` role can build and apply the supergraph including all subgraphs.

As always, `ddn supergraph build apply <supergraph-build-version>` will make it the active supergraph API.

`Subgraph Admin` and `Subgraph Developer` cannot build or apply supergraphs.

### Supergraph and specific subgraphs

An `Owner` or an `Admin` role can build and apply new supergraph and specify subgraphs to be built and applied.

You can also list the available builds of subgraphs to use with:

```bash
ddn subgraph build get
```

```bash title="Build a supergraph based on a specific build and with mutiple specific subgraphs"
ddn supergraph build create --subgraph-version <subgraph_name:subgraph_version> --subgraph-version <subgraph_name:subgraph_version>  --base-supergraph-version <supergraph_version>
```

See more about incremental builds [here](/deployment/hasura-ddn/incremental-builds.mdx).

### A single subgraph

An `Owner`, `Admin`, `Subgraph Admin` and `Subgraph Developer` role can build a single subgraph. All except
`Subgraph Developer` can apply a single subgraph.

```bash title="Build a single subgraph"
ddn subgraph build create --subgraph-version <subgraph_name:subgraph_version> --base-supergraph-version <supergraph_version>
```

```bash title="Apply a single subgraph. Unavailable for Subgraph Developer role"
ddn subgraph build apply --subgraph-version <subgraph_name:subgraph_version>
```

## Merging Existing Projects

If you have two independently developed projects on Hasura DDN that you want to merge into a single project with
independent subgraph development, follow these steps:

1. Choose which project will be the main project and which will be the subgraph project
2. In the main project, create a new subgraph placeholder:

```sh
ddn project subgraph create <subgraph-name>
```

3. Invite the subgraph project collaborators with appropriate permissions
4. Once collaborators accept the invitation, they should set their project context:

```sh
ddn context set project <main-project-name>
```

5. Set up subgraph prefixes if needed to prevent naming conflicts
6. Create a subgraph build on the main project:

```sh
ddn subgraph build create
```

7. The main project owner/admin can then apply the subgraph build:

```sh
ddn subgraph build apply <subgraph-build-version>
```

## Next steps

- [Follow an end-to-end tutorial](/project-configuration/tutorials/work-with-multiple-repositories.mdx)



--- File: ../ddn-docs/docs/project-configuration/subgraphs/subgraph-prefixing.mdx ---
# Subgraph Prefixing

---
sidebar_position: 5
sidebar_label: Subgraph Prefixing
description:
  "Learn how to avoid naming collisions between subgraphs in Hasura DDN by customizing prefixes for root fields and type
  names."
keywords:
  - subgraph prefixing
  - naming collisions
  - subgraph.yaml
  - graphqlRootFieldPrefix
  - graphqlTypeNamePrefix
  - supergraph modeling
  - hasura ddn
  - graphql api
  - subgraphs
  - supergraph
  - data connectors
  - permissions
  - relationships
seoFrontMatterUpdated: true
---

# Subgraph GraphQL Root Field and Type Name Prefixing

Subgraphs are namespaced and metadata object names are independent from one another and cannot conflict. However, the
GraphQL API is where the subgraphs meet, potentially leading to naming collisions.

To avoid collisions between GraphQL root fields and type names across when federating subgraphs, you can optionally
customize the prefixes for each.

For example, if two subgraphs both have a `Users` type, you can apply different prefixes to distinguish one from the
other. This ensures that each subgraph remains unique and prevents any naming conflicts.

You can make these modifications in the `subgraph.yaml` file for a subgraph.

```yaml title="Add the highlighted lines:"
kind: Subgraph
version: v2
definition:
  name: my_subgraph
  generator:
    rootPath: .
    #highlight-start
    graphqlRootFieldPrefix: my_subgraph_
    graphqlTypeNamePrefix: My_subgraph_
    #highlight-end
```

By default, the `subgraph.yaml` file is generated without any prefixes. You can read more about these fields
[here](reference/metadata-reference/build-configs.mdx#subgraph-subgraphgeneratorconfig).

## Renaming GraphQL root fields and GraphQL type name prefixes

This codemod will rename prefixes in already generated metadata. It can also be used to add or remove prefixes
altogether.

The `--from-graphql-root-field` prefix will be stripped if provided, and the new prefix, `--graphql-root-field`, will be
added. If the new prefix is already present, it will not be reapplied.

Examples:

```bash
# Add root field and type name prefixes to the subgraph set in the context
ddn codemod rename-graphql-prefixes --graphql-root-field 'app_' --graphql-type-name 'App_'

# Change the root field prefix for the specified subgraph
ddn codemod rename-graphql-prefixes --subgraph app/subgraph.yaml --from-graphql-root-field 'app_' --graphql-root-field 'new_'
```



--- File: ../ddn-docs/docs/plugins/caching/index.mdx ---
# Caching Plugin

---
title: Caching Plugin
sidebar_position: 1
description: "Learn more about the Caching Plugin for Hasura DDN."
keywords:
  - hasura plugins
  - caching plugin
  - plugins architecture
  - engine plugins
---

# Caching Plugin

## Introduction

The [Caching Plugin](https://github.com/hasura/engine-plugin-caching) allows you to cache responses for specific GraphQL
queries, enhancing performance by reducing repeated computation for frequently executed queries.

The plugin integrates with Hasura DDN as both a **pre-parse plugin** and **pre-response plugin**. It uses Redis as the
caching backend, ensuring fast and reliable storage for cached query results.

Key benefits of the caching plugin include:

- **Improved performance:** Reduces load on your supergraph by caching frequently executed queries.
- **Customizable caching:** Define specific queries to cache and configure cache lifetimes.
- **Integration-ready:** Compatible with Hasura's lifecycle plugin hooks and OpenTelemetry for tracing.

## Next steps

To configure and deploy the Caching Plugin, refer to the [guide](/plugins/caching/how-to.mdx), which provides detailed
steps for setting up the plugin in a local or production environment.



--- File: ../ddn-docs/docs/project-configuration/project-management/index.mdx ---
# ../ddn-docs/docs/project-configuration/project-management/index.mdx

---
sidebar_position: 1
description: "Learn how to manage your project using context and collaborators."
keywords:
  - hasura
  - hasura ddn
  - project management
---

# Project Management

## Introduction

Broadly, projects can be managed using two tools: context and collaborators.

**Context** allows you to swap out local and cloud configuration files and values. This makes it easier to execute
commands via the CLI, switch between environments when testing your API, and executing automated CI/CD scripts.

**Collaborators** can be added to any [Hasura DDN Base or Hasura DDN Advanced project](https://hasura.io/pricing).
Depending on the project's plan, you can add collaborators with read-only access all the way to granular access,
enabling them to only contribute to certain [subgraphs](/project-configuration/subgraphs/index.mdx).

## Next steps

- [Learn how to manage context](/project-configuration/project-management/manage-contexts.mdx)
- [Learn how to invite collaborators](/project-configuration/project-management/manage-collaborators.mdx)



--- File: ../ddn-docs/docs/project-configuration/project-management/manage-contexts.mdx ---
# Manage project contexts

---
sidebar_position: 1
sidebar_label: Manage project contexts
description: "Learn how to manage contexts in a Hasura DDN project."
keywords:
  - hasura
  - hasura ddn
  - context
  - manage context
---

# How to Manage Project Contexts

## Introduction

Contexts simplify your development workflow by making CLI commands more concise and ensuring easy transitions between
environments. They act as predefined configurations that the CLI uses to interact with specific sets of values, making
processes like deploying, testing, or managing your project in CI/CD pipelines easier.

You can manage your project's contexts using the `.hasura/context.yaml` file, which is generated in the root of your
project when the CLI initializes it.

## Contexts

By default, the CLI sets up your project with a `default` context. To add new contexts, use the
[`create-context` CLI command](/reference/cli/commands/ddn_context_create-context.mdx).

For a full list of context-related commands, refer to
[this category of commands](/reference/cli/commands/ddn_context.mdx). Although you can modify these values directly in
the file, we recommend using the CLI for consistency and ease. Examples are provided below.

### Project

The `project` key-value pair can be mapped to a Hasura DDN cloud project. This is used to dictate which cloud-hosted
project should be referenced by the CLI when cloud-based and project-related commands are executed.

```yaml title="When the default context is set, the great-ddn-1234 project will be used by the CLI:"
contexts:
  default:
    #highlight-start
    project: great-ddn-1234
    #highlight-end
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env
    cloudEnvFile: ../.env.cloud
```

When you initialize a new Hasura Cloud project, the `project` key-value will automatically be set by the CLI. You can
also override this manually using [the `ddn context set` command](/reference/cli/commands/ddn_context_set.mdx).

This key-value pair simplifies switching contexts across development stages. For instance, the `default` context could
be used for production builds, while a separate `staging` context could map to a dedicated project for API collaboration
and testing before moving to production.

### Supergraph

The `supergraph` key-value pair maps to a
[Supergraph metadata object](reference/metadata-reference/build-configs.mdx#supergraph-supergraph), which contains a
list of the subgraphs to include in any supergraph build.

```yaml title="When the default context is set, the root supergraph.yaml will be used by the CLI:"
contexts:
  default:
    project: great-ddn-1234
    #highlight-start
    supergraph: ../supergraph.yaml
    #highlight-end
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env
    cloudEnvFile: ../.env.cloud
```

By default, this is set to the root `supergraph.yaml` when you initialize a local project. However, you can customize
this to use any valid Supergraph metadata object.

As with subgraphs below, setting this value makes any supergraph-related CLI commands more concise by eliminating the
need for extra flags.

### Subgraph

The `subgraph` key-value pair maps to a
[Subgraph metadata object](reference/metadata-reference/build-configs.mdx#subgraph-subgraph) which contains information
about which connectors are included in the subgraph and mappings for environment variables.

```yaml title="With the configuration below, if the current context is default, the app/subgraph.yaml configuration file will be used:"
contexts:
  default:
    project: great-ddn-1234
    supergraph: ../supergraph.yaml
    #highlight-start
    subgraph: ../app/subgraph.yaml
    #highlight-end
    localEnvFile: ../.env
    cloudEnvFile: ../.env.cloud
```

For projects with multiple subgraphs, it's handy to use this command to switch between subgraphs:

```sh
ddn context set subgraph <path-to-subgraph-configuration-file>
```

This ensures common actions â€” like adding data connectors and generating metadata after introspection â€” are executed by
the CLI in the correct subgraph, keeping your supergraph organized and resources with their appropriate owners.

### localEnvFile

Your project is initialized with a root-level `.env` file and this is automatically set in the `default` context.
Whenever a new environment variable is added â€” such as when initializing a new connector â€” the CLI will add the
appropriate key-value pairs to this file. It is used in local development such as when running your local services
(i.e., the engine and connectors).

```yaml title="When the default context is set, the root .env file will be used by the CLI for any local development commands:"
contexts:
  default:
    project: great-ddn-1234
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    #highlight-start
    localEnvFile: ../.env
    #highlight-end
    cloudEnvFile: ../.env.cloud
```

Ideally, the values here will â€” such as connection strings to data sources â€” will be mapped to development instances,
whether that be locally or hosted elsewhere. This makes it easy to work with what will eventually become a production
build, but using local resources for quicker iteration and shorter feedback loops.

Any time you need to add additional environment variables to a connector, you can
[use the CLI](/reference/cli/commands/ddn_connector_env_add.mdx).

### cloudEnvFile

Your project includes a root-level `.env.cloud` file, which is automatically set in the current context whenever a new
cloud project is initialized. This file is specifically used for cloud-related operations, such as deploying or managing
your services in a cloud environment.

```yaml title="When the default context is set, the root .env.cloud file will be used by the CLI for any cloud-related commands:"
contexts:
  default:
    project: great-ddn-1234
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env
    #highlight-start
    cloudEnvFile: ../.env.cloud
    #highlight-end
```

:::tip A localEnvFile is required before setting the cloudEnvFile

A `localEnvFile` is required before setting the `cloudEnvFile`. Ensure your `localEnvFile` is set for the current
context **before** initializing a new cloud project to avoid any deployment issues.

:::

## Next steps

Now that you have a better idea of configuring your project for various contexts,
[learn how to collaborate with others](/project-configuration/project-management/manage-collaborators.mdx) by adding
collaborators and defining their roles.



--- File: ../ddn-docs/docs/plugins/caching/how-to.mdx ---
# How to Configure the Caching Plugin

---
title: How to Configure the Caching Plugin
sidebar_position: 2
description: "Learn how to configure the Caching Plugin for Hasura DDN."
keywords:
  - hasura plugins
  - caching plugin
  - plugins architecture
  - engine plugins
  - how-to
  - guide
---

# How to Configure the Caching Plugin

## Introduction

The [caching plugin](https://github.com/hasura/engine-plugin-caching) adds the ability to specify a list of queries
whose responses should be cached.

## Setting up for local development

We can set up everything we need for local development through Docker. Add the following entries to your `compose.yaml`
file:

```yaml
redis:
  image: redis:latest
  ports:
    - 6379:6379

caching:
  build:
    context: https://github.com/hasura/engine-plugin-caching.git
  ports:
    - 8787:8787
  extra_hosts:
    - local.hasura.dev=host-gateway
  volumes:
    - ./globals/plugins/caching-config.js:/app/src/config.js
```

Here, we've added a Redis instance to our Docker Compose project, as well as an instance of the caching plugin. The
caching plugin takes a config file (which we've said here is saved in `./globals/plugins/caching-config.js`, though the
path is up to you). Here's an example config file:

```javascript
export const Config = {
  // Client header configuration
  headers: {
    // A secret that must be provided in incoming requests from the engine.
    // Change this to whatever you'd like, though remember to update the
    // references further on.
    "hasura-m-auth": "zZkhKqFjqXR4g5MZCsJUZCnhCcoPyZ",
  },

  // A URL for redis. If you copied the docker-compose configuration for
  // `redis` above, this doesn't need changing.
  redis_url: "redis://redis:6379",

  // OpenTelemetry configuration. The name of this environment variable will
  // depend on your subgraph name - check your `.env` file to find the correct
  // name. You can also specify any further headers that your telemetry
  // collector may require.
  otel_endpoint: process.env.GLOBALS_OTEL_EXPORTER_OTLP_ENDPOINT,
  otel_headers: {},

  // A list of queries that we want to cache. Note that these queries will be
  // cached based on their parsed structures, so white space doesn't matter.
  queries_to_cache: [
    {
      query: ` query MyQuery {
          customers {
            firstName
            lastName
          }
        }
      `,
      // How long a cached response should live (in seconds).
      time_to_live: 600,
    },
  ],
};
```

The `queries_to_cache` list can be extended to contain all the queries you'd like to be cached. Note that the query
response is cached for each set of session variables and each role, as these may yield different outputs.

:::info Using environment variables

This example uses hard-coded values for the URLs and the request headers, though these can be dynamically and securely
injected using environment variables. Changing the `value` key to `valueFromEnv` allows us to specify the name of an
environment variable from which to get this information. Note that the variables are defined via the `envMapping` config
in `subgraph.yaml`, which states which environment variables should be inherited from the root `.env`.

:::

## Adding the plugin to your project

Once we've configured the plugin, running `ddn run docker-start` should work happily. Now, we just need to configure
Hasura to use the plugin. Add the following to one of your `subgraph.yaml` files:

```yaml
---
kind: LifecyclePluginHook
version: v1
definition:
  pre: parse
  name: cache_get_test
  url:
    valueFromEnv: HASURA_CACHING_PRE_PARSE_URL
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: zZkhKqFjqXR4g5MZCsJUZCnhCcoPyZ
      rawRequest:
        query: {}
        variables: {}
---
kind: LifecyclePluginHook
version: v1
definition:
  pre: response
  name: cache_set_test
  url:
    valueFromEnv: HASURA_CACHING_PRE_RESPONSE_URL
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: zZkhKqFjqXR4g5MZCsJUZCnhCcoPyZ
      rawRequest:
        query: {}
        variables: {}
```

:::info Using environment variables

We've used `valueFromEnv` so that we can dynamically and securely add values from our environment variables. You can add
these values to your root-level `.env` and then map them in the `globals` subgraph.yaml file. Alternatively, you can
include raw strings here using `value` instead of `valueFromEnv` and passing the keys.

For local development, `HASURA_CACHING_PRE_PARSE_URL` should be `http://local.hasura.dev:8787/pre-parse`, and
`HASURA_CACHING_PRE_RESPONSE_URL` should be `http://local.hasura.dev:8787/pre-response`.

:::

## Running the project

At this point, we can create a build of our project and start local development:

```bash
ddn supergraph build local
ddn run docker-start
```

Queries marked in the caching config as cacheable should now be cached. The caching plugin will output logs to indicate
which requests have and have not been cached, so `docker compose logs -f caching` will allow you to watch these logs as
they arise.

## Deploying the plugin

The connector can be deployed as a regular HTTP service, anywhere an Express server can be deployed. When deployed, make
sure to set the `HASURA_CACHING_PRE_PARSE_URL` and `HASURA_CACHING_PRE_RESPONSE_URL` to appropriate values in
`.cloud.env`. Note that the plugin must be visible from your Hasura deployment: if hosting in the Hasura Cloud, the
plugin must be publicly visible. In this instance, make sure to set the `hasura-m-auth` header to something other than
the example given in this guide to keep the plugin secure from malicious third-party users.



--- File: ../ddn-docs/docs/project-configuration/project-management/manage-collaborators.mdx ---
# Manage project collaborators

---
sidebar_position: 2
sidebar_label: Manage project collaborators
description: "Learn how to invite others to a project."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
  - collaborate
---

import Thumbnail from "@site/src/components/Thumbnail";
import Roles from "@site/docs/project-configuration/_roles.mdx";
import Permissions from "@site/docs/project-configuration/_permissions.mdx";

# How to Manage Project Collaborators

## Introduction

Hasura DDN allows multiple users and teams to work together as collaborators on projects by assigning each user specific
roles and permissions.

You can [invite users](#invite-collaborators) to a project, or allow users to [request access](#request-access).

:::info Only available on DDN Base and higher

In order to add collaborators, your project must either be a
[DDN Base or DDN Advanced project](https://hasura.io/pricing).

:::

## Available roles {#roles}

<Roles />

The following are the detailed permissions for the above roles:

<Permissions />

## How to invite collaborators {#invite-collaborators}

### Step 1. Navigate to the Collaborators section

Open your project's console at [https://console.hasura.io](https://console.hasura.io) and select it from the list of
available projects. Once the project is open, click `Settings` in the bottom-left corner and then select `Collaborators`
from the `Project Settings` menu:

<Thumbnail src="/img/ci-cd/0.0.1_console_invite-collaborator.png" alt="Invite a collaborator" width="1000" />

### Step 2. Enter information

Click the `+ Invite Collaborator` button in the top-right corner of the `Collaborators` section and enter the
collaborator's email address, select the access level you'd like to assign them, and click `Invite`.

<Thumbnail src="/img/ci-cd/0.0.1_console_assign-collaborator-role.png" alt="Invite a collaborator" width="1000" />

<br />
:::info Granular Access (Subgraph Collaborators) Only available on DDN Advanced

In order to add subgraph collaborators, your project must be a [DDN Advanced project](https://hasura.io/pricing/ddn).

:::

The invitee will receive an email with a link allowing them to accept the invite and join the project.

## How to accept a collaboration invite {#accept-invite}

### Step 1. Click the link in your email

From your email, click the `View invitation` button. This will send you to
[https://console.hasura.io](https://console.hasura.io) where you can accept it and then explore and contribute to the
project according your [role](#roles).

### Step 2. Explore the project

From your new project, you can explore the console by:

- [Running queries](/graphql-api/queries/) from the GraphiQL explorer.
- Visualizing the supergraph with the Explorer tab.
- Seeing other collaborators present in the project.

### Step 3. Learn how to develop locally

The owner of the project most likely has a Git repository with the project's contents available on a service such as
GitHub. To run the supergraph locally, and make contributions to the deployed supergraph,
[pick up here](/quickstart.mdx) in our getting started docs.

## Allow users to request access {#request-access}

You can adjust your project's settings to allow users to request access when navigating to your project's URL. To do
this, click the `Share` button in the top navigation of your project and select `Request Access`.

<Thumbnail
  src="/img/get-started/console_read-only.png"
  alt="Allow users to request access to your project"
  width="1000"
/>

Each time a user requests access, you'll be able to approve or deny the request from this modal.

## More information

See more about Hasura DDN plans and pricing [here](/reference/pricing.mdx).



--- File: ../ddn-docs/docs/project-configuration/project-management/manage-environments.mdx ---
# Environments

---
sidebar_position: 2.5
sidebar_label: Environments
---

# Alternative Configuration Files per Environment

## Introduction

Following on from setting up and
[managing multiple contexts](/project-configuration/project-management/manage-contexts.mdx), you can also specify
different configuration files per environment. This is useful if you want to use different setups for each.

## Example

For example, if in `development` you want to use `noAuth` mode but for `staging` and `production` you want to use JWT
mode, you can create a supergraph config file for each environment and then specify the correct file in the context.

In the following example we have a `supergraph-development.yaml` file which specifies a chain to the
`subgraph-development.yaml` to the `metadata_development` directory to include for the metadata which sets the `noAuth`
mode for development context.

```yaml title="supergraph-development.yaml"
kind: Supergraph
version: v2
definition:
  subgraphs:
    # highlight-start
    - globals/subgraph-development.yaml
    # highlight-end
    - my_subgraph/subgraph.yaml
```

```yaml title="globals/subgraph-development.yaml"
kind: Subgraph
version: v2
definition:
  name: globals
  generator:
    rootPath: .
  # highlight-start
  includePaths:
    - metadata_development
  # highlight-end
```

```yaml title="globals/metadata_development/auth-config.hml"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: admin
      sessionVariables: {}
```

Then similarly, we would have the supergraph file for the other environments to use which specifies JWT mode in the
`auth-config.yaml` file. You can read more about [auth here](/auth/overview.mdx).



--- File: ../ddn-docs/docs/project-configuration/project-management/service-accounts.mdx ---
# Service Accounts

---
sidebar_position: 3
sidebar_label: Service Accounts
description: "Learn how to create and delete service accounts and regenerate access tokens for the same."
keywords:
  - service account
  - automation
  - continuous integration
  - continuous deployment
  - workflow
  - pipeline
  - hasura ddn
  - graphql api
  - collaborate
---

import Thumbnail from "@site/src/components/Thumbnail";

# Service Accounts

## Introduction

A service account represents an application, service, or automated process rather than a human user. These accounts are
employed for running background jobs, connecting systems, or performing specific operations without requiring manual
intervention.

Project owners or administrators can create and delete service accounts as needed. These accounts can act as
collaborators on other projects, just like regular users, and can hold roles such as supergraph admin or subgraph admin.

Additionally, project owners or administrators can regenerate service account tokens, making them a secure and ideal
alternative to Personal Access Tokens (PAT) for use in CI/CD workflows.

:::info Only available on DDN Base and higher

In order to create service accounts, your project must either be a
[DDN Base or DDN Advanced project](https://hasura.io/pricing/ddn).

:::

## How to create service account

### Step 1: Navigate to the Service Accounts section

As an owner or administrator on the project, open your project's console at
[https://console.hasura.io](https://console.hasura.io) and click `Settings` in the bottom-left corner. Then select
`Service Accounts` from the `Project Settings` menu and click `New Service Account`:

<Thumbnail
  src="/img/service-account/0.0.1_console_create_service_account.png"
  alt="Create a service account"
  width="1000"
/>

### Step 2: Enter details

Enter service account name and click `Continue`:

<Thumbnail
  src="/img/service-account/0.0.1_console_enter_service_account_name.png"
  alt="Create a service account"
  width="1000"
/>

### Step 3: Save service account token

After creating the service account, copy the service account token and store it securely. This token allows the CLI to
authenticate your service account You will not be able to view the token again after this step.

<Thumbnail
  src="/img/service-account/0.0.1_console_copy_service_account_token.png"
  alt="Create a service account"
  width="1000"
/>

### Step 4: Setup service account permissions

After saving the service account token, select the project and access level you want to grant to the service account,
and click `Give Access`:

<Thumbnail
  src="/img/service-account/0.0.1_console_setup_service_account_permissions.png"
  alt="Create a service account"
  width="1000"
/>

<br />
:::info Granular Access Only available on DDN Advanced

In order to add service account as a subgraph administrator, your project must be a
[DDN Advanced project](https://hasura.io/pricing/ddn).

:::

You can skip this step and assign permissions later by
[inviting the service account](/project-configuration/project-management/manage-collaborators.mdx) as a project
collaborator.

## How to use service account token

### Step 1. Login via CLI

```bash
ddn auth login --access-token <service-account-token>
```

### Step 2. Create and apply supergraph build

```bash
# Create supergraph build
ddn supergraph build create [flags]

# Apply supergraph build
ddn supergraph build apply <supergraph-build-version> [flags]
```

## How to regenerate service account token

Navigate to the `Service Accounts` section and click the `Regenerate` button next to the service account for which you
want to regenerate the token:

<Thumbnail
  src="/img/service-account/0.0.1_console_regenerate_service_account_token.png"
  alt="Create a service account"
  width="1000"
/>

## How to delete service account

### Step 1: Navigate to the Service Accounts section

Click the `Delete` button next to the service account you want to delete:

<Thumbnail
  src="/img/service-account/0.0.1_console_delete_service_account.png"
  alt="Create a service account"
  width="1000"
/>

### Step 2: Verify and confirm deletion

<Thumbnail
  src="/img/service-account/0.0.1_console_confirm_service_account_deletion.png"
  alt="Create a service account"
  width="1000"
/>

## More information

See more about Hasura DDN plans and pricing [here](/reference/pricing.mdx).



--- File: ../ddn-docs/docs/project-configuration/project-management/console-collaborator-comments.mdx ---
# Comment on Metadata

---
sidebar_position: 5
sidebar_label: Comment on Metadata
description: "Collaborate with your peers by commenting on your Supergraph Metadata"
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
  - collaborate
  - comment
---

import Thumbnail from "@site/src/components/Thumbnail";

# Commenting on Metadata

## Introduction

Hasura DDN provides a commenting feature that allows your API producers and consumers to start conversations directly on
the API metadata. This feature enhances collaboration by closing the feedback loop and helps teams communicate more
effectively about their API design and implementation.

:::tip Getting Access

This feature is available for all users on [Base and Advanced plans](https://hasura.io/pricing).

:::

## How to comment

You can add comments on various objects from your metadata.

1. Navigate to any model page in your project via the `Explorer` tab.
2. Hover over the field or section you want to comment on.
3. Click on the comment icon that appears.

<Thumbnail src="/img/get-started/console_comment-create.png" alt="Hover over fields to add comments" width="1000" />

## Commenting areas

### Explorer Tab

The **Explorer Tab** is the primary interface where users interact with the API metadata, making it a crucial place for
collaboration. Comments here enable API producers and consumers to discuss design decisions, clarify data models, and
suggest improvements directly on specific metadata elements.

- **Supergraph Page** â€“ Comment on the overall schema design, structure, and implementation details of the supergraph.
- **Subgraph Page** â€“ Provide feedback on individual subgraphs, ensuring alignment with the larger supergraph design.
- **Models â†’ General**
  - **Description** â€“ Clarify the modelâ€™s purpose and usage for better documentation.
  - **Signature** â€“ Discuss function signatures, return types, and argument structures.
  - **GraphQL Root Fields** â€“ Suggest improvements or changes to the root field definitions.
- **Models â†’ Fields & Relationships**
  - **Output Fields** â€“ Ask questions or provide insights on field usage and expected data.
  - **Arguments** â€“ Discuss argument types, required vs. optional parameters, and potential defaults.
  - **Relationships** â€“ Ensure relationships between models are well-defined and documented.
- **Models â†’ Permissions**
  - **Role** â€“ Comment on role-based access control settings.
  - **Read / Create / Update / Delete** â€“ Discuss permission settings for different CRUD operations.

This feature is especially useful for teams working on large-scale API projects, as it ensures everyone stays aligned on
API structure and permissions.

<Thumbnail src="/img/get-started/comments_explorer.png" alt="Comments on Explorer" width="800" />

---

### GraphiQL Tab

The **GraphiQL Tab** is where users can interactively query the API and test GraphQL operations. Commenting here allows
for real-time feedback on API responses, query structures, and potential performance optimizations.

- Discuss unexpected query results and suggest potential schema modifications.
- Collaborate on best practices for query structuring and field selection.
- Provide feedback on performance concerns, such as response times and pagination strategies.
- Leave notes on commonly used queries to improve team-wide API consistency.

By adding comments directly in GraphiQL, teams can streamline debugging and optimize API queries collaboratively.

<Thumbnail src="/img/get-started/comments_graphiql.png" alt="Comments on GraphiQL" width="800" />

---

### Insights Tab

The **Insights Tab** provides performance metrics, traces, and reports about API usage. Commenting here helps teams
analyze and discuss API behavior, identify bottlenecks, and track improvements over time.

- **Performance** â€“ Leave feedback on latency, throughput, and error rates.
- **Platform Report** â€“ Discuss API usage patterns and suggest improvements.
- **Traces** â€“ Analyze request traces and collaborate on optimizing execution paths.

This feature is valuable for DevOps, backend engineers, and API consumers looking to enhance API efficiency.

<Thumbnail src="/img/get-started/comments_insights.png" alt="Comments on Insights" width="800" />

---

### Builds Tab

The **Builds Tab** allows teams to track and validate schema changes across supergraph, subgraph, and connector builds.
Commenting in this area helps teams coordinate schema evolution and avoid breaking changes.

- **Supergraph Builds** â€“ Discuss schema changes at the supergraph level and their impact on consumers.
- **Subgraph Builds** â€“ Leave comments about specific subgraph updates and dependencies.
- **Connector Builds** â€“ Provide feedback on connector integrations and compatibility.
- **Schema Diff** â€“ Highlight breaking changes or inconsistencies between schema versions.

By facilitating discussions on builds, teams can ensure smooth API evolution and prevent unexpected failures.

<Thumbnail src="/img/get-started/comments_builds.png" alt="Comments on Builds" width="800" />

## Notifications

In-app notifications show new comments made on your project.

The notification hub can be found in the top right corner of the console. On clicking the comments button, you will see
all the comments where you are tagged in one place. You can click on a particular comment (deep linking) and go to the
original thread on the console. You can also delete notifications from that menu.

{/* <Thumbnail src="/img/get-started/comments_notification.png" alt="Notifications" width="100" height="100" /> */}
![Alt text](/img/get-started/comments_notification.png)

<br />
<br />

:::info Invite collaborators

You can learn how to invite collaborators [here](/project-configuration/project-management/manage-collaborators.mdx).

:::

## Limitations

The feature is in early access and has known limitations, which are in our backlog. Let us know if you would like to
prioritize any specific functionality.

1. Tagging users on comments
2. Resolving comments
3. Email notifications
4. Ability to auto notify subgraph admin and developers.
5. History Tab for comments.



--- File: ../ddn-docs/docs/plugins/restified-endpoints/index.mdx ---
# RESTified Endpoints Plugin

---
title: RESTified Endpoints Plugin
sidebar_position: 1
description: "Learn more about the RESTified Endpoints Plugin for Hasura DDN."
keywords:
  - hasura plugins
  - plugins
  - engine plugins
  - custom execution
  - custom endpoints
  - custom handlers
  - restified endpoints
  - restified graphql
---

# RESTified Endpoints Plugin

The [RESTified Endpoints Plugin](https://github.com/hasura/engine-plugin-restified-endpoint) allows you to add RESTified
GraphQL endpoints to DDN. This can be used to add custom REST endpoints to DDN that will execute a specified GraphQL
query on the DDN GraphQL API and return the response.

The plugin integrates with Hasura DDN as a **pre-route plugin** and can be deployed as an HTTP service using tools like
Cloudflare Workers or similar services.

Key benefits of using the RESTified Endpoints Plugin include:

- **Simplified API Design**: The plugin enables you to design and expose your GraphQL API as RESTful endpoints, making
  it easier for clients to interact with your API using familiar HTTP methods and conventions.
- **Flexibility**: You can define custom REST endpoints that correspond to specific GraphQL queries, allowing you to
  tailor your API to the needs of your clients.
- **Easy Integration**: The plugin seamlessly integrates with Hasura DDN, ensuring that your RESTified endpoints are
  well-integrated with your existing GraphQL API, providing a single, unified interface for your clients.

## Next steps

To get started with configuring and deploying the RESTified Endpoints Plugin, refer to the
[guide](/plugins/restified-endpoints/how-to.mdx), which walks you through the process of setting up, configuring, and
deploying the plugin.



--- File: ../ddn-docs/docs/plugins/restified-endpoints/how-to.mdx ---
# How to Configure the RESTified Endpoints Plugin

---
sidebar_position: 2
sidebar_label: How to Configure the RESTified Endpoints Plugin
description: "Learn how to configure the RESTified Endpoints plugin to add RESTified GraphQL endpoints to DDN."
keywords:
  - hasura plugins
  - plugins
  - engine plugins
  - custom execution
  - custom endpoints
  - custom handlers
  - restified endpoints
  - restified graphql
seoFrontMatterUpdated: false
---

# How to Configure the RESTified Endpoints Plugin

## Introduction

The [RESTified GraphQL endpoints plugin](https://github.com/hasura/engine-plugin-restified-endpoint) allows you to add
RESTified GraphQL endpoints to DDN. This can be used to add custom REST endpoints to DDN that would execute specified
GraphQL query on the DDN GraphQL API and return the response.

:::info We're using Cloudflare Wrangler

In this example, we're using Cloudflare Wrangler to deploy our plugin as a Cloudflare Worker. However, you can use any
other tool or service that hosts HTTPS services. You can get started with Wrangler
[here](https://developers.cloudflare.com/workers/wrangler/install-and-update/).

:::

## Step 1. Create a new plugin project

Create a new Cloudflare Worker project using the `create-cloudflare` command with the
[`restified-endpoints` plugin template](https://github.com/hasura/engine-plugin-restified-endpoint):

```bash
npx create-cloudflare@latest restified-endpoints-plugin --template https://github.com/hasura/engine-plugin-restified-endpoint
```

This will create a new project with the required files and dependencies.

## Step 2. Install dependencies

Navigate to the new directory and install the dependencies.

```bash
cd restified-endpoints-plugin
npm install
```

Start the local development server.

```bash
npm start
```

## Step 3. Add the plugin configuration

We'll let the engine know about the plugin and to execute it as a pre-route plugin by creating a new metadata file. In
your `global` subgraph's metadata directory, create a new file named `restified-endpoints.hml` and add the following
configuration.

```yaml
kind: LifecyclePluginHook
version: v1
definition:
  pre: route
  name: restified_endpoints
  url:
    valueFromEnv: RESTIFIED_ENDPOINTS_URL
  config:
    matchPath: "/v1/api/rest/*"
    matchMethods: ["GET", "POST"]
    request:
      method: GET
      headers:
        forward:
          - Authorization
          - x-hasura-role
          - x-hasura-ddn-token
        additional:
          hasura-m-auth:
            valueFromEnv: M_AUTH_KEY
      rawRequest:
        path: {}
        query: {}
        method: {}
        body: {}
    response:
      headers:
        additional:
          content-type:
            value: application/json
```

:::tip URL Match

The `matchPath` field is used to match the regex to the request path, while `matchMethods` specifies the HTTP methods to
match. In this example, we're matching all `GET` and `POST` requests to `/v1/api/rest/*`. You can modify these to match
any path and methods (GET/POST/PUT/PATCH/DELETE) you wish.

Also, note that we can not use the pre-defined DDN endpoints (like `/graphql`, `/v1/sql`, `/v1/rest`, `/v1/explain`,
`/healthz` and `/metrics`) in the `matchPath` field.

Moreover, on DDN cloud, for security reasons, only the `/v1/api/rest/*` path is allowed for the pre-route plugin.

:::

:::info Using environment variables

We've used `valueFromEnv` so that we can dynamically and securely add values from our environment variables. You can add
these values to your root-level `.env` and then map them in the `globals` subgraph.yaml file. Alternatively, you can
include raw strings here using `value` instead of `valueFromEnv` and passing the keys.

:::

Next, update the `subgraph.yaml` file to include the metadata file and the environment variables.

```yaml
kind: Subgraph
version: v2
definition:
  name: globals
  ...
  includePaths:
    ...
    - restified-endpoints.hml
  envMapping:
    RESTIFIED_ENDPOINTS_URL:
      fromEnv: RESTIFIED_ENDPOINTS_URL
    M_AUTH_KEY:
      fromEnv: M_AUTH_KEY
```

Finally, we need to add the environment variables to the `.env` file.

```bash
RESTIFIED_ENDPOINTS_URL="http://local.hasura.dev:8787"
M_AUTH_KEY="your-strong-m-auth-key"
```

:::tip M-Auth Key

The `hasura-m-auth` header is a custom header that is used to authenticate the requests to the allowlist plugin. You can
use any strong key here to authenticate the plugin. DDN will automatically add this header to the requests to the
plugin. Also, make sure to update the `src/config.ts` file (in step 5) with the same key.

:::

## Step 4. Create a new build for local development

Create a new supergraph build.

```bash
ddn supergraph build local
```

Start the console for the local supergraph.

```bash
ddn console --local
```

You can now test the plugin by making a request to the RESTified GraphQL endpoints defined in the plugin's
configuration.

## Step 5. Update the plugin config

Update the `src/config.ts` file with the RESTified GraphQL endpoints that you want to add.

```typescript
export const Config = {
  graphqlServer: {
    headers: {
      additional: {
        "Content-Type": "application/json",
      },
      // Please make sure to forward the authentication headers here
      forward: ["hasura_cloud_pat", "x-hasura-ddn-token"],
    },
  },
  headers: {
    "hasura-m-auth": "your-strong-m-auth-key",
  },
  restifiedEndpoints: [
    ...,
    {
      path: "/v1/api/rest/users",
      methods: ["GET", "POST"],
      query: `
        query MyQuery($limit: Int = 10, $offset: Int = 0) {
          Users(limit: $limit, offset: $offset) {
            Name
          }
        }
      `,
    }
  ],
};
```

:::info Hot reloading

The local wrangler development server will automatically reload the plugin when you make changes to the code.

:::

## Step 6. Configure the plugin variables

:::info Setup tracing

To enable tracing for the plugin, you need to update the `wrangler.toml` file with the required configurations. If you
don't want to enable tracing for the plugin, you can skip this step.

:::

In `restified-endpoints-plugin` directory, update the `wrangler.toml` file with the required configurations.

```toml
...
[vars]
OTEL_EXPORTER_OTLP_ENDPOINT = "https://gateway.otlp.hasura.io:443/v1/traces"
OTEL_EXPORTER_PAT = "<PAT>"
GRAPHQL_SERVER_URL = "<DDN_GRAPHQL_SERVER_URL>"
```

Replace `<PAT>` with the Personal Access Token (PAT) for the Hasura Cloud account. You can display this using the
`ddn auth print-access-token` command.

Replace `<DDN_GRAPHQL_SERVER_URL>` with the URL of your DDN GraphQL server. You can find this in the DDN console under
the `Settings > Project Summary` section.

## Step 7. Deploy the plugin

For your plugin to be reachable by your hosted supergraph, we'll need to deploy using Cloudflare Wrangler. The `deploy`
command included in your plugin's `package.json` will do this automatically for you and return the hosted service's URL.

**Note**: Please also update the `wrangler.toml` with your cloud PAT for the tracing to work.

```bash
npm run deploy
```

This will deploy the plugin to Cloudflare Workers and return the URL of the hosted service. Next, update the .env.cloud
file with the URL.

```bash
RESTIFIED_ENDPOINTS_URL="https://<your-deployed-plugin>.workers.dev"
M_AUTH_KEY="your-strong-m-auth-key"
```

## Step 8. Create a new build

Create a new supergraph build on Hasura DDN to check your work in the cloud.

```bash
ddn supergraph build create
```

## Step 9. Apply the build

Apply the build to make it the default one served by your Hasura DDN project endpoint. You can do this from the DDN
console by choosing the build from the `Builds` tab and clicking `Apply Build`.

The engine will execute the plugin for each requests to the RESTified GraphQL endpoints you defined.



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/deprecated-metadata-upgrades.mdx ---
# Deprecated Metadata Upgrades

---
sidebar_position: 1
sidebar_label: Deprecated Metadata Upgrades
description: "Upgrade your metadata to take advantage of new features."
keywords:
  - hasura
  - hasura ddn
  - project metadata
  - supergraph metadata
  - upgrade metadata
seoFrontMatterUpdated: false
---

[//]: # "Add codemods in the order the user is expected to run them."
[//]: # "This is a descending order list. Newer codemods appear at the top."

# Deprecated Metadata Upgrades

Codemods can upgrade your metadata to the latest version so you can take advantage of the latest engine features.

Codemods below are listed with the CLI version they were added in.

## `upgrade-object-boolean-expression-types` (v2.4.0)

Upgrades the deprecated
[`ObjectBooleanExpressionType`](/reference/metadata-reference/boolean-expressions.mdx#objectbooleanexpressiontype-objectbooleanexpressiontype)
to the new [`BooleanExpressionType`](/reference/metadata-reference/boolean-expressions.mdx).

This enables filtering Models on nested objects and arrays, and based on relationships.

```bash
ddn codemod upgrade-object-boolean-expression-types
```

By default the codemod will run against the supergraph.yaml from the context. Provide `--supergraph` or `--subgraph` to
run the codemod against a different supergraph or subgraph.

## `upgrade-graphqlconfig-aggregate` (v2.4.0)

Add aggregates support to metadata through [`AggregateExpression`](/reference/metadata-reference/aggregate-expressions).
Use aggregates (like sum, min, count, etc) in your GraphQL API.

```bash
ddn codemod upgrade-graphqlconfig-aggregate
```

By default the codemod will run against the supergraph.yaml from the context.

The `GraphqlConfig` metadata object is required to be upgraded to enable aggregates for all subgraphs.

Run against a specific supergraph.yaml or subgraph.yaml file

```bash
ddn codemod upgrade-graphqlconfig-aggregate --supergraph ./supergraph.cloud.yaml
ddn codemod upgrade-graphqlconfig-aggregate --subgraph ./app/subgraph.yaml
```

## `upgrade-graphqlconfig-subscriptions` (v2.14.0)

Learn more about upgrading your metadata to have subscriptions by visiting [this page](/graphql-api/subscriptions/).



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/index.mdx ---
# Overview

---
sidebar_position: 1
sidebar_label: Overview
description: "Learn how legacy metadata configuration can be upgraded to newer versions."
keywords:
  - hasura
  - hasura ddn
  - project configuration
seoFrontMatterUpdated: false
---

# Upgrading legacy configurations

## Introduction

A series of legacy configurations and upgrade guides exist to help you bring your project up-to-speed with the latest
and greatest features of Hasura DDN.

## Project config upgrades

- [Upgrade to project config v3](upgrade-project-v3)
- [Upgrade to project config v2](upgrade-project-v2)

## Supergraph config upgrades

- [Upgrade to supergraph config v2](upgrade-supergraph-v2)

## Context config upgrades

- [Upgrade to context config v3](upgrade-context-v3)



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/upgrade-project-v2.mdx ---
# Upgrade to project config v2

---
sidebar_position: 1
sidebar_label: Upgrade to project config v2
description: "Guide to migrate local projects created by CLI versions <= 2024.06.05"
keywords:
  - hasura ddn
  - data delivery network
  - hasura cli
  - project configuration
  - hasura metadata management
  - yaml configuration
  - api development
  - hasura build profiles
  - hasura subgraphs
  - hasura project management
seoFrontMatterUpdated: true
---

import InstallTheCli from "@site/docs/_install-the-cli.mdx";

# Upgrading to project config v2

## What has happened?

A new revision (**2024.06.06**) of the [DDN CLI](/reference/cli/installation.mdx) has been released. With this release,
local project directory structures created by CLI versions **\<= 2024.06.05** are now out-of-date and deprecated.

This page contains some information on what has changed and instructions to migrate your existing local projects.
However, for users to most efficiently familiarize themselves with the new CLI and project structure, we recommend
setting up a new project from scratch by following the new [getting started guide](/how-to-build-with-ddn/overview.mdx).

:::warning A newer CLI version has been released since!

A newer revision (**v1.x.x**) of the [DDN CLI](/reference/cli/installation.mdx) has been released. See the upgrade guide
[here](project/configuration/overview.mdx If your local project was created by CLI versions **\<= 2024.06.05**, you will
need to update your local project directory using this current guide first before upgrading to the above revision.

:::

## What has changed?

- The project config version defined in `hasura.yaml` is bumped from `v1` to `v2`.
- Local dev workflows using Docker have been introduced.
- The DDN CLI commands. See details on the new commands [here](/reference/cli/commands/ddn).
- The local project directory structure.
- `SupergraphManifest` and `ConnectorManifest` objects are now called `Supergraph` and `Connector` respectively.
- The `source` field for `Relationship` objects has been deprecated in favour of the `sourceType` field. Though
  `Relationship` objects with the `source` field will continue to be supported.

We **strongly recommend** you go through our [new getting started guide](/how-to-build-with-ddn/overview) to learn about
all the changes and experience the new workflow to develop your API with DDN.

:::info No changes to existing DDN projects

Note that this a CLI only change and does not impact Hasura DDN projects. You can continue using existing DDN projects
and builds and also be able to create new builds using the new CLI revision (with the new local project structure).

:::

## Migrate an existing local project

Below are steps to convert a local project created with the DDN CLI versions `<= v2024.06.05` to the new project
structure.

### Step 1: Get the new CLI

<InstallTheCli revision="v2" />

Also, update your Hasura VS Code extension to `v0.6.x` if not done automatically.

:::info Need the previous CLI revision?

Note that the above will overwrite your current DDN CLI revision. You can re-install the previous revision by replacing
`/ddn/cli/v2/` with `/ddn/cli/v1/` in the above download command.

:::

### Step 2: Set up a fresh local project

#### Step 2.1: Initialize a new supergraph

Run:

```bash
ddn supergraph init --dir <new-proj-dir>
```

Set the default supergraph config file to the CLI context to avoid having to repeat it in future commands. This file
contains the `Supergraph` object similar to the `SupergraphManifest` object in your existing project directory.

```bash
cd <new-proj-dir> && ddn context set supergraph ./supergraph.local.yaml
```

#### Step 2.2: Copy over supergraph global objects

Copy the files containing the supergraph global objects `CompatibilityConfig`, `GraphqlConfig` and `AuthConfig` from
your existing project directory to the `supergraph_globals` directory in the new project directory. These should
typically be in the `supergraph` directory in your existing project directory.

Note that these objects are already created in the `supergraph_globals` directory of your new project directory. The
`AuthConfig` object created here is to be used for local development. To avoid overwriting it, while copying your
`auth-config.hml` file from the existing project directory rename it to `auth-config.cloud.hml` instead. The
`graphql-config.hml` and `compatibility-config.hml` files on the other hand should be overwritten.

### Step 3: Add your subgraphs

For each subgraph in your project:

Follow the below steps:

#### Step 3.1: Initialize the subgraph

Run:

```bash
ddn subgraph init <subgraph-name>
```

#### Step 3.2: Add an env file for deploying to DDN

A `.env.<subgraph-name>` file is created in the `<subgraph-name>` directory to be used during local development.

Create a new file `.env.cloud.<subgraph-name>` alongside it to be used for deployments to DDN. You can leave this file
empty for now.

### Step 4: Set up your data connectors

**Case 1:**

For self-deployed data connectors in your subgraphs, i.e. `ConnectorManifest` of type `endpoints`:

You can skip this step.

**Case 2:**

For each data connector deployed on Hasura DDN in your subgraphs, i.e. `ConnectorManifest` of type `cloud`:

Follow the steps below:

#### Step 4.1: Initialize data connector

Run:

```bash
ddn connector init <connector-name> --subgraph <subgraph-name> --hub-connector <connector-type>
```

Ensure you use the same connector name and type from your existing `ConnectorManifest`.

#### Step 4.2: Create connector configuration files for deploying to DDN

##### Step 4.2.1: Create a connector.cloud.yaml

Each connector utilizes a `connector.yaml` file for local development. This file contains the `Connector` object similar
to the `ConnectorManifest` object in your existing project.

To deploy your connector to DDN create a `connector.cloud.yaml` file alongside the connector's local `connector.yaml`.
You can simply copy the contents of the `connector.yaml` file and update the `envFile` key's value to `.env.cloud` which
we'll create now.

```yaml title="For example, <subgraph-name>/connector/<connector-name>/connector.cloud.yaml"
kind: Connector
version: v1
definition:
  name: <connector-name>
  subgraph: <subgraph-name>
  source: hasura/<connector-type>:<version>
  context: .
  #highlight-start
  envFile: .env.cloud
  #highlight-end
```

##### Step 4.2.2: Create a .env.cloud for the connector

Create an environment variable file `.env.cloud` alongside the `.env.local` file for the connector. You can leave this
file empty for now.

#### Step 4.3: Add data connector configuration

##### Case 1: For hasura/postgres connectors

###### Step 4.3.1: Set database connection string

Add a new key `CONNECTION_URI` to the file `<subgraph_name>/connector/<connector_name>/.env.cloud` and set your database
connection string as the value.

For example:

```env
CONNECTION_URI=<your_pg_url>
```

###### Step 4.3.2: Introspect the database schema

Run:

```bash
ddn connector introspect --connector <subgraph-name>/connector/<connector-name>/connector.cloud.yaml
```

##### Case 2: For hasura/nodejs connectors

###### Step 4.3.1: Copy over functions

Copy the file(s) containing your TS functions from your existing project directory to the
`<subgraph-name>/connector/<connector-name>` directory in the new project directory. This file should typically be the
`functions.ts` file in the `<subgraph-name>/<connector-name>/connector` directory in your existing project directory.

### Step 5: Bring over connector related metadata

For each data connector:

Follow the below steps:

#### Step 5.1: Copy over connector related metadata files

Copy the files containing metadata objects related to the connector from your existing project directory to the
`<subgraph-name>/metadata` directory in the new project directory (the directory might need to be created). These files
should typically be in the `<subgraph-name>/<connector-name>` directory in your existing project directory and be
suffixed as `.hml`.

- the file with the `DataConnectorLink` object. Typically `<connector-name>.hml`
- the file with the data connector type objects. Typically `<connector-name>-types.hml`
- the files with Models, Commands, Relationships, etc. Typically in `models`/`commands` directory.

Within the `<subgraph-name>/metadata` directory you can choose to arrange these files however you like.

#### Step 5.2: Update DataConnectorLink object

In the `DataConnectorLink` object replace the `url` section with:

```
url:
    readWriteUrls:
      read:
        valueFromEnv: <CONNECTOR_NAME>_READ_URL
      write:
        valueFromEnv: <CONNECTOR_NAME>_WRITE_URL
```

We will set the values of these env vars in the following steps.

#### Step 5.3: (Optional) Update Relationship objects

As mentioned earlier in the what-has-changed section, the `source` field for `Relationship` objects has been deprecated
in favour of the `sourceType` field. Though `Relationship` objects with the `source` field will continue to be
supported, it is recommended to update them.

To update the relationship objects, rename the `source` key in your objects to `sourceType`.

For example:

```yaml
kind: Relationship
version: v1
definition:
  name: <rel-name>
  #highlight-start
  sourceType: <source-object-type>
  #highlight-end
  target: ...
```

### Step 6: Deploy to a DDN project

#### Step 6.1: Set the DDN project to deploy to

Run the following command to set your DDN project to the CLI context to avoid having to repeat it in future commands.
You can find your project name in the `hasura.yaml` file in your existing project directory.

```
ddn context set project <project-name>
```

#### Step 6.2: Deploy the data connectors

For each data connector deployed on Hasura DDN in your subgraphs, i.e. `ConnectorManifest` of type `cloud`:

Run:

```bash
ddn connector build create --connector <subgraph-name>/connector/<connector-name>/connector.cloud.yaml
```

On deployment completion, the read and write URLs for your deployed connector will be returned as a response. You will
need these in the next step.

#### Step 6.3: Set connector endpoints in env files

For each subgraph:

Update the file `<subgraph-name>/.env.cloud.<subgraph-name>` with the read and write URLs of the data connectors in the
subgraph.

For a self-deployed data connector in your subgraph, i.e. `ConnectorManifest` of type `endpoints`, use the connector URL
provided in the existing `ConnectorManifest` as both the read and write URLs.

For data connectors deployed on Hasura DDN in your subgraph, use the connector URLs returned in the previous deploy
step.

For example:

```env title="<subgraph-name>/.env.cloud.<subgraph-name>:"
<CONNECTOR_NAME_1>_READ_URL=<connector1-read-url>
<CONNECTOR_NAME_1>_WRITE_URL=<connector1-write-url>
<CONNECTOR_NAME_2>_READ_URL=<connector2-read-url>
<CONNECTOR_NAME_2>_WRITE_URL=<connector2-write-url>
```

#### Step 6.4: Deploy the supergraph

##### Step 6.4.1: Create a supergraph config file for deploying to DDN

In the root of the new project directory, create a new file `supergraph.cloud.yaml` and copy the contents of the
existing `supergraph.yaml` file to it.

Update the following values in the new file:

- `./supergraph_globals/metadata/auth-config.hml` -> To the name of the AuthConfig file copied earlier. e.g.
  `./supergraph_globals/metadata/auth-config.cloud.hml`
- For each subgraph: `envFile: <subgraph-name>/.env.<subgraph-name>` -> To the name of the cloud env files created
  earlier. e.g. `envFile: <subgraph-name>/.env.cloud.<subgraph-name>`

For example:

```yaml title="supergraph.cloud.yaml"
kind: Supergraph
version: v1
definition:
  supergraph_globals:
    generator:
      rootPath: ./supergraph_globals
    envFile: ./supergraph_globals/.env.supergraph_globals
    includePaths:
      #highlight-start
      - ./supergraph_globals/auth-config.cloud.hml
      #highlight-end
      - ./supergraph_globals/compatibility-config.hml
      - ./supergraph_globals/graphql-config.hml
  subgraphs:
    <subgraph-name>:
      generator:
        rootPath: <subgraph-name>/metadata
      #highlight-start
      envFile: <subgraph-name>/.env.cloud.<subgraph-name>
      #highlight-end
      includePaths:
        - <subgraph-name>/metadata
```

##### Step 6.4.2: Build and deploy your supergraph

Run:

```bash
ddn supergraph build create --supergraph ./supergraph.cloud.yaml
```

On build completion, the build version, API endpoint and console URLs will be returned as response.

#### Step 6.5: Verify migration

You can now head to your project console using the console URL returned in the previous step and verify the API
generated with the above build is the same as what you had earlier.

## Need help?

If you need help migrating your project or have any other questions please reach out to us on our
[Discord](https://hasura.io/discord).

## Legacy project structure

<details>
    <summary>
        See the legacy project structure before this update below.
    </summary>

| File type                                    | Description                                                                                                                |
| -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| [`hasura.yaml`](#hasurayaml)                 | The main configuration file.                                                                                               |
| [`*.env.yaml`](#envyaml)                     | A file that stores environment variables.                                                                                  |
| [`*.supergraph.yaml`](#supergraph-manifests) | Build manifest file(s) for a project.                                                                                      |
| `*.hml`                                      | Hasura metadata files for a project.                                                                                       |
| [`supergraph`](#supergraph)                  | A directory, containing all the necessary configuration and metadata files, for the supergraph-level scope of a project.   |
| [`<SUBGRAPH_DIRECTORY>`](#subgraphs)         | A directory, containing all the necessary configuration and metadata files, for a corresponding subgraph within a project. |

#### hasura.yaml

This is the entrypoint to a Hasura project.

```yaml
version: v1
project: <PROJECT_NAME>
subgraphs:
  app:
    path: ./app
defaultSupergraphManifest: base
```

The `version` section is used to specify the version of the configuration file. The `project` field is used to specify
the project name.

The `hasura.yaml` file also contains a `subgraphs` section. This section is used to specify the various
[subgraphs](#subgraphs) associated with the project.

#### \*.env.yaml

This file is used to store environment variables. The CLI generates a `base.env.yaml` for you by default. You can add
environment variables by adding key-value pairs under a particular subgraph:

```yaml
supergraph: {}
subgraphs:
  #highlight-start
  app:
    APP_CONNECTOR_CONNECTION_URI: "CONNECTION_URI"
#highlight-end
```

And then referencing them in your metadata using `valueFromEnv`:

```yaml
kind: ConnectorManifest
version: v1
spec:
  supergraphManifests:
    - base
definition:
  name: app_connector
  type: cloud
  connector:
    type: hub
    name: <CONNECTOR_NAME>
  deployments:
    - context: .
      env:
        CONNECTION_URI:
          #highlight-start
          valueFromEnv: APP_CONNECTOR_CONNECTION_URI
#highlight-end
```

:::info Accessing environment variables

In the example above, our `supergraphManifests` array references our `base.supergraph.hml` file, which contains an
`envfile` key that maps to our `base.env.yaml`. Thus, any environment variables present in this YAML file will then be
accessible in our HML.

If these files contain sensitive information, you should add them to your gitignore so as to avoid accidentally
committing them to version control.

:::

#### Supergraph manifests {#supergraph-manifests}

Supergraph manifests tell Hasura DDN how to construct your supergraph. A manifest will contain information such as which
subgraphs to include and which resources to use for the build.

```yaml
kind: SupergraphManifest
version: v1
definition:
  name: base
  envfile: base.env.yaml
  subgraphs:
    - app
```

#### Supergraph

The `supergraph` directory contains the supergraph configuration files. The three included files are:

```bash
â”œâ”€â”€ auth-config.hml
â”œâ”€â”€ compatibility-config.hml
â””â”€â”€ graphql-config.hml
```

:::info Organizing files

The contents of these files include the necessary metadata objects to build your supergraph. We've separated them out
into files organized by their purpose, but you can organize them anyway you like so long as the metadata objects within
them are all included.

:::

Each of these files' contents are used to configure the supergraph.

The `auth-config.hml` file contains the authentication configuration for the supergraph, allowing you to utilize either
JWTs or webhooks.

The `compatibility-config.hml` file contains the compatibility configuration â€”Â using a version number â€” for the
supergraph.

The `graphql-config.hml` file contains the GraphQL configuration for the supergraph, which allows you to customize the
available query and mutation capabilities along with the schema.

#### Subgraphs

Each subgraph is listed as a top-level directory in the root of the project. The CLI will init your project with an
`app` subgraph.

Each subgraph directory will contain the following folder structure when a new connector manifest is added for the
subgraph:

```bash
â””â”€â”€ <CONNECTOR_NAME>
â”‚Â Â  â”œâ”€â”€ <CONNECTOR_NAME>-types.hml
â”‚Â Â  â”œâ”€â”€ <CONNECTOR_NAME>.hml
â”‚Â Â  â”œâ”€â”€ connector
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ <CONNECTOR_NAME>.build.hml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ configuration.json
â”‚Â Â  â”‚Â Â  â””â”€â”€ schema.json
â”‚Â Â  â””â”€â”€ models
```

| File/Folder                             | Description                                                                        |
| --------------------------------------- | ---------------------------------------------------------------------------------- |
| `<CONNECTOR_NAME>`                      | The directory containing the build artifacts for the connector.                    |
| `<CONNECTOR_NAME>-types.html`           | The metadata file for the connector's types.                                       |
| `<CONNECTOR_NAME>.hml`                  | The metadata file for the connector based on the introspection of the data source. |
| `<CONNECTOR_NAME>/configuration.json`   | The configuration file for the connector.                                          |
| `<CONNECTOR_NAME>/schema.json`          | The schema file for the connector.                                                 |
| `/connector/<CONNECTOR_NAME>.build.hml` | The build manifest for the connector.                                              |

</details>



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/upgrade-supergraph-v2.mdx ---
# Upgrade to supergraph config v2

---
sidebar_position: 2
sidebar_label: Upgrade to supergraph config v2
description: "Guide to migrate local projects created by CLI versions <= 2024.07.09"
keywords:
  - hasura ddn
  - data delivery network
  - hasura cli
  - project configuration
  - hasura metadata management
  - yaml configuration
  - api development
  - hasura build profiles
  - hasura subgraphs
  - hasura project management
seoFrontMatterUpdated: true
---

import InstallTheCli from "@site/docs/_install-the-cli.mdx";

# Upgrading to supergraph config v2

## What has happened?

A new revision (**v1.x.x**) of the [DDN CLI](/reference/cli/installation.mdx) has been released with some changes to the
project directory structure and CLI commands. With this release, local project directory structures created by CLI
versions **\<= 2024.07.09** are now deprecated.

This page contains some information on what has changed and instructions to migrate your existing local projects.

:::warning A newer CLI version has been released since!

A newer revision (**v2.x.x**) of the [DDN CLI](/reference/cli/installation.mdx) has been released. See the upgrade guide
[here](project/configuration/overview.mdx If your local project was created by CLI versions **\< v2.0.0**, you will need
to update your local project directory using this current guide first before upgrading to the above revision.

:::

## What has changed?

- The project directory structure
  - See details on the updated directory structure [here](/project/configuration/overview.mdx - The `Supergraph` object
    is upgraded from `v1` to `v2`.
  - A `Subgraph` object is introduced to define how to build a subgraph.
  - The `AuthConfig`, `GraphqlConfig` and `CompatibilityConfig` metadata objects are now not defined as supergraph
    globals but can instead be defined in any subgraph of the supergraph. Note that these are still supergraph-level
    objects and only one of these objects can be defined across the supergraph, but they can now be defined in any of
    the subgraphs based on convenience. By default, these are now added to a new **globals** subgraph.
- The `--subgraph` flag in DDN CLI commands now accepts a path to a subgraph config file as opposed to the subgraph
  name.
- The DDN CLI now uses [SemVer](https://semver.org/) instead of CalVer for versioning.
- The `otel-collector` version is now updated to the latest version and the `otel-collector-config.yaml` is now updated
  accordingly.

:::info No changes to existing DDN projects

Note that this is a CLI-only change and does not impact Hasura DDN projects. You can continue using existing DDN
projects and builds and also be able to create new builds on them with both the new and old CLI revisions.

Though we strongly recommend updating to the latest version of the CLI and project structure by following the steps
below.

:::

## Migrate an existing local project

Below are steps to convert a local project created with the DDN CLI versions `<= v2024.07.09` to the new project
structure.

### Step 1: Get the new CLI

<InstallTheCli revision="v3" />

Also, update your Hasura VS Code extension to `v1.x.x` if not done automatically.

:::info Need the previous CLI revision?

Note that the above will overwrite your current DDN CLI revision. You can re-install the previous revision by replacing
`/ddn/cli/v3/` with `/ddn/cli/v2/` in the above download command.

:::

### Step 2: Upgrade existing Supergraph config v1 objects

You can update your existing Supergraph config v1 objects in your local project by running the following in your project
directory:

```bash
ddn codemod upgrade-supergraph-config-v1-to-v2 --dir <project-dir-path>
```

This will detect any existing Supergraph v1 objects in your project directory and initialize a wizard to update them to
the v2 format and make any other related changes, such as initializing the newly introduced Subgraph config objects and
setting up the new **globals** subgraph.

### Step 3: Create new **globals** subgraph on the DDN project

Run the following command to create a new **globals** subgraph on your DDN project:

```bash
ddn project subgraph create globals --project <project-name>
```

### Step 4: Verify the migration

Create a new supergraph build using:

```bash
ddn supergraph build create --supergraph ./supergraph.cloud.yaml
```

On build completion, the build version, API endpoint, and console URLs will be returned as in the response.

You can now head to your project console using the console URL returned in the previous step and verify the API
generated with the above build is the same as what you had earlier.

### Step 5: Update the OpenTelemetry exporter config

Replace the contents of the file `otel-collector-config.yaml` (present at the root of the project by default) with the
following:

```yaml
exporters:
  otlp:
    endpoint: https://gateway.otlp.hasura.io:443
    headers:
      Authorization: pat ${env:HASURA_DDN_PAT}
processors:
  batch: {}
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
service:
  pipelines:
    logs:
      exporters:
        - otlp
      processors:
        - batch
      receivers:
        - otlp
    metrics:
      exporters:
        - otlp
      processors:
        - batch
      receivers:
        - otlp
    traces:
      exporters:
        - otlp
      processors:
        - batch
      receivers:
        - otlp
```

You can now pin the version of `otel-collector` to a specific version (say `0.104.0`) to ensure breaking changes to the
OpenTelemetry collector config does not affect your local dev by setting the field `$.services.otel-collector.image` in
the `docker-compose.hasura.yaml` file to `otel/opentelemetry-collector:0.104.0`.

## Need help?

If you need help migrating your project or have any other questions please reach out to us on our
[Discord](https://hasura.io/discord).

## Legacy project structure

<details>
    <summary>
        See the legacy project structure before this update below.
    </summary>

```bash
.
â”œâ”€â”€ .devcontainer
â”‚  â””â”€â”€ devcontainer.json
â”œâ”€â”€ .hasura
â”‚  â””â”€â”€ context.yaml
â”œâ”€â”€ .vscode
â”‚  â”œâ”€â”€ extensions.json
â”‚  â”œâ”€â”€ launch.json
â”‚  â””â”€â”€ tasks.json
â”œâ”€â”€ engine
â”‚  â”œâ”€â”€ .env.engine
â”‚  â”œâ”€â”€ auth_config.json
â”‚  â”œâ”€â”€ metadata.json
â”‚  â””â”€â”€ open_dd.json
â”œâ”€â”€ globals
â”‚  â”œâ”€â”€ .env.globals.cloud
â”‚  â”œâ”€â”€ .env.globals.local
â”‚  â”œâ”€â”€ subgraph.cloud.yaml
â”‚  â”œâ”€â”€ subgraph.local.yaml
â”‚  â”œâ”€â”€ auth-config.cloud.hml
â”‚  â”œâ”€â”€ auth-config.local.hml
â”‚  â”œâ”€â”€ compatibility-config.hml
â”‚  â””â”€â”€ graphql-config.hml
â”œâ”€â”€ app
â”‚  â”œâ”€â”€ .env.app.cloud
â”‚  â”œâ”€â”€ .env.app.local
â”‚  â””â”€â”€ subgraph.yaml
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ compose.yaml
â”œâ”€â”€ hasura.yaml
â”œâ”€â”€ otel-collector-config.yaml
â”œâ”€â”€ supergraph.cloud.yaml
â””â”€â”€ supergraph.local.yaml
```

| File type                                 | Description                                                                                |
| ----------------------------------------- | ------------------------------------------------------------------------------------------ |
| [`hasura.yaml`](#hasurayaml)              | The file that denotes the root of a Hasura project.                                        |
| [`.env.*`](#envfiles)                     | Files that store environment variables.                                                    |
| [`supergraph.*.yaml`](#supergraph-config) | Files that describe how to build the supergraph.                                           |
| [`subgraph.*.yaml`](#subgraph-config)     | Files that describe how to build a subgraph.                                               |
| [`connector.*.yaml`](#connector-config)   | Files that describe how to build a data connector.                                         |
| `*.hml`                                   | Hasura metadata files for a project.                                                       |
| [`.hasura/context.yaml`](#context-file)   | File that stores certain default values of a project.                                      |
| `compose.yaml`                            | Docker Compose files for local development.                                                |
| [`engine`](#engine)                       | Files mounted to the engine container for local development.                               |
| [`globals`](#globals)                     | Directory containing files for the **globals** subgraph.                                   |
| `otel-collector-config.yaml`              | Configuration for OpenTelemetry Collector used for seeing traces during local development. |

#### hasura.yaml {#hasurayaml}

`hasura.yaml` appears at the root of a Hasura project.

For example:

```yaml
version: v2
```

- `version` specifies the version of the project directory structure.

#### .env.\* {#envfiles}

This file is used to store environment variables. The file should be in the format:

```.env
ENV1=val1
ENV2=val2
```

These files are referenced inside:

- Docker Compose files: Docker Compose files use \*.env files to specify environment variables needed for containers.
- Subgraph files: Subgraph files use \*.env files to specify environment variables. This is needed by `globals` and
  other subgraphs. Each subgraph can have its own .env file.
- Connector files (such as `connector.yaml`): Connector files use \*.env files to specify environment variables needed
  for building the connector.

#### supergraph.\*.yaml {#supergraph-config}

These config files tell Hasura DDN how to construct your supergraph. It will contain information such as which subgraph
config files to use for building the supergraph

For example:

```yaml
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.cloud.yaml
    - app/subgraph.yaml
```

- `version` is the version of the supergraph object
- `subgraphs` specifies paths to all the subgraph config files. These are then read recursively to construct the
  metadata for the supergraph.

By default, the CLI generates two files - `supergraph.cloud.yaml` and `supergraph.local.yaml` but any number of
supergraph config files can be created and referenced in CLI commands.

#### subgraph.\*.yaml {#subgraph-config}

These config files tell Hasura DDN how to construct your subgraph. It will contain information such as which metadata
resources to use for the build.

For example:

```yaml
kind: Subgraph
version: v1
definition:
  name: app
  generator:
    rootPath: .
  includePaths:
    - metadata
```

- `version` is the version of the subgraph object
- `includePaths` specifies the directories and files where metadata for the subgraph can be found. If a directory is
  specified, all the \*.hml files inside the directory and its subdirectories will be used to construct the metadata.
- `generator.rootPath` specifies the directory into which any new files will be generated.

By default, the CLI generates a file called `subgraph.yaml` for a new subgraph but any number of subgraph config files
can be created and referenced in CLI commands.

#### connector.\*.yaml {#connector-config}

These config files tell Hasura DDN how to build your connector. It will contain information such as the type of
connector and the location to the context files needed to build the connector.

For example:

```yaml
kind: Connector
version: v1
definition:
  name: mypg
  source: hasura/postgres:v0.7.0
  context: .
  envFile: .env.local
```

- `version` is the version of the connector object
- `name` is a name given to the connector
- `source` is the connector to use specific to the data source
- `context` specifies the connector directory
- `envFile` specifies the connector specific environment variables to use for introspecting and building the connector.
  If you are deploying your connector on DDN cloud, you also need to specify `subgraph`. Value of this field is name of
  the subgraph you want to deploy your connector to.

By default, the CLI generates two files - `connector.cloud.yaml` and `connector.local.yaml` but any number of connector
config files can be created and referenced in CLI commands.

#### .hasura/context.yaml {#context-file}

This specifies the default DDN project and supergraph file path. The default values are used by all commands that accept
`--supergraph` flag, `--subgraph` flag and `--project` flag. The flags can be used to override the default values.

```yaml
context:
  project: emerging-stag-9129
  supergraph: ../supergraph.cloud.yaml
  subgraph: ../app/subgraph.yaml
```

This file is configured by the `ddn context set` command.

#### Engine {#engine}

The `engine` directory contains the files required for Hasura v3 engine container. This directory has the following
structure:

```bash
â”œâ”€â”€ .env.engine
â”œâ”€â”€ auth_config.json
â”œâ”€â”€ metadata.json
â””â”€â”€ open_dd.json
```

The `.env.engine` file specifies environment variables required by the Hasura v3 engine container.

The `auth_config.json`, `metadata.json` and `open_dd.json` are generated as a result of `ddn supergraph build local`
command and do not need to be edited by the user.

#### Globals {#globals}

The `globals` directory contains the files for the **globals** subgraph which is generated by default to hold the
supergraph-level metadata objects, i.e. `AuthConfig`, `GraphqlConfig` and `CompatibilityConfig`.

For example:

```bash
â”œâ”€â”€ .env.globals.cloud
â”œâ”€â”€ .env.globals.local
â”œâ”€â”€ auth-config.cloud.hml
â”œâ”€â”€ auth-config.local.hml
â”œâ”€â”€ compatibility-config.hml
â”œâ”€â”€ graphql-config.hml
â”œâ”€â”€ subgraph.cloud.yaml
â””â”€â”€ subgraph.local.yaml
```

- `auth-config.cloud.hml` and `auth-config.local.hml` files contain the `AuthConfig` object which define the
  authentication configuration for the supergraph for cloud and local deployments respectively.
- `compatibility-config.hml` file contains the compatibility date configuration for the supergraph.
- `graphql-config.hml` file contains the GraphQL configuration for the supergraph, which allows you to customize the
  available query and mutation capabilities along with the schema.
- `.env.globals.cloud` and `.env.globals.local` files contain all the environment variables, if any, which are required
  by the **globals** subgraph's metadata objects for cloud and local deployments respectively.

</details>



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/upgrade-project-v3.mdx ---
# Upgrade to project config v3

---
sidebar_position: 3
sidebar_label: Upgrade to project config v3
description: "Guide to migrate local projects created by CLI versions < v2.0.0"
keywords:
  - hasura ddn
  - data delivery network
  - hasura cli
  - project configuration
  - hasura metadata management
  - yaml configuration
  - api development
  - hasura build profiles
  - hasura subgraphs
  - hasura project management
toc_max_heading_level: 4
---

import InstallTheCli from "@site/docs/_install-the-cli.mdx";

# Upgrading to project config v3

## What has happened?

A new revision (**v2.x.x**) of the [DDN CLI](/reference/cli/installation.mdx) has been released with some changes to the
project directory structure and CLI commands. With this release, local project directory structures created by CLI
versions **\< v2.0.0** are now deprecated.

## What has changed?

- The project config version defined in `hasura.yaml` is bumped from `v2` to `v3`.
- The `Connector` object is upgraded from `v1` to `v2`.
- The `Subgraph` object is upgraded from `v1` to `v2`.

### Named contexts

Named contexts are introduced. You can now create multiple contexts to easily switch between different deployment
scenarios. e.g.

```bash
ddn supergraph build create --context staging  # will use cloudEnvFile from staging context, e.g. .env.stg
ddn supergraph build create --context prod  # will use cloudEnvFile from prod context, e.g. .env.prod
```

### Top-level env files

To simplify the management of environment variables across connectors and subgraphs in your supergraph, the CLI now
defaults to using a common top-level `.env` file instead of generating separate `.env` files for each subgraph and
connector.

You can set environment files for local development and cloud builds using the `localEnvFile` and `cloudEnvFile` keys in
the context. **If you're using a shared supergraph config file for both local and cloud environments**, you can specify
both environment files in your context to manage local and cloud deployments within the same context.

```bash title="In project root with single top-level env files:"
ddn context set localEnvFile .env
ddn context set cloudEnvFile .env.cloud
```

You can then run supergraph build command for both local and DDN using the same context.

```bash
ddn supergraph build local  # will use localEnvFile from context, i.e. .env
ddn supergraph build create  # will use cloudEnvFile from context, i.e. .env.cloud
```

### Default naming convention for GraphQL types/fields

The default naming convention for GraphQL types and fields has changed. The CLI no longer adds the subgraph name as a
prefix to the generated GraphQL types and fields by default.

You can configure these yourself by [customizing prefixes](#step-42-add-metadata-generator-prefix).

### Subgraph and Connector config env mapping

The environment variables expected by a connector or subgraph now need to be explicitly defined in an `envMapping`
property in their config files.

The `envMapping` field defines how any provided environment variables are mapped to the environment variables expected
by the connector / subgraph.

### Simplified workflows using connector mapping

The mapping between connectors in the subgraph and their corresponding connector links can now be defined in the
subgraph config to simplify connector update and supergraph build operations.

- You can now introspect a connector and update its corresponding connector link using a single command.
- You can now create a supergraph build on Hasura DDN along with its dependent connector builds using a single command.

:::info Check out the quickstart

We recommend you go through our [new quickstart guide](/quickstart.mdx) to check the simplified workflow to develop your
API.

:::

### Connector build secure tokens

Any connector build created on Hasura DDN will now be secured with a token. The token needs to be sent as a Bearer token
in all requests that go to the connector build.

### Command behavior

The behavior, arguments and flags of certain CLI commands have a few changes. Some major changes are listed below:

- `ddn supergraph init`
  - Takes directory as an argument. `--dir` flag is deprecated.
  - Also creates a subgraph `app` in the supergraph by default.
  - Generates a top-level `.env` file for local development.
  - Sets the `supergraph`, `subgraph` and `localEnvFile` keys in the context.
- `ddn connector init`
  - Introduces an interactive flag (`-i`) to simplify choosing the connector type and providing any required env
    variables.
  - Writes the required env variables to a `target-env` file.
  - Also creates the corresponding [data connector link](/reference/metadata-reference/data-connector-links.mdx) for the
    connector by default.
- `ddn connector introspect`
  - Also updates the schema of the corresponding data connector link for the connector by default.
- `ddn connector build create`
  - the `region-env` flag has now been deprecated.
- `ddn supergraph build local`
  - the `subgraph-env` flag has now been deprecated.
  - now outputs the build artifacts to the `/engine/build` directory without the need for the `--output-dir` flag.
  - uses the `localEnvFile` from the context for the build by default.
- `ddn supergraph build create`
  - the `subgraph-env` flag has now been deprecated.
  - uses the `cloudEnvFile` from the context for the build by default.

:::info No changes to existing DDN projects

Note that this is a CLI-only change and does not impact Hasura DDN projects. You can continue using existing DDN
projects and builds and also be able to create new builds on them with both the new and old CLI revisions.

Though, we strongly recommend updating to the latest version of the CLI and project structure by following the steps
below.

:::

## Migrate an existing local project

Below are steps to convert a local project created with the DDN CLI versions `< 2.0.0` to the new project structure.

### Step 1: Get the new CLI

<InstallTheCli revision="v4" />

Also, update your Hasura VS Code extension to `v2.x.x` if not done automatically.

:::info Need the previous CLI revision?

Note that the above will overwrite your current DDN CLI revision. You can re-install the previous revision by replacing
`/ddn/cli/v4/` with `/ddn/cli/v3/` in the above download command.

:::

### Step 2: Run the codemod command

You can update your existing local project by running the following in your project directory:

```bash title="in project root directory, run:"
ddn codemod upgrade-project-config-v2-to-v3 --dir .
```

This command will guide you through a series of transformations to update your project to the latest format.

### Step 3: Verify the migration

Create a new local supergraph build using:

```bash
# set context with local supergraph config e.g. local
ddn context set-current-context local
ddn supergraph build local --output-dir engine
```

Run your local supergraph and verify the generated API is the same as what you had earlier.

Create a new supergraph build on Hasura DDN using:

```bash
# use context with cloud supergraph config e.g. cloud
ddn supergraph build create --context cloud
```

On build completion, the build version, API endpoint, and console URLs will be returned in the response.

You can now head to your project console using the console URL returned in the previous step and verify the API
generated with the above build is the same as what you had earlier.

### Step 4: Make optional changes

#### Step 4.1: Update engine build dir

As mentioned above, the `ddn supergraph build local` now outputs the build artifacts to the `/engine/build` directory
without the need for the `--output-dir` flag. To avoid having to provide the flag in the future, you can move your build
assets to this directory and update the engine compose file to use this directory instead.

##### Step 4.1.1: Move engine build assets

```bash title="in project root directory, run:"
mkdir engine/build
mv engine/*.json engine/build
```

##### Step 4.1.2: Update engine compose file

```yaml title="e.g. compose.yaml"
services:
  ...
  engine:
    ...
    build:
      context: engine
      dockerfile_inline: |-
        FROM ghcr.io/hasura/v3-engine
        #highlight-start
        # update to the following
        COPY ./build /md/
        #highlight-end
    develop:
      watch:
        - action: sync+restart
          #highlight-start
          # update to the following
          path: engine/build
          #highlight-end
          target: /md/
    env_file:
      - engine/.env.engine
    ...
  ...
```

As the engine build directory contains your local build assets, it can be ignored from you version control. You can
choose to do so by adding it to a `.gitignore` file.

#### Step 4.2: Add metadata generator prefix

As the CLI now doesn't add the subgraph name as a prefix to the generated GraphQL types and fields by default. You can
keep the earlier behavior by updating the generator config in your Subgraph configs.

```yaml title="For example, my_subgraph/subgraph.yaml"
kind: Subgraph
version: v2
definition:
  #highlight-start
  name: my_subgraph
  #highlight-end
  generator:
    rootPath: .
    #highlight-start
    graphqlRootFieldPrefix: my_subgraph_
    graphqlTypeNamePrefix: My_subgraph_
    #highlight-end
  envFile: .env.my_subgraph.local
  ...
```

## Things to remember

### Adding new connectors

While adding new connectors, remember to:

- Add the required env vars for the connector to all env files e.g. `.env.cloud`. By default, it will be added to only
  the env file set as localEnvFile in the current context, e.g. `.env` or the one provided via flag.
- Add the connectorMapping and envMapping for the new connector URLs and Authorization headers to all subgraph config
  files. By default, it will be added to only the subgraph config set in the current context or provided via flag.

## Need help?

If you need help migrating your project or have any other questions please reach out to us on our
[Discord](https://hasura.io/discord).

## Legacy project structure

<details>
    <summary>
        See the legacy project structure before this update below.
    </summary>

```bash
.
â”œâ”€â”€ .devcontainer
â”‚  â””â”€â”€ devcontainer.json
â”œâ”€â”€ .hasura
â”‚  â””â”€â”€ context.yaml
â”œâ”€â”€ .vscode
â”‚  â”œâ”€â”€ extensions.json
â”‚  â”œâ”€â”€ launch.json
â”‚  â””â”€â”€ tasks.json
â”œâ”€â”€ engine
â”‚  â”œâ”€â”€ .env.engine
â”‚  â”œâ”€â”€ auth_config.json
â”‚  â”œâ”€â”€ metadata.json
â”‚  â””â”€â”€ open_dd.json
â”œâ”€â”€ globals
â”‚  â”œâ”€â”€ .env.globals.cloud
â”‚  â”œâ”€â”€ .env.globals.local
â”‚  â”œâ”€â”€ subgraph.cloud.yaml
â”‚  â”œâ”€â”€ subgraph.local.yaml
â”‚  â”œâ”€â”€ auth-config.cloud.hml
â”‚  â”œâ”€â”€ auth-config.local.hml
â”‚  â”œâ”€â”€ compatibility-config.hml
â”‚  â””â”€â”€ graphql-config.hml
â”œâ”€â”€ app
â”‚  â”œâ”€â”€ .env.app.cloud
â”‚  â”œâ”€â”€ .env.app.local
â”‚  â””â”€â”€ subgraph.yaml
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ compose.yaml
â”œâ”€â”€ hasura.yaml
â”œâ”€â”€ otel-collector-config.yaml
â”œâ”€â”€ supergraph.cloud.yaml
â””â”€â”€ supergraph.local.yaml
```

| File type                                 | Description                                                                                |
| ----------------------------------------- | ------------------------------------------------------------------------------------------ |
| [`hasura.yaml`](#hasurayaml)              | The file that denotes the root of a Hasura project.                                        |
| [`.env.*`](#envfiles)                     | Files that store environment variables.                                                    |
| [`supergraph.*.yaml`](#supergraph-config) | Files that describe how to build the supergraph.                                           |
| [`subgraph.*.yaml`](#subgraph-config)     | Files that describe how to build a subgraph.                                               |
| [`connector.*.yaml`](#connector-config)   | Files that describe how to build a data connector.                                         |
| `*.hml`                                   | Hasura metadata files for a project.                                                       |
| [`.hasura/context.yaml`](#context-file)   | File that stores certain default values of a project.                                      |
| `compose.yaml`                            | Docker Compose files for local development.                                                |
| [`engine`](#engine)                       | Files mounted to the engine container for local development.                               |
| [`globals`](#globals)                     | Directory containing files for the **globals** subgraph.                                   |
| `otel-collector-config.yaml`              | Configuration for OpenTelemetry Collector used for seeing traces during local development. |

##### hasura.yaml {#hasurayaml}

`hasura.yaml` appears at the root of a Hasura project.

For example:

```yaml
version: v2
```

- `version` specifies the version of the project directory structure.

##### .env.\* {#envfiles}

This file is used to store environment variables. The file should be in the format:

```.env
ENV1=val1
ENV2=val2
```

These files are referenced inside:

- Docker Compose files: Docker Compose files use \*.env files to specify environment variables needed for containers.
- Subgraph files: Subgraph files use \*.env files to specify environment variables. This is needed by `globals` and
  other subgraphs. Each subgraph can have its own .env file.
- Connector files (such as `connector.yaml`): Connector files use \*.env files to specify environment variables needed
  for building the connector.

##### supergraph.\*.yaml {#supergraph-config}

These config files tell Hasura DDN how to construct your supergraph. It will contain information such as which subgraph
config files to use for building the supergraph

For example:

```yaml
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.cloud.yaml
    - app/subgraph.yaml
```

- `version` is the version of the supergraph object
- `subgraphs` specifies paths to all the subgraph config files. These are then read recursively to construct the
  metadata for the supergraph.

By default, the CLI generates two files - `supergraph.cloud.yaml` and `supergraph.local.yaml` but any number of
supergraph config files can be created and referenced in CLI commands.

##### subgraph.\*.yaml {#subgraph-config}

These config files tell Hasura DDN how to construct your subgraph. It will contain information such as which metadata
resources to use for the build.

For example:

```yaml
kind: Subgraph
version: v1
definition:
  name: app
  generator:
    rootPath: .
  includePaths:
    - metadata
```

- `version` is the version of the subgraph object
- `includePaths` specifies the directories and files where metadata for the subgraph can be found. If a directory is
  specified, all the \*.hml files inside the directory and its subdirectories will be used to construct the metadata.
- `generator.rootPath` specifies the directory into which any new files will be generated.

By default, the CLI generates a file called `subgraph.yaml` for a new subgraph but any number of subgraph config files
can be created and referenced in CLI commands.

##### connector.\*.yaml {#connector-config}

These config files tell Hasura DDN how to build your connector. It will contain information such as the type of
connector and the location to the context files needed to build the connector.

For example:

```yaml
kind: Connector
version: v1
definition:
  name: mypg
  source: hasura/postgres:v0.7.0
  context: .
  envFile: .env.local
```

- `version` is the version of the connector object
- `name` is a name given to the connector
- `source` is the connector to use specific to the data source
- `context` specifies the connector directory
- `envFile` specifies the connector specific environment variables to use for introspecting and building the connector.
  If you are deploying your connector on DDN cloud, you also need to specify `subgraph`. Value of this field is name of
  the subgraph you want to deploy your connector to.

By default, the CLI generates two files - `connector.cloud.yaml` and `connector.local.yaml` but any number of connector
config files can be created and referenced in CLI commands.

##### .hasura/context.yaml {#context-file}

This specifies the default DDN project and supergraph file path. The default values are used by all commands that accept
`--supergraph` flag, `--subgraph` flag and `--project` flag. The flags can be used to override the default values.

```yaml
context:
  project: emerging-stag-9129
  supergraph: ../supergraph.cloud.yaml
  subgraph: ../app/subgraph.yaml
```

This file is configured by the `ddn context set` command.

##### Engine {#engine}

The `engine` directory contains the files required for Hasura v3 engine container. This directory has the following
structure:

```bash
â”œâ”€â”€ .env.engine
â”œâ”€â”€ auth_config.json
â”œâ”€â”€ metadata.json
â””â”€â”€ open_dd.json
```

The `.env.engine` file specifies environment variables required by the Hasura v3 engine container.

The `auth_config.json`, `metadata.json` and `open_dd.json` are generated as a result of `ddn supergraph build local`
command and do not need to be edited by the user.

##### Globals {#globals}

The `globals` directory contains the files for the **globals** subgraph which is generated by default to hold the
supergraph-level metadata objects, i.e. `AuthConfig`, `GraphqlConfig` and `CompatibilityConfig`.

For example:

```bash
â”œâ”€â”€ .env.globals.cloud
â”œâ”€â”€ .env.globals.local
â”œâ”€â”€ auth-config.cloud.hml
â”œâ”€â”€ auth-config.local.hml
â”œâ”€â”€ compatibility-config.hml
â”œâ”€â”€ graphql-config.hml
â”œâ”€â”€ subgraph.cloud.yaml
â””â”€â”€ subgraph.local.yaml
```

- `auth-config.cloud.hml` and `auth-config.local.hml` files contain the `AuthConfig` object which define the
  authentication configuration for the supergraph for cloud and local deployments respectively.
- `compatibility-config.hml` file contains the compatibility date configuration for the supergraph.
- `graphql-config.hml` file contains the GraphQL configuration for the supergraph, which allows you to customize the
  available query and mutation capabilities along with the schema.
- `.env.globals.cloud` and `.env.globals.local` files contain all the environment variables, if any, which are required
  by the **globals** subgraph's metadata objects for cloud and local deployments respectively.

</details>



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/upgrade-context-v3.mdx ---
# Upgrade to context config v3

---
sidebar_position: 4
sidebar_label: Upgrade to context config v3
description: "Guide to migrate context config created by CLI versions < v2.4.0"
keywords:
  - hasura ddn
  - data delivery network
  - hasura cli
  - project configuration
  - hasura metadata management
  - yaml configuration
  - api development
  - hasura build profiles
  - hasura subgraphs
  - hasura project management
seoFrontMatterUpdated: true
toc_max_heading_level: 4
---

# Upgrading to context config v3

## What has happened?

A new revision (**v2.5.0**) of the [DDN CLI](/reference/cli/installation.mdx) has been released with enhancements to the
context config. This update introduces a powerful new scripting feature, allowing users to define and execute custom
scripts directly from their context configuration.

## What has changed?

- The project's context config is bumped from `v2` to `v3`
- A new scripts field has been introduced in the context configuration.
- The CLI now supports running custom scripts with environment detection.

### Scripting Feature

The new scripting feature allows you to define custom scripts in your context configuration. These scripts can be
executed with `ddn run` command, and the CLI will automatically detect your environment to run the appropriate version
of the script. Here's an example of the new context configuration structure:

```yaml
kind: Context
version: v3
definition:
  current: default
  contexts:
    default:
      supergraph: ../supergraph.yaml
      subgraph: ../app/subgraph.yaml
      localEnvFile: ../.env
  scripts:
    docker-start:
      bash: HASURA_DDN_PAT=$(ddn auth print-access-token) docker compose --env-file .env up --build --pull always -d
      powershell: $Env:HASURA_DDN_PAT = (ddn auth print-access-token); docker compose --env-file .env up --build --pull always -d
```

#### Running Scripts

To run a script, use the command:

```bash
ddn run docker-start
```

#### Multi-Platform Support

You can define scripts for bash and PowerShell under a single script name. DDN CLI will automatically select the correct
script version based on your environment.

#### Custom Scripts

You're free to create your own scripts tailored to your workflow needs. This allows you to encapsulate complex command
sequences into simple, memorable script names.

:::info Paths in scripts are relative to your project's root path. :::

### Backwards Compatibility

Please note that context configuration v2 is still fully supported. While we encourage you to upgrade to v3 to take
advantage of the new scripting features, you can continue to use v2 configurations without any issues.

## Migrate an existing project

To start using this new feature in an existing project:

1. Update your CLI to the latest version.

```bash
ddn update-cli
```

2. Run codemod from Hasura project directory to update your project context configuration to v3.

```bash
ddn codemod upgrade-context-v2-to-v3 --dir .
```

3. By default, we add a `docker-start` script for you so that you can just run `ddn run docker-start` to get the local
   local supergraph and connectors running using Docker

## Creating and running a custom script

Add this under your context configuration's script field:

```yaml
my-custom-script:
  bash: echo "Hello from bash!"
  powershell: Write-Host "Hello from PowerShell!"
```

Then run it using:

```bash
ddn run my-custom-script
```

## Need help?

If you need help migrating your project or have any other questions please reach out to us on our
[Discord](https://hasura.io/discord).

## Legacy project structure

<details>
    <summary>
        See the legacy context config structure before this update below.
    </summary>

```
kind: Context
version: v2
definition:
  current: default
  contexts:
    default:
      project: emerging-stag-9129
      supergraph: ../supergraph.yaml
      subgraph: ../app/subgraph.yaml
      localEnvFile: ../.env
      cloudEnvFile: ../.env.cloud
```

</details>



--- File: ../ddn-docs/docs/project-configuration/upgrading-project-config/fix-traces-env-var.mdx ---
# Fix env var for OTLP endpoint

---
sidebar_position: 5
sidebar_label: Fix env var for OTLP endpoint
description: "Fix the name of the environment variable used to configure OTLP endpoint"
keywords:
  - hasura ddn
  - data delivery network
  - hasura cli
  - project configuration
  - hasura metadata management
  - yaml configuration
  - api development
  - hasura build profiles
  - hasura subgraphs
  - hasura project management
seoFrontMatterUpdated: true
toc_max_heading_level: 4
---

# Fix the OTLP endpoint environment variable

## What has happened?

In DDN CLI versions **v2.7.0** and below, the environment variable `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`
was used to configure the endpoint for sending traces from connectors. However, this environment variable
is not supported by all Hasura connectors. Starting with CLI v2.7.1, the environment variable 
`OTEL_EXPORTER_OTLP_ENDPOINT` is now used, which is supported by all Hasura connectors.

## What has changed?

When DDN CLI initializes a connector using `ddn connector init` the environment variable `OTEL_EXPORTER_OTLP_ENDPOINT`
is used instead of `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` in `connector.yaml` and `compose.yaml` files.

## Migrate an existing project
To start using this new feature in an existing project:

1. Update your CLI to the latest version.
```bash
ddn update-cli
```
2. Run codemod from Hasura project directory to update the name of the environment variables
```bash
ddn codemod fix-traces-env-var --dir .
```

This will fix the name of the environment variables in the connector configuration files and docker compose files.

## Verifying the changes

To verify that the traces are working correctly,

1. Run the local engine using `ddn run docker-start`
2. Open local console using `ddn console --local`
3. Execute a GraphQL query
4. Click on 'View Trace' and verify that the traces from the connector are also shown

## Need help?

If you need help migrating your project or have any other questions please reach out to us on our
[Discord](https://hasura.io/discord).



--- File: ../ddn-docs/docs/recipes/overview.mdx ---
# Recipes

---
title: Recipes
sidebar_position: 1
description: "Learn about Hasura's powerful plugins architecture."
sidebar_label: Overview
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
hide_table_of_contents: true
seoFrontMatterUpdated: false
---

import { OverviewPlainCard } from "../../src/components/OverviewPlainCard";
import { OverviewTopSectionIconNoVideo } from "@site/src/components/OverviewTopSectionIconNoVideo";
import Icon from "@site/static/icons/beaker.svg";

# Recipes

## Introduction

There's loads of ways you can customize your Hasura DDN supergraph. In this section, you'll find specific recipes that
will aid you in solving problems succinctly. Whether you're looking for how to wire up a particular auth provider,
connect to an LLM, or export your own o11y metrics â€”Â this section has you covered.

## Find out more

- [Permissions](/auth/permissions/index.mdx)
- [Project configuration](/recipes/project-config/index.mdx)



--- File: ../ddn-docs/docs/recipes/project-config/rename-subgraph.mdx ---
# Rename a subgraph

---
sidebar_position: 1
sidebar_label: Rename a subgraph
description: "Learn how to rename a subgraph in your local project directory"
keywords:
  - hasura
  - hasura ddn
  - recipe
  - guide
  - project configuration
  - rename
  - subgraph
seoFrontMatterUpdated: false
---

# Rename a subgraph

## Introduction

In this recipe, you'll learn how to rename a subgraph in your local project directory.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- Stopped any running docker services related to the project.

:::

## Recipe

### Step 1. Update subgraph config files

Update the name of the subgraph in all the [subgraph config files](/project/configuration/overview.mdxthe subgraph. The
subgraph config is typically located at `<subgraph-name>/subgraph.yaml`.

```yaml title="<subgraph-name>/subgraph.yaml"
kind: Subgraph
version: v2
definition:
  #highlight-start
  name: <new-subgraph-name>
  #highlight-end
  generator:
    rootPath: .
  includePaths:
    - metadata
  envMapping: ...
```

### Step 2. Update connector config files of the subgraph

Update the reference of the subgraph in all the [connector config files](/project/configuration/overview.mdxsubgraph.
The connector config is typically located at `<subgraph-name>/connector/<connector-name>/connector.yaml`.

```yaml title="<subgraph-name>/connector/<connector-name>/connector.yaml"
kind: Connector
version: v2
definition:
  name: <connector-name>
  #highlight-start
  subgraph: <new-subgraph-name>
  #highlight-end
  source: hasura/<connector-type>
  context: .
  envMapping: ...
```

### Step 3. Update compose files of connectors of the subgraph

Update the name of the connector service in the [compose file](/project/configuration/overview.mdxthe connectors
belonging to the subgraph. The connector compose file is typically located at
`<project-root>/<subgraph-name>/connector/<connector-name>/compose.yaml`.

```yaml title="<subgraph-name>/connector/<connector-name>/compose.yaml"
services:
  #highlight-start
  <new-subgraph-name>_<connector-name>:
    #highlight-end
    build:
      context: .
      dockerfile_inline: ...
```

### Step 4. (Optional) Update subgraph directory name

The subgraph directory is typically named as the subgraph name itself. The directory name does not impact any
functionality, but you might want to rename it to maintain consistency. The subgraph directory is typically located at
the project root, i.e., `<project-root>/<subgraph-name>/`.

Rename the directory to `<project-root>/<new-subgraph-name>/`.

Renaming the subgraph directory name will require updates to any references to files within the directory. These should
typically be in:

#### Step 4.1. Supergraph config files

Update the path to the subgraph config files in all [supergraph config files](/project/configuration/overview.mdxlocated
at the project root, i.e., `<project-root>/supergraph.yaml`.

```yaml title="supergraph.yaml"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    #highlight-start
    - <new-subgraph-name>/subgraph.yaml
    #highlight-end
    ...
```

#### Step 4.2. Engine compose file

Update the path to the connector compose files included in the engine [compose
file](/project/configuration/overview.mdxproject root, i.e., `<project-root>/compose.yaml`.

```yaml title="compose.yaml"
include:
  #highlight-start
  - path: <new-subgraph-name>/connector/<connector-1>/compose.yaml
  - path: <new-subgraph-name>/connector/<connector-2>/compose.yaml
  #highlight-end
  ...
services:
  engine:
    build:
    ...
```

#### Step 4.3. Context config

The [context config file](/project/configuration/overview.mdxsubgraph config file path saved in the context. Update the
subgraph config path if set.

```yaml title=".hasura/context.yaml"
kind: Context
version: v3
definition:
  current: default
  contexts:
    default:
      supergraph: ../supergraph.yaml
      #highlight-start
      subgraph: ../<new-subgraph-name>/subgraph.yaml
      #highlight-end
      ...
```

## Learn more

- [Project configuration](/project/configuration/overview.mdx



--- File: ../ddn-docs/docs/recipes/project-config/remove-subgraph.mdx ---
# Remove a subgraph

---
sidebar_position: 2
sidebar_label: Remove a subgraph
description: "Learn how to remove a subgraph from your local project directory"
keywords:
  - hasura
  - hasura ddn
  - recipe
  - guide
  - project configuration
  - remove
  - subgraph
seoFrontMatterUpdated: false
---

# Remove a subgraph

## Introduction

In this recipe, you'll learn how to remove a subgraph from your local project directory.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).
- Stopped any running docker services related to the project.

:::

## Recipe

### Step 1. Delete subgraph directory

Delete the directory containing the subgraph related config files, connectors and metadata of the subgraph. The subgraph
directory is typically located at `<subgraph-name>`.

### Step 2. Update supergraph config files

Remove the path to the subgraph config files in all [supergraph config files](/project/configuration/overview.mdxlocated
at the project root, i.e., `<project-root>/supergraph.yaml`.

```yaml title="supergraph.yaml"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    #highlight-start
    - <subgraph-name>/subgraph.yaml
    #highlight-end
    ...
```

### Step 3. Update engine compose file

Remove references to any compose files of connectors in the deleted subgraph from the engine compose file. The engine
compose file is typically located at `<project-root>/compose.yaml`.

```yaml title="<project-root>/compose.yaml"
include:
  #highlight-start
  - path: <subgraph-name>/connector/<connector-1>/compose.yaml
  - path: <subgraph-name>/connector/<connector-2>/compose.yaml
  #highlight-end
  ...
services:
  engine: ...
```

### Step 4. Remove subgraph config file from context

The [context config file](/project/configuration/overview.mdxsubgraph config file path saved in the context. Remove the
`subgraph` key if set as the deleted subgraph config file.

```yaml title=".hasura/context.yaml"
kind: Context
version: v3
definition:
  current: default
  contexts:
    default:
      supergraph: ../supergraph.yaml
      #highlight-start
      subgraph: ../<subgraph-name>/subgraph.yaml
      #highlight-end
      ...
```

### Step 5. (Optional) Remove subgraph relevant environment variables

You can remove the environment variables that were defined for your subgraph from the env files that you might have. The
CLI-generated environment variables for a subgraph typically start with the `<SUBGRAPH_NAME>_` prefix.

```.env title="For example, .env"
...
#highlight-start
<SUBGRAPH_NAME>_<CONNECTOR>_READ_URL="<connector-read-url>"
<SUBGRAPH_NAME>_<CONNECTOR>_WRITE_URL="<connector-write-url>"
<SUBGRAPH_NAME>_<CONNECTOR>_AUTHORIZATION_HEADER="Bearer <roken>"
#highlight-end
...
```

## Learn more

- [Project configuration](/project/configuration/overview.mdx



--- File: ../ddn-docs/docs/recipes/project-config/index.mdx ---
# Project configuration

---
sidebar_position: 3
sidebar_label: Project configuration
description: "Recipes on making common updates in the local project configuration."
keywords:
  - hasura
  - hasura ddn
  - recipe
  - guide
  - project configuration
seoFrontMatterUpdated: false
---

# Project configuration

## Introduction

In this section of recipes, we'll provide step-by-step guides for common patterns in managing your local project
configuration.

If you're unfamiliar with the local project configuration, [check out the
docs](/project/configuration/overview.mdxbefore diving deeper into one of these recipes.

## Recipes

- [Rename a subgraph](/recipes/project-config/rename-subgraph.mdx)



--- File: ../ddn-docs/docs/recipes/project-config/independent-connector-deployment.mdx ---
# Deploy connectors independently

---
sidebar_position: 4
sidebar_label: Deploy connectors independently
description: "Learn how to manage independent deployments for connectors and the supergraph"
keywords:
  - hasura
  - hasura ddn
  - recipe
  - guide
  - project configuration
  - connector
  - deployment
  - independent
seoFrontMatterUpdated: false
---

# Deploy connectors and supergraph independently

## Introduction

By default, the CLI command to build the supergraph, i.e.
[ddn supergraph build create](/reference/cli/commands/ddn_supergraph_build_create.mdx), also builds the connectors
required by the supergraph for convenience. Though in certain cases, you might want to do these steps independently,
e.g. when the connectors are self-hosted and not deployed on Hasura DDN.

In this recipe, you'll learn how to manage independent deployments for your connectors and the supergraph.

:::info Prerequisites

Before continuing, ensure you have:

- A [local Hasura DDN project](/quickstart.mdx).

:::

## Recipe

### Basics

The supergraph interacts with a connector via the
[DataConnectorLink](/reference/metadata-reference/data-connector-links.mdx) metadata object which contains the NDC
schema of the connector along with the URLs where the connector is running.

Hence, to independently manage the connector and supergraph deployments, we need to ensure that the corresponding
DataConnectorLink object is kept in sync with the connector.

### Step 1. Build the connector

Build and deploy your connector and note the:

- Read url, say `<deployed-connector-read-url>`
- Write url, say `<deployed-connector-write-url>`
- Authorization header, say `Bearer <deployed-connector-token>`

For example, to build the connector on DDN:

```bash title="Build connector on DDN using config at <subgraph-name>/connector/<connector-name>/connector.yaml"
ddn connector build create --connector <subgraph-name>/connector/<connector-name>/connector.yaml \
  --env-file .env.cloud \
  --project <project-name>
```

:::info

The `--project` and `--env-file` flags can be skipped if the keys `project` and `cloudEnvFile` are set in the context.

:::

### Step 2. Update connector URLs and Auth header in the data connector link

Update the environment variables that correspond to the connector read, write URLs and the Authorization header in the
data connector link:

```yaml title="For example: <subgraph-name>/metadata/<connector-link-name>.hml"
kind: DataConnectorLink
version: v1
definition:
  name: <connector-link-name>
  #highlight-start
  url:
    readWriteUrls:
      read:
        valueFromEnv: <CONNECTOR>_READ_URL
      write:
        valueFromEnv: <CONNECTOR>_WRITE_URL
  headers:
    Authorization:
      valueFromEnv: <CONNECTOR>_AUTHORIZATION_HEADER
  #highlight-end
  schema: ...
```

Update the environment variables in the corresponding env file:

```env title="For example: .env.cloud"
...
#highlight-start
<CONNECTOR>_READ_URL="<deployed-connector-read-url>"
<CONNECTOR>_WRITE_URL="<deployed-connector-write-url>"
<CONNECTOR>_AUTHORIZATION_HEADER="Bearer <deployed-connector-token>"
#highlight-end
...
```

### Step 3. Build the supergraph without connectors

Build the supergraph to get a supergraph build using the connector deployed above:

```bash title="Create supergraph build on DDN without building the related connectors:"
#highlight-start
ddn supergraph build create --no-build-connectors \
#highlight-end
  --supergraph supergraph.yaml \
  --env-file .env.cloud \
  --project <project-name>
```

:::info

The `--project`, `--supergraph` and `--env-file` flags can be skipped if the keys `project`, `supergraph` and
`cloudEnvFile` are set in the context.

:::

### Step 4. (Optional) Add a custom script to build the supergraph

You can add the above command as a [custom script](/project/configuration/overview.mdxthe `--no-build-connectors` flag
each time.

For example, you can update your context config to the following:

```yaml title="<project-root>/.hasura/context.yaml:"
kind: Context
version: v3
definition:
  current: default
  contexts:
    default: ...
  scripts:
    docker-start:
      bash: HASURA_DDN_PAT=$(ddn auth print-access-token) docker compose --env-file .env up --build --pull always -d
      powershell: $Env:HASURA_DDN_PAT = (ddn auth print-access-token); docker compose --env-file .env up --build --pull always -d
    #highlight-start
    build-supergraph:
      bash:
        ddn supergraph build create --no-build-connectors --supergraph supergraph.yaml --env-file .env.cloud --project
        <project-name>
      powershell:
        ddn supergraph build create --no-build-connectors --supergraph supergraph.yaml --env-file .env.cloud --project
        <project-name>
    #highlight-end
```

Now you can run the following command to build the supergraph:

```bash
ddn run build-supergraph
```

:::info

The `--project`, `--supergraph` and `--env-file` flags can be skipped if the keys `project`, `supergraph` and
`cloudEnvFile` are set in the context.

:::

## Learn more

- [Project configuration](/project/configuration/overview.mdx



--- File: ../ddn-docs/docs/observability/overview.mdx ---
# Basics

---
sidebar_position: 1
sidebar_label: Basics
description:
  "Explore Hasura's observability tools that optimize the monitoring and debugging of your GraphQL API. Learn about
  performance check, error debugging, and gaining insights into the use of your GraphQL API for increased productivity
  and efficiency."
keywords:
  - hasura ddn
  - graphql api
  - api observability
  - api monitoring
  - api debugging
  - api performance
  - api metrics
  - graphql metrics
  - api tracing
  - hasura tools
seoFrontMatterUpdated: true
---

# Basics

## Built-in features

Hasura DDN prevents the need of adding additional observability tools to your application stack. Instead, you can use
our suite of built-in tools to monitor your API and diagnose any issues along your data pipeline.

## Find out more

- [Traces](/observability/built-in/traces.mdx)
- [Metrics](/observability/built-in/metrics.mdx)
- [Analytics](/observability/built-in/model-analytics/index.mdx)
- [Query plan](/observability/built-in/explain/index.mdx)



--- File: ../ddn-docs/docs/observability/built-in/index.mdx ---
# ../ddn-docs/docs/observability/built-in/index.mdx

---
sidebar_position: 1
description: ""
keywords:
  - hasura ddn
  - api observability
  - metrics
  - traces
  - logs
  - tools
---

# Built-in Features



--- File: ../ddn-docs/docs/observability/built-in/traces.mdx ---
# Traces

---
sidebar_position: 1
sidebar_label: Traces
description:
  "Learn how to leverage OpenTelemetry traces to track user requests across various services in the Hasura Data Delivery
  Network. Understand how to diagnose and debug potential failure points and monitor performance for better application
  management."
keywords:
  - hasura ddn
  - traces
  - tracing
  - graphql api
  - opentelemetry
  - application debugging
  - application performance
  - user request tracking
  - api monitoring
  - application management
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Traces

## Introduction

[Query traces](https://opentelemetry.io/docs/concepts/signals/traces/) track and map journeys of user requests across
various services or components. You can use these to quickly and easily view a summary for a query, see how much time it
took to complete, the number of models that were used, commands which were accessed, and more.

Traces are typically used to diagnose or debug which part of your application could potentially be responsible for a
failure or error state and to monitor the performance of end-user interactions with your application. You can do this by
seeing how the query traverses through the various components of your supergraph and identify or debug potential failure
points.

Traces are generated by instrumenting application code. Hasura DDN instruments all API queries with the OpenTelemetry
format and supports tracing out of the box. **This means that you don't have to set up your own OpenTelemetry
collector.** Simply head to the `Query Tracing` tab from `Insights` panel for a project's Console, and you'll see traces
for all your API requests.

:::info Accessing traces

You can access each query's trace using the toast in the bottom-right corner of the Console and instantly debug any
potential issues.

<Thumbnail src="/img/o11y/tracing.gif" alt="Execute a query" width="1000px" />

Additionally, under the `Insights` panel, you can find the `Query Traces` tab to view all traces for your supergraph.

<Thumbnail src="/img/o11y/v0.1.1._console_query-traces-queries.png" alt="Execute a query" width="1000px" />

:::

## Topology

The topology of a query trace is a visual representation of the journey of a user request across various services or
components. Each step in the journey is represented as a node in the topology, and the edges between the nodes represent
the relationship between the steps. You can click on each node to view more details about the step.

<Thumbnail src="/img/o11y/v0.1.1_console_topology.png" alt="Execute a query" width="1000px" />

In the example above, you can see the trace begins with the client and moves its way through the various regions before
arriving at the database.

## Spans

Spans represent the basic unit of work in a distributed system. They describe the operation that an individual component
of the system is doing, such as the time taken to execute a function or a request.

<Thumbnail src="/img/o11y/v0.1.1_console_spans.png" alt="Execute a query" width="1000px" />

As we can see from the screenshot, there are two major spans:

| Span name      | Description                                                                                    |
| -------------- | ---------------------------------------------------------------------------------------------- |
| authentication | The span responsible for handling authentication before the request is executed by the engine. |
| query          | The span responsible for executing the GraphQL query.                                          |

These spans can be expanded to view more details about the operation, such as the time taken to execute the operation,
the status of the operation, and more.

Every request to your GraphQL API will have a unique `trace-id` associated with it and may have the following spans:

- `/graphql`
  - `handle_authentication`
    - `execute_authentication`
  - `handle_request`
    - `execute_query`
      - `execute`
        - `execute_ndc_query`
          - `Execute Query`
            - `<Connector spans>`
        - `execute_ndc_mutation`
          - `<Connector spans>`

| Span name                | Description                                                                                                                                                                                 |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `/graphql`               | The root span for every request to your GraphQL API.                                                                                                                                        |
| `handle_authentication`  | The span responsible for handling authentication before the request is executed by the engine.                                                                                              |
| `execute_authentication` | The span responsible for executing authentication, either using JWTs or via a Webhook.                                                                                                      |
| `handle_request`         | The span responsible for handling a GraphQL request after it is successfully authenticated by the engine.                                                                                   |
| `execute_query`          | The top-level span for executing a GraphQL query.                                                                                                                                           |
| `execute`                | The span responsible for executing each query in your GraphQL request. This span has an attribute `usage_counts` which describes all the models that were used as a part of this operation. |
| `execute_ndc_query`      | The span responsible for executing a single ndc query. This span has an attribute `field` which describes the schema type that was executed.                                                |
| `Execute Query`          | The span responsible for executing the generated query on the data source.                                                                                                                  |
| `execute_ndc_mutation`   | The span responsible for executing a single ndc mutation. This span has an attribute `field` which describes the field that was executed.                                                   |
| `<Connector spans>`      | Optional spans from connectors to which the engine sends requests.                                                                                                                          |

### Connector spans

Connectors can have internal spans that vary depending on the data source or business logic implemented. Those traces
are exported separately from the connector and are handled automatically if the connector is deployed to DDN cloud.
However, self-hosted connectors need to configure the variables below or connector traces will not be visible on the DDN
console:

- `OTEL_EXPORTER_OTLP_ENDPOINT`: `https://gateway.otlp.hasura.io:443`
- `OTEL_EXPORTER_OTLP_HEADERS`: `Authorization=pat <personal-access-token>`

The trace ID is propagated through [B3 (Zipkin) headers][B3 propagation], and must be extracted accordingly to ensure
the spans are connected. If you are using a connector SDK, this should be handled for you automatically.

[B3 propagation]: https://github.com/openzipkin/b3-propagation

:::info Personal Access Token (PAT)

You can generate a personal access token [here](https://cloud.hasura.io/account-settings/access-tokens).

:::



--- File: ../ddn-docs/docs/observability/built-in/metrics.mdx ---
# Metrics

---
sidebar_position: 2
sidebar_label: Metrics
description:
  "Learn how to leverage metrics to track user requests across various services in the Hasura Data Delivery Network.
  Understand how to diagnose and debug potential failure points and monitor performance for better application
  management."
keywords:
  - hasura ddn
  - metrics
  - performance
  - latency
  - request rate
  - error rate
  - graphql api
  - opentelemetry
  - application debugging
  - application performance
  - api monitoring
  - application management
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Metrics

## Introduction

Hasura DDN ships with a set of metrics that help you monitor and debug performance of your GraphQL API. Simply head to
the `Performance` tab from `Insights` panel for a project's console, and you'll see performance metrics for all your API
requests.

From the performance page, metrics can be viewed for either an entire project or a single build.

<Thumbnail src="/img/o11y/v0.1.1_console_performance-metrics.png" alt="Execute a query" width="1000px" />

## Time-series metrics

Time-series metrics are a set of metrics that are collected over time. These metrics are useful for tracking the health
of your GraphQL API and diagnosing potential failure points. **These metrics can be filtered one day at a time and are
retained for 30 days.**

### Request Rates

The number of requests to your GraphQL API per minute.

### Request Latency

Request latency is the time taken to execute a request to your GraphQL API. The P99 (99th percentile) metric indicates
that 99% of requests are completed within the recorded time whereas the P95 (95th percentile) metric indicates that 95%
of requests are completed within the recorded time.

### Error Rate

The percentage of requests to your GraphQL API that resulted in an error.



--- File: ../ddn-docs/docs/observability/built-in/platform-dashboard.mdx ---
# Platform Dashboard

---
sidebar_position: 5
sidebar_label: Platform Dashboard
description:
  "The Platform Dashboard is a comprehensive reporting tool designed to provide insights into the performance, security,
  and usage of the Data Delivery Netowork"
keywords:
  - hasura ddn
  - metrics
  - performance
  - latency
  - request rate
  - error rate
  - graphql api
  - opentelemetry
  - application debugging
  - application performance
  - api monitoring
  - platform dashboard
  - dashboard
  - application management
---

import Thumbnail from "@site/src/components/Thumbnail";

# Platform Dashboard

## Introduction

Go to the `Platform Report` tab in the `Insights` panel for your project to view platform metrics. This dashboard
provides a visual overview of key metrics across four main categories, offering a comprehensive assessment of your API.

## Security and Governance

:::tip Available on

Security and Governance metrics are available on [DDN Free plans](https://hasura.io/docs) and up.

:::

The Security and Governance dashboard provides comprehensive insights into three key areas:

### Authentication status

Quickly identify and resolve authentication configuration issues:

- Displays current authentication status (Configured/Not Configured)
- Shows authentication type when configured (JWT, etc.)
- Provides step-by-step guidance for securing your project when no authentication is configured

<Thumbnail
  src="/img/get-started/dashboard_auth.png"
  alt="The Security and Governance report showing authentication status"
  width="1000"
/>

### Permission analysis

Monitor and manage permissions across your supergraph:

1. **Models Security**

   - Tracks models with row-level filtering for non-admin roles
   - Identifies models with unrestricted access
   - Provides detailed recommendations for securing access

2. **Types Security**

   - Monitors field-level access restrictions
   - Highlights types with unrestricted field access
   - Offers guidance for implementing proper field-level security

3. **Commands Security**
   - Tracks command execution restrictions
   - Identifies commands with unrestricted access
   - Provides recommendations for implementing argument presets

<Thumbnail
  src="/img/get-started/dashboard_permissions.png"
  alt="The Security and Governance report showing permission analysis"
  width="1000"
/>

### Documentation coverage

Track and improve API documentation:

1. **Model & Field Descriptions**

   - Monitors description coverage for models and their fields
   - Shows percentage of documented items
   - Identifies models and fields needing documentation

2. **Command Descriptions**
   - Tracks documentation coverage for commands
   - Provides metrics on documented vs. undocumented commands
   - Helps maintain comprehensive API documentation

<Thumbnail
  src="/img/get-started/dashboard_docs.png"
  alt="The Security and Governance report showing documentation coverage"
  width="1000"
/>

## API reliability

:::tip Available on

Available on DDN Base plans and up.

:::

Monitor your API's health with:

- System Reliability: Error/Success ratio
- Team based analysis coming soon!

<Thumbnail
  src="/img/get-started/dashboard_reliability.png"
  alt="The API Reliability dashboard showing success rate and error metrics"
  width="1000"
/>

## API usage trends

:::tip Available on

API Usage Trends metrics are available on DDN Base plans and up.

:::

### Requests per day

<Thumbnail src="/img/get-started/dashboard_request_trend.png" alt="x" width="1000" />

### Deprecated metadata

Deprecated Features: List of deprecated objects.

<Thumbnail src="/img/get-started/dashboard_usage.png" alt="x" width="1000" />

## Developer productivity

:::tip Available on

Basic metrics available on DDN Base plans and up. Advanced team analytics available on DDN Advanced plans.

:::

Track team collaboration and access patterns:

### Project access distribution

- Visual breakdown of user roles (Owner, Admin, Read-only)
- Total user count
- Available on DDN Base plans

### Subgraph access distribution

- Per-subgraph collaboration metrics
- Admin and read-only user distribution
- Available on DDN Advanced plans

<Thumbnail
  src="/img/get-started/dashboard_productivity.png"
  alt="The Developer Productivity dashboard showing team collaboration metrics"
  width="1000"
/>

You can print the report out using the print icon on the top right

<Thumbnail src="/img/get-started/dashboard_print.png" alt="x" width="1000" />



--- File: ../ddn-docs/docs/observability/built-in/model-analytics/index.mdx ---
# Model Analytics Overview

---
description: "Monitor the operations made and fields used for models/commands with Hasura's Model Analytics."
title: Model Analytics Overview
sidebar_label: Model Analytics
keywords:
  - graphql
  - graphql analysis
  - hasura v3
  - hasura model analytics
  - api debugging
  - graphql performance
  - query optimization
---

# Analytics

## Introduction

Operation and Field Analytics in Hasura provides deep insights into the usage patterns of your
[models](/reference/metadata-reference/models.mdx) and [commands](/reference/metadata-reference/commands.mdx).

This feature allows you to understand how your supergraph is being utilized and identify the most and least queried
portions of your supergraph. It enables you to view GraphQL field-level and operation-level analytics. By focusing on
these metrics, you can make informed decisions to optimize and enhance the functionality of your supergraph, ensuring it
meets the needs of your users effectively.

:::info Feature Availability

This feature is available from [DDN Base Plan](https://hasura.io/pricing) onwards with the retention period of one
month.

:::

## Next steps

- Check out [operation-level analytics](/observability/built-in/model-analytics/operation.mdx).
- Check out [field-level analytics](/observability/built-in/model-analytics/field.mdx).



--- File: ../ddn-docs/docs/observability/built-in/model-analytics/operation.mdx ---
# Operation Analytics

---
sidebar_label: Operation Analytics
sidebar_position: 2
description:
  "Dive into the comprehensive guide for the Hasura Operation level analytics. Understand how different operations
  (queries and mutations) are interacting with your models and commands."
keywords:
  - graphql analysis
  - query optimization
  - execution plan
  - hasura explain api
  - api debugging
  - graphql performance
  - data connector explain
  - api endpoint analysis
  - query execution
  - api troubleshooting
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Operation Analytics

## Introduction

Operation-level analytics provide a broader view of how different operations (queries and mutations) are interacting
with your [models](/reference/metadata-reference/models.mdx) and [commands](/reference/metadata-reference/commands.mdx).
This helps in identifying usage patterns, most and least frequently accessed portions of your data graph. In turn
helping optimize performance, manage resources better, and identify potential areas for improvement or scaling.

Operation analytics can be accessed via the `Analytics` tab within the `Explorer` panel for any specific model or
command, allowing users to view data for either the project API or of a particular build.

<Thumbnail src="/img/model-analytics/operation-analytics.png" alt="Operation Analytics" width="1000px" />

## Model Requests

The Model Requests section provides a visual representation of the number of requests per day made on a particular
model. Such insights help in pinpointing peak and low usage periods and in understanding how the model's usage varies
over time.

<Thumbnail src="/img/model-analytics/model-request-graph.png" alt="Operation Analytics" width="1000px" />

## Operation Summary

The Operation Summary section provides an overview of the operations with the highest and lowest request counts. This
information is crucial for understanding which queries or mutations most or least frequently access a model.

<Thumbnail src="/img/model-analytics/operation-summary.png" alt="Operation Analytics" width="1000px" />

## Operations List

The Operations List section provides a detailed table of all operations for a particular model. This table includes the
operation name, depth level, operation type, and the number of requests. This comprehensive view helps in identifying
and analyzing the usage patterns and complexities of different operations on a model.

### Table columns

- **Operation:** The name of the GraphQL operation.
- **Depth:** Indicates the nesting depth of the operation. Depth in graphql queries can be a performance bottleneck.
- **Type:** The type of operation, such as query or mutation.
- **Number of Requests:** The total number of requests made for this operation.

<Thumbnail src="/img/model-analytics/operations-list.png" alt="Operation Analytics" width="1000px" />

### Operation details

Clicking on the operation name in the table shows additional details about the selected operation.

- **Query Details:** Displays the structure of the GraphQL query, showing the fields being queried.
- **Operation Requests:** A graph that shows the request traffic for this operation over the last month, allowing users
  to identify usage trends and peak periods.
- **Models Used:** Lists the models to which the query is made, providing insight into the data sources involved in the
  operation.

This detailed view enables you to dive deeper into specific operations, understand their structure, and analyze their
usage over time, enabling you to make informed decisions about your APIs evolution and optimization.

<Thumbnail src="/img/model-analytics/Operation-detail.png" alt="Operation Analytics" width="1000px" />



--- File: ../ddn-docs/docs/observability/built-in/model-analytics/field.mdx ---
# Field Analytics

---
sidebar_label: Field Analytics
sidebar_position: 3
description:
  "Dive into the comprehensive guide for the Hasura Field level analytics. Understand the fields frequency of access and
  how each field is used within your models."
keywords:
  - graphql analysis
  - query optimization
  - execution plan
  - hasura explain api
  - api debugging
  - graphql performance
  - data connector explain
  - api endpoint analysis
  - query execution
  - api troubleshooting
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Field Analytics

## Introduction

Field analytics is a crucial component in understanding the granular usage patterns of your API fields. This feature
allows API producers to gain detailed insights into how specific fields within their models are being accessed and
utilized. For example, consider a [model](/reference/metadata-reference/models.mdx) `GetUserWithNotificationsAndReviews`
with fields `notificationsMsg` and `reviews`. Field-level analytics enable you to determine how often
`GetUserWithNotificationsAndReviews.review` was queried over the last month and identify the operations involved in
these queries.

By tracking the frequency and context of field queries, you can make informed decisions about API evolution, performance
optimization, and data governance.

Field analytics can be accessed via the `Analytics` tab within the `Explorer` panel for any specific model or command,
allowing you to view data for either the project API or of a particular build.

<Thumbnail src="/img/model-analytics/field-analytics.png" alt="Field Analytics" width="1000px" />

## Use Cases

- **API Evolution and Deprecation:** Field analytics helps in tracking the usage of deprecated fields. By understanding
  which deprecated fields are still in use, API producers can plan the sunsetting of these fields more effectively,
  ensuring minimal disruption to consumers.
- **Operational Insights and Incident Response:** In the event of an incident, field analytics can help identify
  correlations between specific fields and operational issues. By analyzing field usage patterns, support teams can
  quickly pinpoint potential problem areas and address them efficiently.
- **Resource Optimization and Decision Making:** Understanding which fields are most and least used helps in optimizing
  data fetching strategies. API producers can allocate resources more efficiently, reduce unnecessary data processing,
  and enhance the overall performance of their APIs.

Field analytics provides API producers with the detailed insights necessary to manage and optimize their API offerings
effectively. By leveraging these insights, you can make informed decisions that enhance API performance, ensure
compliance, and deliver better value to your users.

## Field Summary

Field Summary provides a quick glance at the most and least accessed fields within your API. This feature categorizes
field usage into three distinct types: direct queries, usage via relationships, and usage as arguments. These access
modes help understand the operations calling fields across subgraphs and show the power of a rich federated
architecture.

<Thumbnail src="/img/model-analytics/field-summary.png" alt="Field Analytics" width="1000px" />

### Access Mode

The Access Mode provides a breakdown of the different ways in which fields are accessed within your API.

```graphql
query ProductHomePage {
  topTShirtsInUS: products( #1 Select list
    where: { countryOfOrigin: { _eq: "US" }, category: { name: { _eq: "T-Shirts" } } } # 2 Filtering and 7 Nested Filtering
    # order_by: {price: Asc} # 3 Sorting
    order_by: { category: { name: Asc } } # 8 Nested Sorting Same Database
    offset: 1 # 4 Pagination
    limit: 5 # 4 Pagination
  ) {
    id
    name
    price
    description
    manufacturedBy: manufacturer {
      name # 5 Same Database Join
    }
    latestOrders: orders {
      # Two level nested join across database
      createdAt
      userActivity: user {
        name # 6 Three Level Nested Join across database
      }
    }
    # 9 Nested paginate (top n)
    topReviews: reviews(
      where: { createdAt: { _gt: "2023-10-15" } } # Filtering
      order_by: { rating: Desc } # Sorting
      limit: 3 # Pagination
    ) {
      rating
      text
    }
  }
}
```

- **Direct Queries:** Fields accessed directly in the query without nesting. For example, in the query above, `price`
  and `description` are accessed directly.
- **Usage via Relationships:** Fields accessed in a query via relationship within a subgraph or across subgraphs. For
  example, in the query above, `orders.createdAt` is accessed via the relationship from source `products` and
  relationship `orders`.
- **Usage as Arguments:** Fields used as a argument to filter data in a query. For example, in the query above,
  `productId` is used as an argument to filter data.

:::info Field accessed as arguments via relatioships

Some fields are accessed as both as arguments and via relationships.

In the query above, `category.name` is accessed as argument via relationship name `category`.

:::

## Field List

The Field List section provides a detailed breakdown of all fields within a model, their access modes, and the number of
requests made.

<Thumbnail src="/img/model-analytics/field-list.png" alt="Field Analytics" width="1000px" />

### Table columns

- **Field Name:** The name of the field within the model.
- **Access Mode:** The mode in which the field is accessed.
- **Number of Requests:** The total number of requests made for this field.

### Field details

Clicking on the field name in the table opens a sidebar modal that provides in-depth details about the selected field.
The Field Detail section provides a comprehensive analysis of a specific field within a model. This section is designed
to offer in-depth insights into field usage, including request patterns, the operations accessing the field, and the
context of these accesses.

<Thumbnail src="/img/model-analytics/field-detail.png" alt="Field Analytics" width="1000px" />

- **Field Requests Graph:** The Field Requests Graph provides a visual representation of the number of requests made on
  a particular field in a day over a month. This information is crucial for understanding the load on your field and can
  aid in performance tuning and resource allocation.
- **Field Operations List:** The Field Operations List provides a detailed table of all operations that access a
  particular field. This table includes the operation name, access mode, relationship source, and the number of requests
  made from a particular query to that field. This comprehensive view helps in identifying and analyzing the usage
  patterns and complexities of different operations that access the field.

Clicking on the operation name in the table opens a sidebar modal that provides in-depth details about the selected
operation.

<Thumbnail src="/img/model-analytics/query-detail.png" alt="Field Analytics" width="1000px" />



--- File: ../ddn-docs/docs/observability/built-in/explain/index.mdx ---
# Explain Overview

---
description:
  "Explore Hasura's GraphQL query execution with this guide. Learn about various execution types like ModelSelect,
  CommandSelect, and HashJoin, and understand how Hasura's DDN optimizes query performance."
title: Explain Overview
sidebar_label: Explain
keywords:
  - graphql
  - graphql analysis
  - hasura v3
  - hasura explain
  - api debugging
  - graphql performance
  - data connector explain
  - api endpoint analysis
  - query execution
  - execution plan
  - api troubleshooting
  - query optimization
  - dataconnector
  - modelselect
  - commandselect
  - hashjoin
sidebar_position: 1
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Hasura Query Plan

The Hasura DDN engine can output the GraphQL execution plan for a given query via the "explain" API.

The execution plan output is represented as a tree of type [ExplainStep](#explainstep). You can also visualize the query
plan, SQL plan, and SQL query used for each execution by clicking the `Explain query` button in your GraphiQL's request
field:

<Thumbnail src="/img/explain/0.0.1_console_explain-api-sql.png" alt="Explain with sql" width="700px" />

## ExplainStep

The `ExplainStep` output can be one of the following:

| ExplainStep   | Description                                                               |
| ------------- | ------------------------------------------------------------------------- |
| ModelSelect   | A select on the data connector's model                                    |
| CommandSelect | A select on the data connector's command                                  |
| ForEach       | A for-each loop on the data returned by the parent step                   |
| HashJoin      | A hash join of the data returned by the steps to construct valid response |
| Sequence      | A sequential execution of steps                                           |
| Parallel      | A parallel execution of steps                                             |

### ModelSelect

A `ModelSelect` step represents fetching data from a [Model](/reference/metadata-reference/models.mdx). This includes
fetching across local relationships. For example, the following query's execution plan will be a `ModelSelect` only:

```graphql
query LocalRelationship {
  cities {
    # Backed by a model
    code
    name
    state {
      # Local relationship
      name
    }
  }
}
```

<Thumbnail src="/img/explain/local-relationship.png" alt="Local relationship" width="700px" />

[Click here](./api-reference#modelselect) for the API reference of `ModelSelect`.

### CommandSelect

Just like the [ModelSelect](#modelselect), a `CommandSelect` represents fetching data from a
[Command](docs/reference/metadata-reference/commands.mdx). [Click here](./api-reference#commandselect) for the API
reference of `CommandSelect`.

### ForEach

A ForEach step represents fetching data from a `DataConnector` for each of the data from the parent node. A `ForEach`
will always be present if we need to fetch some additional data (such as from a remote
[relationship](/reference/metadata-reference/relationships.mdx)) for the data returned by the parent node. For example,
the following query's execution plan will have a `ForEach` step:

```graphql
query RemoteRelationship {
  cities {
    # Backed by a model
    code
    name
    weather {
      # Remote relationship
      forecast
    }
  }
}
```

<Thumbnail src="/img/explain/remote-relationship.png" alt="Remote relationship" width="700px" />

### HashJoin

A `HashJoin` step represents joining the data fetched from two different steps. For example, the above query's execution
plan will have a `HashJoin` step for joining the `Weather` model to `City`.

### Sequence

A `Sequence` step represents a sequential execution of steps. For example, the following query's execution plan will
have a `Sequence` step (as we need to fetch a `Weather` instance for each of the `City` models fetched):

```graphql
query RemoteRelationship {
  cities {
    # Backed by a model
    code
    name
    weather {
      # Remote relationship
      forecast
    }
  }
}
```

<Thumbnail src="/img/explain/remote-relationship.png" alt="Remote relationship" width="700px" />

### Parallel

A `Parallel` step represents a parallel execution of steps. For example, the following query's execution plan will have
a `Parallel` step (as we can fetch `Department` and `Census` in parallel):

```graphql
query ParallelExecution {
  cities {
    # Backed by a model
    code
    name
    departments {
      # Remote relationship
      name
      ministers {
        name
      }
    }
    census {
      # Remote relationship
      data
    }
  }
}
```

<Thumbnail src="/img/explain/parallel-execution.png" alt="Parallel execution" width="700px" />

## Example

In the example below, for each user, we are fetching the user's name, their notifications via a local relationship, and
their favorite artists from a remote relationship:

```graphql
query MyQuery {
  users {
    name
    # local relationship
    notifications {
      id
      message
    }
    # remote relationship
    user_to_favorite_artist {
      name
    }
  }
}
```

<Thumbnail src="/img/o11y/v0.1.1_console_explain-plan.png" alt="Explain plan example" width="700px" />

Now, from [the explain plan](/observability/built-in/explain/api-reference#example), we can understand that:

- There are three sequential steps:
  - Make a selection on the `users` model
  - For each user, make a selection on the `artists` model
- HashJoin the `users` and `artists` models to construct the response



--- File: ../ddn-docs/docs/observability/built-in/explain/api-reference.mdx ---
# API Reference

---
sidebar_label: API Reference
sidebar_position: 2
description:
  "Dive into the comprehensive guide for the Hasura Explain API. Understand how to analyze GraphQL queries, interpret
  execution plans, and optimize your API interactions for improved performance."
keywords:
  - graphql analysis
  - query optimization
  - execution plan
  - hasura explain api
  - api debugging
  - graphql performance
  - data connector explain
  - api endpoint analysis
  - query execution
  - api troubleshooting
seoFrontMatterUpdated: true
---

# Explain API Reference

## Introduction

The Explain API is an endpoint for analyzing GraphQL queries. Given a query and authorization, it will return the
execution plan for the engine and data connector, if supported.

## Endpoint

All requests are `POST` requests to the `/v1/explain` endpoint.

## API Spec

### Request

The request expects the exact same payload as the GraphQL API (including authentication-related headers).

- `query`: the GraphQL query to be analyzed.
- `variables` (optional): the variables used in the GraphQL query.
- `operationName` (optional): the name of the GraphQL operation.

```http
POST /v1/explain HTTP/1.1
Content-Type: application/json

{
  "query": "<GraphQL query>",
  "variables": {
    "var1" : "...",
    "var2" : "..."
  }
}
```

#### Sample request

```http
POST /v1/explain HTTP/1.1
Content-Type: application/json

{
  "query": "query GetAlbumTracks($AlbumId: Int) {\n  AlbumByID(AlbumId: $AlbumId) {\n    Title\n    Tracks {\n      Name\n    }\n  }\n}",
  "variables": {
    "AlbumId": 1
  },
  "operationName": "GetAlbumTracks"
}
```

### Response

The response for a query is the engine's plans for executing the GraphQL query:

```none
{
  "explain": "<ExplainStep>",
  "errors": [<GraphQLError>]
}
```

#### ExplainStep

The `ExplainStep` can be one of the following:

1. `ModelSelect`: A select on the data connector's model
2. `CommandSelect`: A select on the data connector's command
3. `ForEach`: A for-each loop on the data returned by the parent step
4. `HashJoin`: A hash join of the data returned by the steps to construct valid response
5. `Sequence`: A sequential execution of steps
6. `Parallel`: A parallel execution of steps

#### ModelSelect {#modelselect}

A ModelSelect represents a select on the data connector's model. It has the following structure:

```none
{
  "type": "modelSelect",
  "value": {
    "modelName": "<ModelName>",
    "queryRequest": <NDCQueryRequest>,
    "ndcExplain": <NDCExplain>
  }
}
```

The fields in the `ModelSelect` are:

| Key          | Schema                                                                                 | Description                                  |
| ------------ | -------------------------------------------------------------------------------------- | -------------------------------------------- |
| modelName    | String                                                                                 | The name of the model being selected         |
| queryRequest | [NDCQueryRequest](https://hasura.github.io/ndc-spec/reference/types.html#queryrequest) | The query request sent to the data connector |
| ndcExplain   | NDCExplain                                                                             | The explain response from the data connector |

#### CommandSelect {#commandselect}

A CommandSelect represents a select on the data connector's command. It has the following structure:

```none
{
  "type": "commandSelect",
  "value": {
    "commandName": "<CommandName>",
    "queryRequest": <NDCQueryRequest>,
    "ndcExplain": <NDCExplain>
  }
}
```

The fields in the `CommandSelect` are:

| Key          | Schema                                                                                 | Description                                  |
| ------------ | -------------------------------------------------------------------------------------- | -------------------------------------------- |
| commandName  | String                                                                                 | The name of the command being selected       |
| queryRequest | [NDCQueryRequest](https://hasura.github.io/ndc-spec/reference/types.html#queryrequest) | The query request sent to the data connector |
| ndcExplain   | NDCExplain                                                                             | The explain response from the data connector |

#### ForEach

A ForEach represents a for-each loop on the data returned by the parent step. This is present if the engine is going to
perform remote joins. It has the following structure:

```none
{
  "type": "forEach",
  "value": <ForEachStep>
}
```

The value of the `ForEach` step can be either a [ModelSelect](#modelselect) or a [CommandSelect](#commandselect).

#### HashJoin

A HashJoin represents a hash join of the data returned by the steps to construct valid response. This is present if the
engine is going to perform remote joins. It has the following structure:

```none
{
  "type": "hashJoin"
}
```

#### Sequence

A Sequence represents a sequential execution of steps. It has the following structure:

```none
{
  "type": "sequence",
  "value": [<ExplainStep>]
}
```

#### Parallel

A Parallel represents a parallel execution of steps. It has the following structure:

```none
{
  "type": "parallel",
  "value": [<ExplainStep>]
}
```

#### NDCExplain

The `NDCExplain` contains the explanation of the query execution from the DataConnector's point of view. It has the
following structure:

```none
{
  "type": "response" || "error" || "notSupported"
  "value": <data connector specific response>
}
```

The `NDCExplain` can be of the following types:

1. `response`: The data connector supports explaining query and has given a valid response.
2. `error`: The data connector has raised some error while explaining the query.
3. `notSupported`: The data connector doesn't support explaining NDC queries.

## Example

Let's try to understand the meaning of various nodes in [ExplainStep](#explainstep) through examples.

Using the following query:

```graphql
query {
  Album {
    AlbumId
    ArtistId
    Artist {
      # a remote relationship
      ArtistId
    }
    Tracks {
      # a local relationship
      TrackId
      Album {
        # a remote relationship
        AlbumId
      }
    }
  }
}
```

We get the following response:

```json
{
  "explain": {
    "type": "sequence",
    "value": [
      {
        "type": "modelSelect",
        "value": {
          "modelName": "Album",
          "queryRequest": {
            "collection": "Album",
            "query": {
              "fields": {
                "AlbumId": {
                  "type": "column",
                  "column": "AlbumId"
                },
                "ArtistId": {
                  "type": "column",
                  "column": "ArtistId"
                },
                "__hasura_phantom_field__ArtistId": {
                  "type": "column",
                  "column": "ArtistId"
                },
                "Tracks": {
                  "type": "relationship",
                  "query": {
                    "fields": {
                      "TrackId": {
                        "type": "column",
                        "column": "TrackId"
                      },
                      "__hasura_phantom_field__AlbumId": {
                        "type": "column",
                        "column": "AlbumId"
                      }
                    }
                  },
                  "relationship": "[{\"subgraph\":\"connector_2\",\"name\":\"Album\"},\"Tracks\"]",
                  "arguments": {}
                }
              }
            },
            "arguments": {},
            "collection_relationships": {
              "[{\"subgraph\":\"connector_2\",\"name\":\"Album\"},\"Tracks\"]": {
                "column_mapping": {
                  "AlbumId": "AlbumId"
                },
                "relationship_type": "array",
                "target_collection": "Track",
                "arguments": {}
              }
            }
          },
          "ndcExplain": {
            "type": "response",
            "value": {
              "details": {
                "Execution Plan": "Aggregate  (cost=2342.72..2342.73 rows=1 width=32)\n  ->  Aggregate  (cost=2342.70..2342.71 rows=1 width=32)\n        ->  Nested Loop Left Join  (cost=11.06..2341.65 rows=210 width=40)\n              ->  Seq Scan on \"Album\" \"%0_Album\"  (cost=0.00..12.10 rows=210 width=8)\n              ->  Subquery Scan on \"%3_rows\"  (cost=11.06..11.08 rows=1 width=32)\n                    ->  Aggregate  (cost=11.06..11.07 rows=1 width=32)\n                          ->  Bitmap Heap Scan on \"Track\" \"%2_Track\"  (cost=4.29..11.05 rows=2 width=8)\n                                Recheck Cond: (\"%0_Album\".\"AlbumId\" = \"AlbumId\")\n                                ->  Bitmap Index Scan on \"IFK_TrackAlbumId\"  (cost=0.00..4.29 rows=2 width=0)\n                                      Index Cond: (\"AlbumId\" = \"%0_Album\".\"AlbumId\")",
                "SQL Query": "EXPLAIN\nSELECT\n  coalesce(json_agg(row_to_json(\"%5_universe\")), '[]') AS \"universe\"\nFROM\n  (\n    SELECT\n      *\n    FROM\n      (\n        SELECT\n          coalesce(json_agg(row_to_json(\"%6_rows\")), '[]') AS \"rows\"\n        FROM\n          (\n            SELECT\n              \"%0_Album\".\"AlbumId\" AS \"AlbumId\",\n              \"%0_Album\".\"ArtistId\" AS \"ArtistId\",\n              \"%0_Album\".\"ArtistId\" AS \"__hasura_phantom_field__ArtistId\",\n              \"%1_RELATIONSHIP_Tracks\".\"Tracks\" AS \"Tracks\"\n            FROM\n              \"public\".\"Album\" AS \"%0_Album\"\n              LEFT OUTER JOIN LATERAL (\n                SELECT\n                  row_to_json(\"%1_RELATIONSHIP_Tracks\") AS \"Tracks\"\n                FROM\n                  (\n                    SELECT\n                      *\n                    FROM\n                      (\n                        SELECT\n                          coalesce(json_agg(row_to_json(\"%3_rows\")), '[]') AS \"rows\"\n                        FROM\n                          (\n                            SELECT\n                              \"%2_Track\".\"TrackId\" AS \"TrackId\",\n                              \"%2_Track\".\"AlbumId\" AS \"__hasura_phantom_field__AlbumId\"\n                            FROM\n                              \"public\".\"Track\" AS \"%2_Track\"\n                            WHERE\n                              (\"%0_Album\".\"AlbumId\" = \"%2_Track\".\"AlbumId\")\n                          ) AS \"%3_rows\"\n                      ) AS \"%3_rows\"\n                  ) AS \"%1_RELATIONSHIP_Tracks\"\n              ) AS \"%1_RELATIONSHIP_Tracks\" ON ('true')\n          ) AS \"%6_rows\"\n      ) AS \"%6_rows\"\n  ) AS \"%5_universe\""
              }
            }
          }
        }
      },
      {
        "type": "parallel",
        "value": [
          {
            "type": "forEach",
            "value": {
              "type": "modelSelect",
              "value": {
                "modelName": "Artist",
                "queryRequest": {
                  "collection": "Artist",
                  "query": {
                    "fields": {
                      "ArtistId": {
                        "type": "column",
                        "column": "ArtistId"
                      }
                    },
                    "where": {
                      "type": "binary_comparison_operator",
                      "column": {
                        "type": "column",
                        "name": "ArtistId",
                        "path": []
                      },
                      "operator": {
                        "type": "equal"
                      },
                      "value": {
                        "type": "variable",
                        "name": "$ArtistId"
                      }
                    }
                  },
                  "arguments": {},
                  "collection_relationships": {},
                  "variables": []
                },
                "ndcExplain": {
                  "type": "response",
                  "value": {
                    "details": {
                      "Execution Plan": "",
                      "SQL Query": "EXPLAIN\nSELECT\n  coalesce(json_agg(\"%5_universe_agg\".\"universe\"), '[]') AS \"universe\"\nFROM\n  (\n    SELECT\n      row_to_json(\"%2_universe\") AS \"universe\"\n    FROM\n      json_to_recordset(cast($1 as json)) AS \"%0_%variables_table\"(\"%variable_order\" int)\n      CROSS JOIN LATERAL (\n        SELECT\n          *\n        FROM\n          (\n            SELECT\n              coalesce(json_agg(row_to_json(\"%3_rows\")), '[]') AS \"rows\"\n            FROM\n              (\n                SELECT\n                  \"%1_Artist\".\"ArtistId\" AS \"ArtistId\"\n                FROM\n                  \"public\".\"Artist\" AS \"%1_Artist\"\n                WHERE\n                  (\n                    \"%1_Artist\".\"ArtistId\" = cast(\"%0_%variables_table\".\"$ArtistId\" as int4)\n                  )\n              ) AS \"%3_rows\"\n          ) AS \"%3_rows\"\n      ) AS \"%2_universe\"\n    ORDER BY\n      \"%0_%variables_table\".\"%variable_order\" ASC\n  ) AS \"%5_universe_agg\""
                    }
                  }
                }
              }
            }
          },
          {
            "type": "sequence",
            "value": [
              {
                "type": "forEach",
                "value": {
                  "type": "modelSelect",
                  "value": {
                    "modelName": "Album",
                    "queryRequest": {
                      "collection": "Album",
                      "query": {
                        "fields": {
                          "AlbumId": {
                            "type": "column",
                            "column": "AlbumId"
                          }
                        },
                        "where": {
                          "type": "binary_comparison_operator",
                          "column": {
                            "type": "column",
                            "name": "AlbumId",
                            "path": []
                          },
                          "operator": {
                            "type": "equal"
                          },
                          "value": {
                            "type": "variable",
                            "name": "$AlbumId"
                          }
                        }
                      },
                      "arguments": {},
                      "collection_relationships": {},
                      "variables": []
                    },
                    "ndcExplain": {
                      "type": "response",
                      "value": {
                        "details": {
                          "Execution Plan": "",
                          "SQL Query": "EXPLAIN\nSELECT\n  coalesce(json_agg(\"%5_universe_agg\".\"universe\"), '[]') AS \"universe\"\nFROM\n  (\n    SELECT\n      row_to_json(\"%2_universe\") AS \"universe\"\n    FROM\n      json_to_recordset(cast($1 as json)) AS \"%0_%variables_table\"(\"%variable_order\" int)\n      CROSS JOIN LATERAL (\n        SELECT\n          *\n        FROM\n          (\n            SELECT\n              coalesce(json_agg(row_to_json(\"%3_rows\")), '[]') AS \"rows\"\n            FROM\n              (\n                SELECT\n                  \"%1_Album\".\"AlbumId\" AS \"AlbumId\"\n                FROM\n                  \"public\".\"Album\" AS \"%1_Album\"\n                WHERE\n                  (\n                    \"%1_Album\".\"AlbumId\" = cast(\"%0_%variables_table\".\"$AlbumId\" as int4)\n                  )\n              ) AS \"%3_rows\"\n          ) AS \"%3_rows\"\n      ) AS \"%2_universe\"\n    ORDER BY\n      \"%0_%variables_table\".\"%variable_order\" ASC\n  ) AS \"%5_universe_agg\""
                        }
                      }
                    }
                  }
                }
              },
              {
                "type": "hashJoin"
              }
            ]
          }
        ]
      },
      {
        "type": "hashJoin"
      }
    ]
  }
}
```

The JSON above represents the execution plan for the query. The query is broken down into multiple steps, each step is a
`ModelSelect` or a `ForEach` step. The `ModelSelect` step represents a select on the data connector's model, and the
`ForEach` step represents a for-each loop on the data returned by the parent step.

This allows you to understand how the query is executed and how the data is fetched from the data connector.



--- File: ../ddn-docs/docs/deployment/overview.mdx ---
# Basics

---
sidebar_position: 0
title: Basics
description: "Learn the basics of deploying Hasura DDN projects"
keywords:
  - hasura ddn
  - hasura enterprise
  - hasura private ddn
  - private ddn
  - enterprise ddn
  - byoc
seoFrontMatterUpdated: true
---

import { OverviewTopSectionIconNoVideo } from "@site/src/components/OverviewTopSectionIconNoVideo";
import { OverviewPlainCard } from "@site/src/components/OverviewPlainCard";
import Icon from "@site/static/icons/features/deployment.svg";

# Deployment

## Introduction

There are a few ways to deploy your supergraph API.

## Hasura DDN

You can host your API on fully-managed Hasura DDN.

- Zero infrastructure management
- Global CDN and edge caching
- Automatic scaling and high availability
- Built-in monitoring and observability
- Managed security and updates

[Read more](/deployment/hasura-ddn/index.mdx)

## Private DDN

Hasura Private DDN is an enterprise-grade deployment offering to run your API on dedicated infrastructure.

You can also host your [Data Plane](/help/glossary#data-plane) on your own infrastructure, and the
[Control Plane](/help/glossary#control-plane) on Hasura DDN.

- Enhanced security through private networking
- Direct private connectivity to your data sources
- Custom infrastructure requirements
- Compliance with specific regulatory requirements

[Read more](/private-ddn/overview.mdx)

## Self-Hosted

You can also self-host the v3 engine and connectors on your own Docker-based infrastructure.

- Includes a basic version of the console with GraphiQL and metadata explorer.
- Includes all API related features, plugins and connectors.
- Does not include a control plane: no built-in CI/CD (or build versioning), schema registry, API analytics.
- Observability is not built-in but you can export traces to your own 3rd party service.

[Read more](/deployment/self-hosted/index.mdx)



--- File: ../ddn-docs/docs/deployment/hasura-ddn/index.mdx ---
# Overview

---
sidebar_position: 0
title: Overview
description: "Learn about the different deployment options for Hasura DDN projects"
keywords:
  - hasura ddn
  - deployment
  - hosting
  - self-hosted
  - private ddn
  - enterprise ddn
---

# Deployment Overview

Deploying to Hasura DDN is the optimal way to run your supergraph API. For more details on dedicated infrastructure, see
[Hasura Private DDN](/private-ddn/overview.mdx). For more details on pricing plans, see
[our pricing page](https://hasura.io/pricing).

## Hasura DDN

With Hasura DDN your API runs in a globally distributed cloud where performance, availability, and security are handled
for you.

To deploy your API and make it accessible to the world you will create builds of your connectors, subgraphs, and the
supergraph. This can all be done in a single command. You will then apply the supergraph build to your project, making
it the "official" API for the project.

## Next steps

Use the How To guide for the most common steps to deploy your project to DDN. Try the tutorial for more detailed steps
depending on your specific connector.

- [How to deploy your project to DDN](/deployment/hasura-ddn/deploy-to-ddn.mdx)
- [Deploy specific connectors and subgraphs to DDN](/deployment/hasura-ddn/incremental-builds.mdx)
- [Set up CI/CD for DDN](/deployment/hasura-ddn/ci-cd.mdx)
- [Deploy to DDN tutorial](/deployment/hasura-ddn/tutorial/index.mdx)

### Project configuration:

- [Deploy from multiple repositories.](/project-configuration/subgraphs/working-with-multiple-repositories.mdx)



--- File: ../ddn-docs/docs/deployment/hasura-ddn/deploy-to-ddn.mdx ---
# Deploy to DDN

---
sidebar_position: 2
sidebar_label: Deploy to DDN
description: "Learn how to deploy your project to Hasura DDN."
keywords:
  - hasura ddn
  - deploy
  - deployment
  - hosting
---

# Deploying your project to Hasura DDN

Deploying your project to Hasura DDN is a simple process and can be done in 3 steps.

## Deployment flow

1. Initialize a project on Hasura DDN.
2. Create a supergraph build on Hasura DDN.
3. Apply the supergraph build to your project on Hasura DDN.

To begin this guide you will need to have a local project set up and running with at least one subgraph and connector.
Check out the [quickstart](/quickstart.mdx) for more information on how to get started.

### Step 1: Create a project on Hasura DDN

```bash title="Initialize a project on Hasura DDN"
ddn project init
```

You can optionally also append a project name to this command.

The CLI will respond with the project name, the subgraph(s) and env file which were created, and a hint to create a
supergraph build.

You now have a project on Hasura DDN with the same subgraphs as you have locally. Currently, the subgraphs are empty.

:::info Create vs Initialize

The `ddn project init` command is similar to `ddn project create` but it also creates the subgraphs and env file for
you. `ddn project create` will only create an empty project and you will need to create the DDN project subgraphs and
local `.env.cloud` file yourself.

:::

### Step 2: Create a supergraph build on Hasura DDN

```bash
ddn supergraph build create
```

This command will create builds for each connector, subgraph, and the supergraph. Each of these can be built
independently but this command will create them all.

The CLI will respond with the build version, the API URL, the Console URL, the PromptQL URL, the Project Name, and a
hint to browse the build on the console.

You can now use the console to explore the supergraph build by browsing to the console URL or by running
`ddn console --build-version <build-version>` command.

The build is not yet the "official" applied API for the project. A project can have multiple builds with their own API
endpoints but only one applied at a time as the "official" API.

```bash
ddn supergraph build apply <build-version>
```

eg:

```bash
ddn supergraph build apply 85b0961544
```

This build is now the "official" applied API for the project and is accessible via the API URL in the output of the
command, via the console, or any client accessing via the API URL.

## Summary

There are many more options and configurations available for deploying your project to Hasura DDN and we have detailed
the simplest and most common flow here.



--- File: ../ddn-docs/docs/deployment/hasura-ddn/incremental-builds.mdx ---
# Deploying incrementally

---
sidebar_position: 2
sidebar_label: Deploying incrementally
description: "Learn how to deploy your project incrementally to Hasura DDN."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
---

# Deploying Incrementally

Incremental deployments give you fine-grained control over the compositions of which versions of connectors, subgraphs,
and eventual supergraph are deployed to Hasura DDN. It is particularly helpful if you have a complex project structure
or need to maintain strict control over which services or subgraphs receive new changes. For example:

- You need to test a new version of a connector in isolation before pushing it out to the rest of your API.
- A single subgraph requires frequent updates, and you don't want to rebuild or redeploy the entire supergraph each
  time.
- Multiple teams work on separate parts of the API, and you only want to incrementally roll out changes to one service
  at a time.

By deploying incrementally, you can control the rollout process, test updates more thoroughly in smaller chunks, and
reduce potential downtime or bugs impacting the rest of your API. If you'd rather deploy everything at once and don't
need granular control, check out the more general approach in our
[Deploy to DDN](/deployment/hasura-ddn/deploy-to-ddn.mdx) guide.

## Building and deploying specific connectors

### Step 1: Create a new build of the connector on Hasura DDN.

To build the connector and have it running on the Hasura DDN infrastructure you can run the following command:

```bash
ddn connector build create --connector <path-to-connector.yaml>
```

The output will include the ConnectorBuild Id, the ConnectorBuild Read URL, the ConnectorBuild Write URL, and the
Authorization Header used to communicate with the connector.

### Step 2: Update the env values

Update the env values for the connector URLs and Authorization Header in the corresponding `.env.cloud` file:

```env
APP_MY_CONNECTOR_CONNECTION_URI="<existing-connection-uri>"
APP_MY_CONNECTOR_AUTHORIZATION_HEADER="<new-authorization-header>"
APP_MY_CONNECTOR_READ_URL="<new-read-url>"
APP_MY_CONNECTOR_WRITE_URL="<new-write-url>"
```

### Step 3: Build the supergraph without building connectors

```bash
ddn supergraph build create --no-build-connectors
```

The build version will be output after the build is complete.

Note that you can also build subgraphs individually without building connectors, see the section on
[building and deploying a specific subgraph build](#build-deploy-subgraph) below.

### Step 4: Deploy the supergraph

```bash
ddn supergraph build apply <build-version>
```

This will apply the build to the supergraph to become the active API.

:::info Resource Limits

Connectors in Hasura DDN request specific memory and CPU allocations for their deployment. Resource limits can only be
modified for [Private DDN](/private-ddn/connector-deployment-resources.mdx). For Public DDN, the resource limits are
fixed to the values below, and cannot be downsized or upscaled.

- **Fixed CPU**: 1 vCPU
- **Fixed RAM**: 2GB

:::

## Building and deploying a specific subgraph {#build-deploy-subgraph}

You can build subgraphs independently too.

### Step 1: Build

```bash
ddn subgraph build create
```

This will create a build of the subgraph and any connectors that are associated with it and output the build version.

As always, this will use the current subgraph set in the `context.yaml` file. You can override this with the
`--subgraph` flag or change subgraph context with `ddn context set subgraph <path-to-subgraph.yaml>`.

:::info Hasura DDN only

There is no local subgraph-only build command.

:::

### Step 2 Option 1: Build the supergraph with the new subgraph build

You can create a build of the rest of the supergraph (with the currently applied supergraph build or a different
specific one) but direct Hasura DDN to use the new specific subgraph build.

```bash title="Build the supergraph with a specific subgraph build and the currenly applied supergraph build"
ddn supergraph build create --subgraph-version <subgraph-name>:<subgraph-build-version> --base-supergraph-on-applied
```

```bash title="Build the supergraph with multiple specific subgraph builds and a specific supergraph build"
  ddn supergraph build create --subgraph-version <subgraph-name>:<subgraph-build-version> --subgraph-version <subgraph-name>:<subgraph-build-version> --base-supergraph-version <supergraph-build-version>
```

This will output the build version and you can now use it for testing or apply the supergraph build to be the active
API.

```bash
ddn supergraph build apply <build-version>
```

You can compose any combination of existing subgraph builds this way to create your supergraph builds.

### Step 2 Option 2: Apply a specific subgraph build to the active API

You can also apply the subgraph build directly to the active API.

```bash
ddn subgraph build apply <build-version>
```

## Summary

You have full control over the composition of your supergraph and can build and deploy subgraphs and connectors
incrementally and independently to compose your supergraph.



--- File: ../ddn-docs/docs/deployment/hasura-ddn/ci-cd.mdx ---
# CI/CD

---
sidebar_position: 3
title: CI/CD
description: "Set up CI/CD for Hasura DDN projects using any CI/CD platform"
---

# CI/CD

Because the DDN CLI plays the central role in controlling and managing metadata builds and connector deployments, it
allows developers to easily create effective CI/CD pipelines for managing projects, creating and applying builds or
promoting them to another environment.

## Generic CI/CD Setup

The user should follow the following generic implementation steps which we will go through below when setting up CI/CD
for their Hasura DDN projects.

1. Prepare a service account access token.
2. Use a linux environment to run the DDN CLI.
3. Install and login to the DDN CLI.
4. Clone the repo with the DDN supergraph metadata.
5. Check context or use flags.
6. Run DDN CLI commands.

### Step 1: Prepare a service account access token

See the [service account access token section](/project-configuration/project-management/service-accounts.mdx) to learn how to create a service
account access token.

### Step 2: Use a Linux environment to run the DDN CLI

Any service which can provide a Linux-like environment will work, eg: GitHub Actions, GitLab CI, CircleCI, etc.

Indeed, a Windows or macOS environment will work and run the DDN CLI but for CI/CD pipelines, we recommend using a Linux
environment for speed, availability and convenience.

### Step 3: Install and login to the DDN CLI

```sh
curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/v4/get.sh | bash
```

:::info curl

Your environment should already have `curl` installed. If not, you can install it using your package manager. eg:

```sh
sudo apt update && sudo apt upgrade
sudo apt install curl
curl --version
```

:::

### Step 4: Clone the repo with the DDN supergraph metadata

Clone (or checkout) the repository that contains all of your DDN supergraph metadata. For example, if you have a public
GitHub repository, run:

```sh
git clone https://github.com/<your-org>/<your-repo>.git
```

If your repository is private, you may need a
[GitHub personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)
or similar for your git provider to clone it.

```
git clone https://username:<pat>@github.com/<your-account-or-org>/<repo>.git
```

### Step 5: Check context or use flags

Make sure your context is set properly to reference the correct projects, subgraphs, etc., or use the correct flags in
your DDN CLI commands.

You can set your context using the `ddn context set-current-context <context-name>` command. Read more about
[contexts](/project-configuration/project-management/manage-environments.mdx).

You can also of course use specific flags in your DDN CLI commands eg: `--project`, `--supergraph`,
`--base-supergraph-version`, `--log-level`, `--out` to make your commands more explicit.

### Step 6: Run DDN CLI commands

You can now run any DDN CLI commands required in the CI/CD workflow. You will likely be creating builds and applying
them to projects.

```sh
ddn supergraph build create
```

```sh
ddn supergraph build apply <build-id>
```

## GitHub Actions Setup

We have a GitHub Action that you can use to deploy your Hasura DDN projects.
[See here](https://github.com/hasura/ddn-deployment).



--- File: ../ddn-docs/docs/deployment/hasura-ddn/region-routing.mdx ---
# Region Routing

---
title: Region Routing
sidebar_position: 4
description:
  "Understand how to set up region routing in Hasura DDN to achieve efficient data fetching and lower latency. Benefit
  from optimal database performance and global data access with Hasura."
keywords:
  - multi-region routing
  - region routing
  - hasura
  - postgreSQL
  - data fetching
  - latency
  - geo-routing
  - hasura data connector
  - database optimization
seoFrontMatterUpdated: true
---

# Region Routing

## Introduction

With region routing, you can define the deployment configuration of your data connector for different regions.

For data connectors that connect to a data source, e.g. [PostgreSQL](/how-to-build-with-ddn/with-postgresql.mdx), it is
recommended to deploy the connector in a region closest to the data source to ensure efficient communication between the
connector and the data source.

For other data connectors, e.g. [Typescript](https://hasura.io/connectors/nodejs), it is recommended to deploy the
connector in a region closest to the consumers of the API to ensure efficient communication between the connector and
the Hasura engine.

If you have a distributed data source, with multi-region routing, you can ensure that data is fetched from the data
source closest to the user, thus minimizing latency for the request, improving the performance of your application, and
providing a better user experience.

See the list of supported regions [below](#regions).

## Single-Region Routing

You can modify the `Connector` object as per the highlighted values in the example below to force the deployment of your
data connector to a specific region. If the region is not specified, the connector will be deployed randomly to one of
the [supported regions](#regions).

```yaml title="For example, in my_subgraph/connector/my_connector/connector.yaml:"
kind: Connector
version: v2
definition:
  name: my_connector
  subgraph: my_subgraph
  source: hasura/connector_name:<version>
  context: .
  #highlight-start
  regionConfiguration:
    - region: <region from the list below>
      mode: ReadWrite
      envMapping:
        <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
          fromEnv: <CONNECTOR_ENV_VAR> # e.g. Env Var set as DB read write URL
  #highlight-end
```

## Multi-Region Routing

You can modify the `Connector` object as per the highlighted values in the example below to define the deployment
configuration of your connector across multiple regions.

:::note Currently only supported for PostgreSQL

Multi-region routing is currently supported only for the
[PostgreSQL connector](/how-to-build-with-ddn/with-postgresql.mdx).

Support for other data connectors will be added soon.

:::

```yaml title="For example, in my_subgraph/connector/my_connector/connector.yaml:"
kind: Connector
version: v2
definition:
  name: my_connector
  subgraph: my_subgraph
  source: hasura/connector_name:<version>
  context: .
  #highlight-start
  regionConfiguration:
    - region: <region1: region from the list below>
      mode: ReadWrite
      envMapping:
          <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
              fromEnv: <CONNECTOR_ENV_VAR_REGION_1> # e.g. Env Var set as DB read write replica URL in region1
    - region: <region2: region from the list below>
      mode: ReadOnly
      envMapping:
          <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
              fromEnv: <CONNECTOR_ENV_VAR_REGION_2> # e.g. Env Var set as DB read only replica URL in region2
    - region: <region3: region from the list below>
      mode: ReadOnly
      envMapping:
          <CONNECTOR_ENV_VAR>: # e.g. CONNECTION_URI
              fromEnv: <CONNECTOR_ENV_VAR_REGION_3> # e.g. Env Var set as DB read only replica URL in region3
  #highlight-end
```

## Supported regions {#regions}

Currently, Hasura DDN supports the following regions in GCP for multi-region routing:

- `gcp-asia-south1`
- `gcp-asia-southeast1`
- `gcp-australia-southeast1`
- `gcp-europe-west1`
- `gcp-southamerica-east1`
- `gcp-us-east4`
- `gcp-us-west2`



--- File: ../ddn-docs/docs/deployment/hasura-ddn/tutorial/01-create-a-project.mdx ---
# Create a project

---
sidebar_position: 1
sidebar_label: Create a project
description: "Learn how to get started with Hasura DDN and your GraphQL API."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
---

import Thumbnail from "@site/src/components/Thumbnail";

# Create a Project

## What's about to happen?

We're going to create a project on Hasura DDN.

[**Projects**](/project/configuration/overview.mdxproject will contain exactly one supergraph. When you create a
project, you're creating a resource on Hasura DDN. Eventually, this can be used to serve your API.

:::tip Required

- [DDN CLI](/quickstart.mdx)
- A new or existing [supergraph](/quickstart.mdx)

:::

<Thumbnail src="/img/get-started/ERD/create-project.png" alt="Create a DDN project" width="1000px" />

## Steps

### Step 1. Create a project

Let's start by creating the project on Hasura DDN.

```bash title="From any directory, run:"
ddn project init
```

<details>
  <summary>This command will return the project name and the console URL for your project:</summary>

```bash title="Example output:"
4:19PM INF Project "cool-cat-3685" created on Hasura DDN successfully
+-----------+---------------+
| Project   | fast-pug-7230 |
+-----------+---------------+
| Subgraphs | globals,app   |
+-----------+---------------+
```

</details>

#### What did this do?

The CLI did a lot under the hood:

- It provisioned a new project for you on Hasura DDN and then returned the project name and console URL for your
  project.
- This command also set context so this local project is mapped to the newly-created Hasura DDN project.
- It created subgraphs from your existing local project for the project on Hasura DDN.
- The CLI created a new `.env.cloud` file for variables to be used in your hosted project.

### Step 2. Update your environment variables

The following variables configure how the engine connects to a connector and authenticate that connection. These
variables are automatically updated by the CLI when creating a new connector build. They define the URLs for both
reading and writing data, as well as the authentication details needed to communicate with the connector.

| Variable                                | Description                                                                                                                              |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |
| `APP_MY_CONNECTOR_AUTHORIZATION_HEADER` | Used for setting the authorization header to authenticate requests to the connector.                                                     |
| `APP_MY_CONNECTOR_READ_URL`             | URL for reading data from the connector. Separation of read URLs allows DDN to route requests across multiple database replicas.         |
| `APP_MY_CONNECTOR_WRITE_URL`            | URL for writing data to the connector. Write URLs are routed to replicas that support write operations, configured via `connector.yaml`. |

Hasura DDN overrides the following values at runtime, so they can be left blank in the `.env.cloud` file and are
consumed by the data connector:

| Variable                                              | Description                                  |
| ----------------------------------------------------- | -------------------------------------------- |
| `APP_MY_CONNECTOR_HASURA_CONNECTOR_PORT`              | Configures the connector's HTTP server port. |
| `APP_MY_CONNECTOR_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` | Endpoint for exporting OpenTelemetry traces. |
| `APP_MY_CONNECTOR_HASURA_SERVICE_TOKEN_SECRET`        | Secret for authenticating service requests.  |

If your deployed data source is available at a different address than what was used locally, you'll need to update the
connection strings in your `.env.cloud` file for each connector. This is especially important for variables like
database connection URIs.

For example, if your local environment uses a `MY_SUBGRAPH_MY_PG_CONNECTION_URI` variable, and your cloud database is
hosted elsewhere, update the `.env.cloud` file with the correct connection string:

```env title="Example of updating a connection string:"
MY_SUBGRAPH_MY_PG_CONNECTION_URI="postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app"
```

This updated value should point to the data source you intend to use with your deployed API, ensuring it can connect
properly in the cloud environment.

:::info Tunneling a connection?

If you want to test a build on Hasura DDN using a local database, you can use a tool like [ngrok](https://ngrok.com) to
create a tunnel to your local machine. You can then update the connection URI to use this value in the `.env.cloud`
file.

:::

### Step 3. Deploy your supergraph

You're ready to deploy your supergraph! The following command will build and deploy your data connectors, update the
connectors' URLs in your `.env.cloud` file, and â€” finally â€” create a new build of your supergraph and deploy it ðŸŽ‰

```bash title="Run:"
ddn supergraph build create
```

## Next steps

After you check out your newly-deployed supergraph, we recommend applying a build so it's served by your project. You
can do that from the `GraphiQL` tab of the console, or using the CLI.

```bash title="First, get the list of all builds:"
ddn supergraph build get
```

After the CLI returns the list of builds, you can apply one using its build version:

```bash title="Pass a build version to apply it:"
ddn supergraph build apply <supergraph-build-version>
```



--- File: ../ddn-docs/docs/deployment/hasura-ddn/tutorial/02-create-a-subgraph.mdx ---
# Create a subgraph

---
sidebar_position: 2
sidebar_label: Create a subgraph
description: "Learn how to get started with Hasura DDN and your GraphQL API."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
---

import Thumbnail from "@site/src/components/Thumbnail";

# Create a Subgraph

## What's about to happen?

You're about to create a subgraph on Hasura DDN.

Subgraphs can be rebuilt at any point to reflect changes made in connectors or other metadata related to models,
commands, relationships, or permissions.

:::tip Required

- [DDN CLI](/quickstart.mdx)
- A new or existing [project](/deployment/hasura-ddn/tutorial/01-create-a-project.mdx)

:::

For each subgraph you're deploying, run the following to create it on Hasura DDN, replacing `my_subgraph` with your
desired name. The name should match the subgraph name from your `supergraph.yaml` file in your local metadata.

<Thumbnail src="/img/get-started/ERD/create-subgraph2.png" alt="Create a subgraph on Hasura DDN" width="1000px" />

## Steps

### Step 1. Create a subgraph

```bash title="From any project directory, run:"
ddn project subgraph create my_subgraph
```

## What did this do?

Seemingly, this was an unimpressive step. However, subgraphs in your local metadata will map to resources on Hasura DDN.
Thus, for each subgraph you have in local metadata, you'll create the companion on Hasura DDN.

## Next steps

With a subgraph provisioned on Hasura DDN, you can now begin
[building and deploying your connectors](/deployment/hasura-ddn/tutorial/deploy-a-connector) that service this subgraph.



--- File: ../ddn-docs/docs/deployment/hasura-ddn/tutorial/03-deploy-a-connector.mdx ---
# Deploy a connector

---
sidebar_position: 3
sidebar_label: Deploy a connector
description: "Deploy a connector to Hasura DDN"
hide_table_of_contents: true
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
---

import Thumbnail from "@site/src/components/Thumbnail";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import PostgreSQL from "./partials/postgreSQL/_deployment-tutorial.mdx";
import ClickHouse from "./partials/clickHouse/_deployment-tutorial.mdx";
import TypeScript from "./partials/typeScript/_deployment-tutorial.mdx";
import Python from "./partials/python/_deployment-tutorial.mdx";
import Go from "./partials/go/_deployment-tutorial.mdx";
import OpenAPI from "./partials/openAPI/_deployment-tutorial.mdx";
import GraphQL from "./partials/graphQL/_deployment-tutorial.mdx";
import MongoDB from "./partials/mongoDB/_deployment-tutorial.mdx";

# Deploy a Connector

## What's about to happen?

Hasura DDN will host your deployed connectors for you, ensuring rapid delivery of data from your API to your consumers.

<Tabs groupId="deploy-connector" className="api-tabs">
  <TabItem value="PostgreSQL" label="PostgreSQL">

<PostgreSQL />

  </TabItem>
  <TabItem value="MongoDB" label="MongoDB">

<MongoDB />

  </TabItem>
  <TabItem value="ClickHouse" label="ClickHouse">

<ClickHouse />

  </TabItem>
  <TabItem value="TypeScript" label="TypeScript">

<TypeScript />

  </TabItem>
  <TabItem value="Python" label="Python">

<Python />

  </TabItem>

<TabItem value="Go" label="Go">

<Go />

  </TabItem>
  <TabItem value="OpenAPI" label="OpenAPI">

<OpenAPI />

  </TabItem>
  <TabItem value="GraphQL" label="GraphQL">

<GraphQL />

  </TabItem>
  <TabItem value="Other" label="Other">

The process to deploy a connector is more or less the same for all connectors and varies mostly in the environment
variables required. See the [specific connector's documentation](https://hasura.io/hub) which may be on the GitHub repo
for more information.

## Steps

:::tip Required

- [DDN CLI](/quickstart.mdx)
- A new or existing [supergraph](/quickstart.mdx)
- A new or existing [subgraph](/quickstart.mdx)
- A new or existing [data connector](/quickstart.mdx)
- A new or existing [project](/deployment/hasura-ddn/tutorial/01-create-a-project.mdx)
- A new or existing [subgraph on Hasura DDN](/deployment/hasura-ddn/tutorial/02-create-a-subgraph.mdx)

:::

### Step 1. Update the root `.env.cloud`

Usually, we will need to update the root `env.cloud` file with the connection URI we need for the cloud hosted connector
to be able to communicate with the cloud based data source.

```env title="For example, ./.env.cloud"
CONNECTION_URI=<connection-uri>
```

### Step 2. Build and deploy your cloud connector

At this stage, you can now build and deploy your connector on Hasura DDN.

You can build the entire supergraph with all subgraphs and connectors.

```bash
ddn supergraph build create
```

You can also specify exactly which subgraphs and connectors you want to build. See
[incremental builds](/deployment/hasura-ddn/incremental-builds.mdx) for more information.

To build a specific connector, run the following replacing the directory names with those that reflect your project.

```bash title="Taking care to update the path to your connector's configuration file, run:"
ddn connector build create --connector ./my_subgraph/connector/my_connector/connector.yaml
```

The CLI will respond with the URLs required to communicate with your deployed connector.

In this command we're passing the `--connector` argument, which specifies the path to the connector's configuration file
which we want to build.

## What did this do?

The steps above built and deployed your connector to Hasura DDN. So long as the connection URI you provided is reachable
by DDN, your connector will be able to communicate between your API and your data source.

## Next steps

If you have other connectors needed for a supergraph, repeat these steps for each connector. Otherwise, you're ready to
[create a new build of your supergraph](/deployment/hasura-ddn/tutorial/04-deploy-your-supergraph.mdx) and deploy it to
Hasura DDN!

  </TabItem>
</Tabs>



--- File: ../ddn-docs/docs/deployment/hasura-ddn/tutorial/04-deploy-your-supergraph.mdx ---
# Deploy your supergraph

---
sidebar_position: 4
sidebar_label: Deploy your supergraph
description: "Learn how to get started with Hasura DDN and your GraphQL API."
keywords:
  - hasura ddn
  - graphql api
  - quickstart
  - getting started
  - guide
---

import Thumbnail from "@site/src/components/Thumbnail";

# Deploy your Supergraph

## What's about to happen?

You're about to deploy your supergraph to Hasura DDN, our globally-distributed, highly-available, lightning-fast hosted
service!

<Thumbnail src="/img/get-started/ERD/deploy-supergraph.png" alt="Deploy your supergraph to Hasura DDN" width="1000px" />

## Steps

:::tip Required

- [DDN CLI](/quickstart.mdx)
- A new or existing [supergraph](/quickstart.mdx)
- A new or existing [subgraph](/quickstart.mdx)
- A new or existing [data connector](/quickstart.mdx)
- A new or existing [project](/deployment/hasura-ddn/tutorial/01-create-a-project.mdx)

:::

### Step 1. Build and deploy your supergraph

```bash title="Run:"
ddn supergraph build create
```

:::info Project set in context

Remember that because we set the project context, we don't need to pass the project name as a flag in the command.

:::

### Step 2. Explore your supergraph in the Hasura Console

The CLI will respond with a build version and build **Console URL**. _Click on it!_

You can go ahead and explore the API for this build in the Hasura Console!

### Step 3. Apply your supergraph as your project's endpoint.

An _applied build_ is the default one that is served by your Hasura DDN project endpoint.

```bash title="To apply a build, run:"
ddn supergraph build apply <supergraph-build-version>
```

:::info Your API is still private

By default, all Hasura DDN projects are in `Private` API Access Mode and accessible only to project collaborators. In
your console on DDN you will see a JWT token set in the `x-hasura-ddn-token` header which you can use to access your API
from both the console and any client app. This token will expire after 1 hour and you can regenerate it by refreshing
the console.

You can navigate to the project's settings and switch the API Access Mode setting to `Public` to make your API
accessible to anyone.

**Note: Your API will be now be accessible from any source and you should protect it using either the
[JWT](/auth/jwt/index.mdx) or [webhook](/auth/webhook/index.mdx) auth configuration.**

:::

## What did this do?

When you ran the command above, the CLI used the configuration you provided to create an immutable build of your
supergraph on Hasura DDN. This build is now accessible via the build's GraphQL endpoint and in the Hasura Console for
exploration.

Teammates can explore the API, interact with it, and provide feedback before you iterate and create a new build for
testing. Or, if you're ready, you can apply the build so that it's served by the project's endpoint. And, should you
realize you applied it a _little_ early, you can easily roll it back by applying an older build.

## Next steps

At this point, you have all the ingredients and knowledge to create a robust supergraph that composes data across
various sources and aggregates them into a single, reliable, performant API. Before moving to production, consider the
resources below:

### Migrations

Hasura recommends a number of third-party solutions for managing database migrations. Commonly, users implement
migrations via CI/CD with [Flyway](https://flywaydb.org/) or similar resources.

:::info Doesn't Hasura manage migrations?

In v2, Hasura provided a built-in migration tool. However, as v3 metadata is decoupled from the underlying data source,
you are free to manage your migrations however you wish.

:::

### Performance optimizations

Hasura provides a suite of observability tools directly in a project's DDN console. You can view traces, query plans,
and general usage statistics. These are helpful for diagnosing common bottlenecks and problems with your application's
performance. You can read more about these [here](/observability/overview.mdx).

### CI/CD

You can create a pipeline for deployments using any tools you wish. As we recommend initializing a git repository early
in the project creation process, and provide operability with environment variables, you can follow any git-workflow
best practices for moving between development, staging, and production environments. Additionally, we provide a
configurable [GitHub Action](https://github.com/marketplace/actions/ddn-deployment) for automatically managing your
deployments.



--- File: ../ddn-docs/docs/deployment/hasura-ddn/tutorial/index.mdx ---
# Basic Deployment Tutorial

---
sidebar_position: 6
sidebar_label: Basic Deployment Tutorial
description: "Learn how to deploy your project to Hasura DDN."
keywords:
  - hasura ddn
  - tutorial
  - deploy
  - deployment
  - hosting
---

# Deploying your Project to Hasura DDN

To deploy your API, you need to collectively deploy your connectors and supergraph. Hasura DDN can host these for you,
enabling easy management via the Hasura Console on Hasura DDN. We'll follow these steps over the next several pages:

1. Create a project on Hasura DDN.
2. Deploy each connector you've used to connect data (e.g., PostgreSQL, MongoDB, TypeScript).
3. Deploy your supergraph.



--- File: ../ddn-docs/docs/deployment/self-hosted/docker-compose-simple.mdx ---
# Docker Compose

---
sidebar_position: 1
sidebar_label: Docker Compose
keywords:
  - hasura ddn
  - tutorial
  - deploy
  - deployment
  - hosting
  - self-hosted
id: docker-compose-simple
pagination_prev: deployment/self-hosted/index
pagination_next: deployment/self-hosted/docker-containers
---

# Self-Host: Docker Compose

## Introduction

This guide will take you through hosting your supergraph API on a single cloud machine using Docker Compose. This
approach is similar to running a local development setup in production.

## Prerequisites

- A cloud server (e.g., DigitalOcean Droplet, AWS EC2, etc.) running Linux
- Docker and Docker Compose v2.20 or later installed on the server
- SSH access to your server
- Open port `3280` for TCP traffic
- A Hasura account
- A Hasura DDN project
- Your supergraph project files

:::info Don't yet have a project?

You'll need to create a set of project metadata files using the [DDN CLI](/reference/cli/index.mdx). You can use the
[Quickstart](/quickstart.mdx) docs or get started with a [particular connector](/how-to-build-with-ddn/overview.mdx). We
recommend completing this set of steps on your host machine or via your own CI setup.

:::

## Step 1. Prepare your server

Create your server with your cloud provider and SSH into it.

1. SSH into your cloud server:

   ```bash
   ssh user@your-server-ip
   ```

2. Install Docker if not already installed. Follow the
   [official Docker installation guide](https://docs.docker.com/engine/install/) for your Linux distribution. Docker
   Compose is included with modern versions of Docker.

3. Create a directory for your supergraph:

   ```bash
   mkdir -p /opt/hasura-supergraph
   cd /opt/hasura-supergraph
   ```

4. Install the DDN CLI:

   ```bash
   curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/v4/get.sh | bash
   ```

## Step 2. Get your supergraph files

1. Copy your local supergraph project files to the server. From your local machine in your supergraph project directory
   run:

   ```bash
   rsync -avz --progress ./ <your-server-username>@<your-server-ip>:/opt/hasura-supergraph/
   ```

   Or use git to clone your repository if your project is version controlled:

   ```bash
   git clone your-repository-http-url .
   ```

   :::info GitHub SSH URLs

   To clone with an SSH url from GitHub
   [you will need to configure your SSH access](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-ssh-urls).

   :::

   Note that the `.env` files are ignored by default in your `.gitignore` file so cloning will not include them. You
   will need to either copy them over manually or recreate them on the server.

## Step 3. Authenticate the DDN CLI

You will still need to authenticate the DDN CLI to your Hasura account. Because your headless server does not have a
browser, you will need to use a personal access token.

On your local machine, once you are logged in with `ddn auth login` run:

```bash
ddn auth print-access-token
```

This will print out your personal access token. Copy it and on the cloud server run:

```bash
ddn auth login --access-token <access-token>
```

This will authenticate the DDN CLI with your Hasura account.

## Step 4. Build your supergraph

```bash
ddn supergraph build local
```

## Step 5. Check server network access

You will need to ensure that your server can be accessed from the internet over TCP on port `3280`. Use your cloud
provider's network tools to set up access.

For example, on DigitalOcean, you can create a firewall rule to allow inbound TCP traffic on port `3280` (or all ports)
for all IPv4 sources.

## Step 6. Start your services

```bash
ddn run docker-start
```

Docker logs will be output to the console. You can start another ssh session to run new commands, or amend the
`.hasura/context.yaml` `docker-start` script to include the "detatched" flag `-d` at the end.

Check the status of your containers with:

```bash
docker ps
```

## Step 7. Access your supergraph

Your supergraph API should now be running in production on your cloud server. Access it at
`https://<your-domain-or-ip>:3280/graphql`

You can check your access with this curl command:

```bash
curl -X POST https://<your-domain-or-ip>:3280/graphql \
     -H "Content-Type: application/json" \
     -d '{"query": "{ __schema { types { name } } }"}'
```

You should see a JSON response with the schema of your supergraph.

## Troubleshooting

If you encounter issues:

1. Check service logs:

   ```bash
   docker compose logs -f docker-compose-service-name
   ```

2. Verify all required files and volumes are present

3. Ensure all environment variables are correctly set

4. Check network connectivity between services

## Updating your deployment

To update your deployment with new changes:

1. Pull the latest code changes

2. Rebuild your supergraph:

   ```bash
   ddn supergraph build local
   ```

3. Restart the services:
   ```bash
   docker stop $(docker ps -q)
   ddn run docker-start
   ```



--- File: ../ddn-docs/docs/deployment/self-hosted/index.mdx ---
# Deploying to your own server

---
sidebar_position: 1
sidebar_label: Deploying to your own server
description: "Learn how to deploy your DDN project to your own server."
pagination_next: deployment/self-hosted/docker-compose-simple
keywords:
  - hasura ddn
  - tutorial
  - deploy
  - deployment
  - hosting
  - self-hosted
---

# Deploying to your own infrastructure

## Introduction

Hasura DDN is designed as a distributed system where the engine and data connectors each operate as independent
services. When deploying to your own infrastructure, you can choose to manage these services using Docker â€” either with
Docker Compose for convenience or by running individual containers for more granular control.

## Docker Compose

Running Docker Compose on a cloud server as if it were a local development environment. This is a limited deployment
strategy where the engine and all connectors are deployed on the same server.

[Learn more](/deployment/self-hosted/docker-compose-simple.mdx)

## Standalone containers with Docker

Running individual Docker containers on a cloud server without using Docker Compose. This approach allows for more
granular control over each container, but requires manual setup of networking, environment variables, and dependencies.

[Learn more](/deployment/self-hosted/docker-containers.mdx)



--- File: ../ddn-docs/docs/deployment/self-hosted/kubernetes-self-hosted.mdx ---
# K8s Self-Hosted

---
sidebar_position: 1
sidebar_label: K8s Self-Hosted
description: "Learn how to install a Self-Hosted (Customer Managed) Data Plane."
sidebar_class_name: hidden-sidebar-item
unlisted: true
keywords:
  - hasura ddn
  - enterprise ddn
  - private ddn
---

import Thumbnail from "@site/src/components/Thumbnail";

# Self-Hosted (Customer Managed) Data Plane Installation Guide

Documentation here targets customers who want to self host and self manage their clusters as well as their workloads.
Here, you will find a full set of instructions, which takes you from local development all the way to having your
workloads running under your Kubernetes hosted data plane.

## Prerequisites {#prerequisites}

Before continuing, ensure you go through the following checklist and confirm that you meet all the requirements

- [DDN CLI](/quickstart.mdx) (Latest)
- [Docker v2.20](/quickstart.mdx) (Or greater)
  - You can also run `ddn doctor` to confirm that you meet the minimum requirements
- [Helm3](https://helm.sh/docs/intro/install/) (Prefer latest)
- [Hasura VS Code Extension](/quickstart.mdx) (Recommended, but not required)
- Access to a Kubernetes cluster
  - See Kubernetes version requirement [below](#kubernetes-version)
- Ability to build and push images that can then be pulled down from the Kubernetes cluster
- A user account on the Hasura DDN Control Plane
- A Data Plane id, key & customer identifier created by the Hasura team. These will be referenced as `<data-plane-id>`,
  `<data-plane-key>` & `<customer-identifier>`

### Kubernetes version requirement {#kubernetes-version}

These instructions were tested under the following:

- Amazon Elastic Kubernetes Service (EKS)
- Azure Kubernetes Service (AKS)
- Google Kubernetes Engine (GKE)
- Non-Cloud Kubernetes

Version requirement: `1.28+`

## Step 1. Local development {#local-development}

:::note

In this step, you will be setting up your local development. You will create a supergraph, add connector(s) to it and
perform a local build.

For more details regarding local development, you may reference our
[Getting Started Guide](/how-to-build-with-ddn/overview/).

:::

```bash title="Login with the DDN CLI."
ddn auth login
```

```bash title="Create a local directory where your supergraph will be created.  Afterwards, cd into it."
mkdir hasura_project && cd hasura_project
```

```bash title="Initialize supergraph."
ddn supergraph init .
```

```bash title="Add a connector.  Here, we are using the -i flag for interactive mode."
ddn connector init -i
```

```bash title="Introspect the data source.  Substitute <connector-name> with name you chose for your connector above."
ddn connector introspect <connector-name>
```

```bash title="Add models, commands and relationships based on the output from previous command.  Substitute <connector-name> with name you chose for your connector above."
ddn model add <connector-name> "*"
ddn command add <connector-name> "*"
ddn relationship add <connector-name> "*"
```

```bash title="Build the supergraph locally."
ddn supergraph build local
```

```bash title="Run docker.  Ensure all containers start up successfully."
ddn run docker-start
```

```bash title="Verify via local console."
ddn console --local
```

At this point, you have connected and introspected a data source and built your supergraph locally. Verify that
everything is working as expected before moving on to the next section.

## Step 2. Build connector(s) {#build-connectors}

:::warning Building images with proper target platform

Ensure that you are building the image with the proper target platform. If you need to build an image for a different
target platform, specify it via `export DOCKER_DEFAULT_PLATFORM=<target_platform>` prior to running the commands below.
A common `<target_platform>` to use is `linux/amd64`.

:::

:::note

Repeat the steps below for each connector within your supergraph.

:::

```bash title="Build image via docker compose.  Find the <service-name> from app/connector/<connector>/compose.yaml"
docker compose build <service-name>
```

```bash title="Re-tag the image."
docker tag <root-folder-name>-app_<connector> <your_registry>/<connector>:<your_tag>
```

```bash title="Push the image to your registry."
docker push <your_registry>/<connector>:<your_tag>
```

## Step 3. Deploy connector(s) {#deploy-connectors}

:::info Hasura DDN Helm Repo

Our DDN Helm Repo can be found [here](https://github.com/hasura/ddn-helm-charts/tree/main). Ensure that you run through
the `Get Started` section there first before you attempt to install any Helm charts.

Contact the Hasura team if you don't see a Helm chart available for your connector.

:::

Execute `helm search repo hasura-ddn` in order to find the appropriate Helm chart for your connector. The connector
chart name will be referenced as `<connector-chart-name>` within this step.

A typical connector Helm install command would look like this:

:::info

- `<connector-helm-release-name>`: Helm release name for your connector.

- `<namespace>`: Namespace to deploy connector to.

- `<container_repository_path>`: Container repository path (include the image name) which you chose in
  [Step #2](#build-connectors).

- `<your_tag>`: Image tag which you chose in [Step #2](#build-connectors).

- `<connector-type>`: The connector type. Select the appropriate value from
  [here](https://raw.githubusercontent.com/hasura/ddn-helm-charts/main/CONNECTORS).

- `<data-plane-id>`: Data Plane id, provided by the Hasura team.

- `<data-plane-key>`: Data Plane key, provided by the Hasura team.

- `<connector_env_variable>`: Connector Env variable name and corresponding `<value>`. Run
  `helm show readme <connector-chart-name>` in order to view the connector's README, which will list out the ENV
  variables that you need to set in the `helm upgrade` command.

  :::

:::tip

A common connector ENV variable that always needs to be passed through is
`connectorEnvVars.HASURA_SERVICE_TOKEN_SECRET`. This value comes from the supergraph's `.env` file.

:::

```bash title="Connector Helm install."
helm upgrade --install <connector-helm-release-name> \
  --set namespace="<namespace>" \
  --set image.repository="<container_repository_path>" \
  --set image.tag="<your_tag>" \
  --set dataPlane.id="<data-plane-id>" \
  --set dataPlane.key="<data-plane-key>" \
  --set connectorEnvVars.<connector_env_variable>="<value>" \
  hasura-ddn/<connector-type>
```

## Step 4. Create a cloud project {#create-cloud-project}

Ensure that you are running the commands here from the root of your supergraph.

```bash title="Create cloud project."
ddn project init --data-plane-id <data-plane-id>
```

This command will create a cloud project and will report back as to what the name of your cloud project is. We will
reference this name going forward as `<ddn-project-name>`.

## Step 5. Update .env.cloud with connector URLs {#update-env-cloud}

Next, you will need to access the `.env.cloud` file which was generated at the root of your supergraph.

For **each** READ_URL & WRITE_URL, you will need to make the necessary adjustments to ensure that your v3-engine (Which
will be running in your Kubernetes cluster) is properly configured to reach each one of your connectors. The current
values that these environment variables point to are for local connectivity.

The URLs of these environment variables needs to be structured as shown below.

:::info

URL: **http://`<connector-helm-release-name>`-`<connector-type>`.`<namespace>`:8080**

- `<connector-helm-release-name>`: Helm release name for your connector.

- `<connector-type>`: The connector type. Select the appropriate value from
  [here](https://raw.githubusercontent.com/hasura/ddn-helm-charts/main/CONNECTORS).

- `<namespace>`: Namespace where your connector was deployed to.

:::

## Step 6. Create a cloud build {#create-cloud-build}

After making the necessary changes to your `.env.cloud` file, run the below command. This will create a cloud build and
will also generate the necessary artifacts which will later on be consumed by your v3-engine.

```bash title="Create a build for your cloud project."
ddn supergraph build create --self-hosted-data-plane --output-dir build-cloud --project <ddn-project-name> --out json
```

At this point, take note of the `<build_version>` and `<observability_hostname>` which will be outputted here. You will
need these later.

## Step 7. Build v3-engine {#build-engine}

:::warning Building images with proper target platform

Ensure that you are building the image with the proper target platform. If you need to build an image for a different
target platform, specify it via `export DOCKER_DEFAULT_PLATFORM=<target_platform>` prior to running the commands below.
A common `<target_platform>` to use is `linux/amd64`.

:::

Ensure that you are running the commands from the root of your supergraph.

```bash title="Create a Dockerfile for v3-engine."
cat <<EOF >> Dockerfile
FROM ghcr.io/hasura/v3-engine
COPY ./build-cloud /md/
EOF
```

```bash title="Build the image via docker build.  Tag this image with your own registry and a custom tag of your choosing."
docker build -t <your_registry>/v3-engine:<your_tag> .
```

```bash title="Push the image to your registry."
docker push <your_registry>/v3-engine:<your_tag>
```

## Step 8. Deploy v3-engine {#deploy-engine}

:::note

Every time you create a new cloud build, execute the steps below.

:::

See the DDN Helm Repo [v3-engine](https://github.com/hasura/ddn-helm-charts/tree/main/charts/v3-engine) section for full
documentation. A typical v3-engine Helm installation would look like this:

:::info

- `<v3-engine-helm-release-name>`: Helm release name for v3-engine. A suggested name is: `v3-engine-<build_version>`.

- `<namespace>`: Namespace to deploy v3-engine to.

- `<container_repository_path>`: Container repository path (includes the image name) which you chose in
  [Step #7](#build-engine).

- `<your_tag>`: Image tag which you chose in [Step #7](#build-engine).

- `<observability_hostname>`: Observability hostname. This was returned in output when `ddn supergraph build create` was
  executed.

- `<data-plane-id>`: Data Plane id, provided by the Hasura team.

- `<data-plane-key>`: Data Plane key, provided by the Hasura team.

:::

```bash title="v3-engine helm install."
helm upgrade --install <v3-engine-helm-release-name> \
  --set namespace="<namespace>" \
  --set image.repository="<container_repository_path>" \
  --set image.tag="<your_tag>" \
  --set observability.hostName="<observability_hostname>" \
  --set dataPlane.id="<data-plane-id>" \
  --set dataPlane.key="<data-plane-key>" \
  hasura-ddn/v3-engine
```

## Step 9. Create ingress {#create-build-ingress}

:::note

Every time you create a new cloud build, execute the steps below.

:::

If you're using <b>nginx-ingress</b> and <b>cert-manager</b>, you can deploy using the below manifest (ie. Save it to a
file and run `kubectl apply -f <file_name>`). Ensure that you modify this accordingly

:::info

- `<build_version>`: This was part of the output when you ran `ddn supergraph build create` command.

- `<domain>`: Domain which will be used for accessing this ingress. This could be constructed in the following format:
  **`<build_version>`.`<your_fqdn>`**, where `<your_fqdn>` is a hostname of your own choosing which will host your build
  specific APIs.

- `<namespace>`: Namespace where your v3-engine was deployed to.

- `<v3-engine-helm-release-name>`: Helm release name for your v3-engine.

:::

```bash
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
  labels:
    app: v3-engine-<build_version>
  name: v3-engine-<build_version>
  namespace: <namespace>
spec:
  ingressClassName: nginx
  rules:
  - host: <domain>
    http:
      paths:
      - backend:
          service:
            name: <v3-engine-helm-release-name>-v3-engine
            port:
              number: 3000
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - <domain>
    secretName: <domain>-tls-certs
```

Next, you will be running the command below in order to record the ingress URL within Hasura's Control Plane.

:::info

- `<ingress-url>`: Domain name, chosen above and prepended with protocol.

:::

```bash title="Record the ingress URL for your build version."
ddn supergraph build set-self-hosted-engine-url <ingress-url> --build-version <build_version> --project <ddn-project-name>
```

## Step 10. Deploy v3-engine (Project API specific) {#deploy-engine-project-api}

:::note Why am I deploying v3-engine again via Helm?

In this step, you will be deploying v3-engine using a unique Helm release name. You will re-use this release name
whenever you need to apply a specific build to the Project API. This is done in order to maintain the immutable build
and Project API separation of DDN.

:::

See the DDN Helm Repo [v3-engine](https://github.com/hasura/ddn-helm-charts/tree/main/charts/v3-engine) section for full
documentation.

:::info

- `<v3-engine-project-api-helm-release-name>`: Helm release name for v3-engine Project API. This should be a unique
  release name which is specific to your Project API deployment. **You will use this same name everytime you need to
  apply a build to the project API**.

- `<namespace>`: Namespace to deploy v3-engine to.

- `<container_repository_path>`: Container repository path which is tied to the specific build (includes the image
  name).

- `<your_tag>`: Image tag which is tied to the specific build.

- `<project_api_observability_hostname>`: Observability hostname for Project API. This value is constructed as follows:
  `<ddn-project-name>.<customer-identifier>.observability`

- `<data-plane-id>`: Data Plane id, provided by the Hasura team.

- `<data-plane-key>`: Data Plane key, provided by the Hasura team.

:::

```bash title="v3-engine helm install."
helm upgrade --install <v3-engine-project-api-helm-release-name> \
  --set namespace="<namespace>" \
  --set image.repository="<container_repository_path>" \
  --set image.tag="<your_tag>" \
  --set observability.hostName="<project_api_observability_hostname>" \
  --set dataPlane.id="<data-plane-id>" \
  --set dataPlane.key="<data-plane-key>" \
  hasura-ddn/v3-engine
```

## Step 11. Create ingress (Project API specific) {#create-project-ingress}

:::tip _This step needs to be executed just one time_

Below you will be creating an ingress for the Project API.

:::

**NOTE:** We are once again using an example of an ingress object which will work provided you have <b>nginx</b> and

<b>cert-manager</b> installed on your cluster. Save the contents into a file and run `kubectl apply -f <file_name>`.

:::info

- `<namespace>`: Namespace where your v3-engine was deployed to.

- `<domain>`: Domain which will be used for accessing this ingress. This could be constructed in the following format:
  **`<prod-api>`.`<your_fqdn>`** or **`<ddn-project-name>`.`<your_fqdn>`** (If you want to name it after the project
  name). Note that `<your_fqdn>` is a hostname of your own choosing which will host your Project API.

- `<v3-engine-project-api-helm-release-name>`: Helm release name for your v3-engine. This matches the
  `<v3-engine-project-api-helm-release-name>` that was specified in [Step 10](#deploy-engine-project-api).

  :::

```bash
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
  labels:
    app: v3-engine
  name: v3-engine
  namespace: <namespace>
spec:
  ingressClassName: nginx
  rules:
  - host: <domain>
    http:
      paths:
      - backend:
          service:
            name: <v3-engine-project-api-helm-release-name>-v3-engine
            port:
              number: 3000
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - <domain>
    secretName: <domain>-tls-certs
```

After you create the ingress above, you will be running the command below in order to record the project's API URL
within Hasura's Control Plane.

:::info

- `<ingress-url>`: Domain name, chosen above and prepended with protocol.

:::

```bash title="Record the ingress URL for your project's API."
ddn project set-self-hosted-engine-url <ingress-url>
```

## Step 12. Apply a build to Project API {#apply-build}

:::tip

Every time you need to apply a specific build to your Project API, execute this step.

:::

Repeat the `helm upgrade` instructions in [Step 10](#deploy-engine-project-api), using the unique Helm release name
which you chose for your Project API. **Ensure that you are passing along the appropriate image tag for your v3-engine
(ie. The image tag which is associated with the specific build that you want to apply).**

After you go through the Helm installation, you need to go ahead and mark the build as applied. Proceed with the final
step below.

:::info

- `<build_version>`: This is the build version which you just ran the `helm upgrade` for.

:::

```bash title="Mark build as applied."
ddn supergraph build apply <build_version>
```

## Step 13. View Project API via console {#view-api}

Access [Hasura console](https://console.hasura.io) and locate your cloud project. Access your project and verify your
deployment.



--- File: ../ddn-docs/docs/deployment/self-hosted/docker-containers.mdx ---
# Standalone Docker Containers

---
sidebar_position: 2
sidebar_label: Standalone Docker Containers
keywords:
  - hasura ddn
  - tutorial
  - deploy
  - deployment
  - hosting
  - self-hosted
id: docker-containers
pagination_prev: deployment/self-hosted/index
pagination_next: private-ddn/overview
---

# Self-Host: Standalone Docker Containers

## Introduction

This guide walks you through deploying a Hasura DDN supergraph API using individual Docker containers. This modular
approach allows you to:

- Run services on the same machine or distribute them across multiple servers.
- Scale individual components independently.
- Manually configure networking, environment variables, and orchestration.

## Prepare your project's metadata

To begin, you'll need to create a set of project metadata files using the [DDN CLI](/reference/cli/index.mdx). You can
use the [Quickstart](/quickstart.mdx) docs or get started with a
[particular connector](/how-to-build-with-ddn/overview.mdx). We recommend completing this set of steps on your host
machine or via your own CI setup.

### Step 1. Create a `.env.deployment` file

```sh title="In the root of your project, run the following to scaffold out your new env file:"
cp .env .env.deployment
```

```plaintext title="For each connector, replace the READ and WRITE URLs, for example:"
<SUBGRAPH_NAME>_<CONNECTOR_NAME>_READ_URL=http://<host>:<port>
<SUBGRAPH_NAME>_<CONNECTOR_NAME>_WRITE_URL=http://<host>:<port>
```

:::info What are these URLs?

In the strings above, `host` will refer to whatever domain or IP address your connector(s) will run on and `port` refers
to the exposed port on which the connector is listening.

You'll need to know these values before continuing.

:::

### Step 2. Create a new context

```sh title="If you haven't yet done this, create a new context using the CLI:"
ddn context create-context deployment
```

```yaml title="In .hasura/context.yaml, use the newly-created .env.deployment file:" {5}
contexts:
  deployment:
    supergraph: ../supergraph.yaml
    subgraph: ../app/subgraph.yaml
    localEnvFile: ../.env.deployment
```

### Step 3. Build your supergraph

```sh title="Using the deployment context, use the DDN CLI to create your project's metadata:"
ddn supergraph build local
```

This will generate a set of JSON configuration files in `/engine/build` using your `.env.deployment` file.

:::info Commit this to version control (optional)

As you'll be using this metadata to build your Docker images on your server(s), you'll need a method by which you can
copy these source files over. We recommend using version control (such as Git), but you can also SSH into your server
and use `scp` to transfer over your project.

:::

## Build and run the engine

### Step 1. Build your engine's image

```bash title="On your server, with Docker installed and your latest metadata available, run the following command from the root of your project:"
docker build -t my-engine -f engine/Dockerfile.engine engine
```

This will create an image named `my-engine` using the engine's Dockerfile.

### Step 2. Run your engine

```sh title="Then, from the root of your project, start the container with the required env vars:"
docker run --rm -p 3280:3000 \
  -e AUTHN_CONFIG_PATH=/md/auth_config.json \
  -e ENABLE_CORS=true \
  -e ENABLE_SQL_INTERFACE=true \
  -e INTROSPECTION_METADATA_FILE=/md/metadata.json \
  -e METADATA_PATH=/md/open_dd.json \
  -e OTLP_ENDPOINT=<your_otlp_collector_endpoint> \
  --add-host=local.hasura.dev:host-gateway \
  my-engine
```

```sh title="You can verify this by sending a schema introspection request to your API:"
curl -X POST https://<your-domain-or-ip>:3280/graphql \
     -H "Content-Type: application/json" \
     -d '{"query": "{ __schema { types { name } } }"}'
```

Of course, without a connector built and running, you won't be able to query data from the API.

## Build and run a connector

This process will need to be repeated for **each** connector.

### Step 1. Build your connector's image

```bash title="On your server, with Docker installed and the latest metadata available, run the following command from the root of your project:"
docker build -t my-connector -f <subgraph_name>/connector/<connector_name>/.hasura-connector/Dockerfile.<connector_name> <subgraph_name>/connector/<connector_name>/
```

This will create an image named `my-connector` using the connector's Dockerfile.

### Step 2. Run your connector

```sh title="Then, from the root of your project, start the container with the required env vars:"
docker run --rm -p 9758:8080 \
  -e CONNECTION_URI="<your_database_connection_uri>" \
  -e HASURA_SERVICE_TOKEN_SECRET="<your_service_token_secret>" \
  -e OTEL_EXPORTER_OTLP_ENDPOINT="<your_otlp_collector_endpoint>" \
  -e OTEL_SERVICE_NAME="<subgraph_name>_<my_connector>" \
  --add-host=local.hasura.dev:host-gateway \
  my-connector
```

:::info Where do I find these values?

You can find these values in the `.env.deployment` file you created earlier.

:::

### Step 3. Test your setup

```sh title="You can verify this by sending a real request to your API:"
curl -X POST https://<your-domain-or-ip>:3280/graphql \
     -H "Content-Type: application/json" \
     -d '{"query": "{ <some_type> { <fields> { <field> } } }"}'
```

If everything is set up correctly, your API will return data from your connected source via the data connector(s). If
you encounter issues, check your logs to ensure that your engine and connector containers are running correctly.



--- File: ../ddn-docs/docs/private-ddn/overview.mdx ---
# Private DDN

---
sidebar_position: 1
title: Private DDN
sidebar_label: Basics
description:
  "Get started with Hasura Private DDN to build secure, scalable, and customizable GraphQL APIs on your own
  infrastructure or in the cloud."
keywords:
  - hasura ddn
  - private ddn
  - byoc
  - vpc
  - enterprise
  - data plane
---

# Hasura Private DDN

## Introduction

Hasura Private DDN allows you to provision private Data Planes â€” either on your own infrastructure, or dedicated Hasura
infrastructure â€” to give you complete control over your supergraphs. With Hasura Private DDN, enterprises can ensure
data privacy, security, and compliance by hosting the data plane within their own Virtual Private Cloud (VPC) provided
by Hasura or any infrastructure you choose.

:::info An enterprise contract is required for Private DDN

To enable Private DDN, you'll need an Enterprise contract. Reach out to sales [here](https://hasura.io/contact-us).

:::

## Data Planes

Data Planes are at the core of Hasura Private DDN. They serve as the runtime environment for your API, handling data
access, authorization, and other operations. By deploying Data Planes privately, you can:

- **Maintain data sovereignty**: Ensure data never leaves your private infrastructure.
- **Enhance security**: Leverage existing enterprise-grade security measures, including firewalls, IAM policies, and
  more.
- **Optimize performance**: Reduce latency by hosting the data plane close to your data sources.
- **Enable BYOC**: Bring your own cloud and deploy the data plane on the cloud provider of your choice, whether it's
  AWS, GCP, Azure.

Each Data Plane integrates seamlessly with Hasura DDN's Control Plane, enabling collaborative API development, version
control, and CI/CD integration while keeping runtime operations secure and private.

## Next Steps

Ready to get started with Hasura Private DDN? Here's what you can do next:

- [Architecture](/private-ddn/architecture/index.mdx)
- [Create a Data Plane](/private-ddn/creating-a-data-plane/index.mdx)
- [Add collaborators to a Data Plane](/private-ddn/data-plane-collaboration.mdx)
- [Create a project on a private Data Plane](/private-ddn/create-a-project-on-a-data-plane.mdx)
- [DDN workspace](/private-ddn/ddn-workspace.mdx)
- [Learn more about the architecture](/reference/ddn-topology-architecture.mdx)


--- File: ../ddn-docs/docs/private-ddn/data-plane-collaboration.mdx ---
# Add collaborators to a Data Plane

---
sidebar_position: 4
sidebar_label: Add collaborators to a Data Plane
description: "Learn how to use the Data Plane collaboration feature for Private DDN."
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
seoFrontMatterUpdated: false
---

import Thumbnail from "@site/src/components/Thumbnail";

# How to Add Collaborators to a Data Plane

## Introduction

The Data Plane collaboration feature allows you to manage and facilitate collaboration on the
[Data Plane](/reference/ddn-topology-architecture.mdx#data-plane) for which you are an owner. The owner of the Data
Plane can invite other users to collaborate/create projects on the Data Plane. The invited users can create and manage
projects on the Data Plane. This feature enables users to invite, accept, reject, and remove collaborators for
multi-team collaboration.

## How to get started with Data Plane collaboration

### Inviting a collaborator

To invite a user to your Data Plane, you need to open the
[Data Plane Management Dashboard](https://console.hasura.io/data-plane/). The dashboard will show all available Data
Planes. Select the Data Plane for which you have the `owner` role. Click `Invite Collaborator`. Enter the email address
of the user you want to invite and click `Send Invites`. The invited user will receive an email with an invitation link.

<Thumbnail src="/img/deployment/invite_data_plane_collaborator.png" alt="Invite Data Plane Collaborator" />

### Accepting or rejecting an invitation

The invited user can accept or reject the invitation by clicking on the invitation link received in the email or going
to the [Data Plane Management Dashboard](https://console.hasura.io/data-plane/). The dashboard will show all the invites
received by the user. The user can accept or reject the invitation by clicking `Accept` or `Decline`.

<Thumbnail src="/img/deployment/accept_reject_data_plane_invitation.png" alt="Accept or Reject Data Plane Invitation" />

### Removing a collaborator

You can remove any Data Plane collaborator by going to the
[Data Plane Management Dashboard](https://console.hasura.io/data-plane/). Select the Data Plane for which you have the
`owner` role, you'll be able to see all the collaborators of the Data Plane. Click `Remove` to remove the collaborator.

<Thumbnail src="/img/deployment/remove_data_plane_collaborator.png" alt="Remove Data Plane Collaborator" />

## Data Plane collaboration permissions

Currently there are only two roles available for Data Plane collaboration:

- `owner`: The owner of the Data Plane has full access to the Data Plane and can invite or remove collaborators.
- `member`: The member of the Data Plane can create projects on the Data Plane and manage them.

## Next steps

Learn how to [create a project on a Private Data Plane](/private-ddn/create-a-project-on-a-data-plane.mdx).



--- File: ../ddn-docs/docs/private-ddn/connector-deployment-resources.mdx ---
# Connector Deployment Resources

---
title: Connector Deployment Resources
sidebar_position: 5
description:
  "Understand how to configure resource limits for connectors in Hasura DDN to optimize performance and cost. Learn
  about CPU, memory, and deployment considerations."
keywords:
  - connector deployment
  - resource limits
  - cpu allocation
  - memory allocation
seoFrontMatterUpdated: true
---

# Connector Deployment Resources

## Introduction

Connectors in Hasura DDN can request specific resources for their deployment. These include memory and CPU allocation.
Understanding these limits ensures optimized performance and cost efficiency.

## Default Resource Allocation

By default, connectors have the following resource allocations:

- **CPU**: 1 vCPU
- **Memory**: 2GB RAM

## Custom Resource Limits

Resource limits can only be modified for **Private DDN**. For **Public DDN**, the resource limits are fixed to **1 vCPU
and 2GB RAM**, and cannot be downsized or upscaled.

### Public DDN

- **Fixed CPU**: 1 vCPU
- **Fixed RAM**: 2GB
- CPU and memory cannot be customized for Hasura-provided connectors.
- The limit and request are always set to the same value.

### Private DDN

- **Default CPU**: 1 vCPU
- **Default RAM**: 2GB
- The defaults, minimum, and maximum allowed values can be configured per data plane.

## Memory and CPU Specifications

Memory and CPU resources are specified using the same units as Kubernetes:

- ### Memory Resource Units

Limits and requests for memory are measured in bytes. Memory can be expressed as a plain integer or as a fixed-point
number using one of the following quantity suffixes:

- `E`, `P`, `T`, `G`, `M`, `k` (base-10 units)
- `Ei`, `Pi`, `Ti`, `Gi`, `Mi`, `Ki` (power-of-two units)

For example, the following values represent approximately the same amount of memory:

- `128974848`, `129e6`, `129M`, `128974848000m`, `123Mi`

> **Note:** Pay attention to the case of the suffixes. If you request `400m` of memory, this means `0.4` bytes, which is
> likely a mistake. Instead, you might intend to specify `400Mi` (400 mebibytes) or `400M` (400 megabytes).

- ### CPU Resource Units

Limits and requests for CPU resources are measured in CPU units. In Hasura DDN, `1` CPU unit is equivalent to `1`
physical CPU core or `1` virtual core, depending on whether the node is a physical host or a virtual machine.

Fractional requests are allowed. When a connector specifies `cpu: 0.5`, it is requesting half as much CPU time compared
to `1.0` CPU. For CPU resource units, the quantity `0.1` is equivalent to `100m`, which can be read as "one hundred
millicpu." Some refer to this as "one hundred millicores," and it is understood to mean the same thing.

CPU resources are always specified as an absolute amount, never as a relative amount. For example, `500m` CPU represents
the same amount of computing power whether the connector runs on a single-core, dual-core, or multi-core machine.

## Defining Resource Limits in `connector.yaml`

Resource limits can be set at both the top level and per region.

```yaml
title: "Example connector.yaml configuration"
kind: Connector
version: v2
definition:
  name: my_connector
  resources:
    memory: 128M
    cpu: 0.5
  regionConfiguration:
    - region: us-central1
      resources:
        memory: 128M
        cpu: 2
```

### Resource Limit Overrides

When resource limits are defined at both the top level and the region level, the region-specific values take precedence
over the top-level values for that particular region.

For example, if a connector is configured as follows:

```yaml
definition:
  resources:
    memory: 512M
    cpu: 1
  regionConfiguration:
    - region: us-central1
      resources:
        memory: 128M
        cpu: 2
```

- In `us-central1`, the connector will have **128MB memory and 2 vCPUs**, overriding the top-level `512MB` memory and
  `1 vCPU` settings.
- In any other region not explicitly defined, the top-level values (`512MB memory, 1 vCPU`) will apply.

This allows for fine-grained control over resource allocation based on deployment needs.

## Deployment Considerations

- If no region is specified for a connector, it is deployed to a random region.

By optimizing resource configurations, connectors can achieve optimal performance while maintaining cost efficiency in
Hasura DDN.



--- File: ../ddn-docs/docs/private-ddn/create-a-project-on-a-data-plane.mdx ---
# Create a project

---
sidebar_position: 5
sidebar_label: Create a project
description: "Learn how to create a project on a private Data Plane."
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
seoFrontMatterUpdated: false
---

import Thumbnail from "@site/src/components/Thumbnail";
import Prereqs from "@site/docs/_prereqs.mdx";

# Create a Project on a Data Plane

## Introduction

:::tip Important

This guide will walk you through the steps for creating a project on a Data Plane in
[Private Hasura DDN](/private-ddn/overview.mdx). Before proceeding, you'll first need
to be [invited](/private-ddn/data-plane-collaboration.mdx) to a Data Plane.

If you are unable to meet the prerequisites listed below or your data source is inaccessible
from the location where you run DDN CLI commands, consider using a [DDN Workspace](/private-ddn/ddn-workspace.mdx) instead.

:::

<Prereqs />

## Step 1. Authenticate your CLI

```sh title="Being by authenticating your CLI seession:"
ddn auth login
```

A browser window will open and prompt you for your login credentials. After successfully logging in, you'll be able to
return to the CLI.

The DDN CLI will scaffold out all the necessary files and directories for your project.

## Step 2. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running ls in your terminal, or by opening the directory in your preferred editor.

## Step 3. Add a data source

The command below will launch a wizard via the DDN CLI which will assist you in adding a data connector. You can learn
more about adding sources via data connectors [here](/data-sources/connect-to-a-source.mdx).

```sh title="Add a data source using the CLI:"
ddn connector init <connector_name> -i
```

This will provision the necessary files â€” using your configuration â€” for connecting a data source to your supergraph
API.

:::info Organize your data into subgraphs

You can also organize your API into **subgraphs**. Subgraphs are generally organized around team functions or business
outcomes and allow for independent ownership and governance of data sources. Learn more
[here](/project-configuration/subgraphs/index.mdx).

:::

## Step 4. Generate your metadata

```sh title="First, introspect your data source:"
ddn connector introspect <connector_name>
```

```sh title="Then, generate the metadata:"
ddn model add <connector_name> '*'
ddn command add <connector_name> '*'
ddn relationship add <connector_name> '*'
```

You'll see metadata files generated for each entity in your data source. You can learn more
[here](/data-modeling/overview.mdx).

## Step 5. Create a new local build

```sh title="You can now create a local build:"
ddn supergraph build local
```

```sh title="Then run your local services, including the Hasura DDN Engine and your connector(s):"
ddn run docker-start
```

```sh title="Finally, open the local development console usign the CLI:"
ddn console --local
```

The development console will open wherein you can run your first query.

## Step 6. Create a project on your Data Plane

You'll create a new project using the DDN CLI and by passing two flags: the `data-plane-id` and the `plan`.

:::info Get Data Plane ID

You can find all the Data Planes you have access to on this page: https://console.hasura.io/data-plane

:::

:::info Available Plans

- ddn_free
- ddn_base
- ddn_advanced

Read more about plans [here](/reference/pricing.mdx)

:::

```sh title="Create a new project using your Data Plane ID and plan choice:"
ddn project init --data-plane-id <data-plane-id> --plan <plan-name>
```

The CLI will return information about your newly created project.

## Step 7. Build and deploy your supergraph

```sh title="build and deploy you supergraph:"
ddn supergraph build create
```

When this process is complete, the CLI will return a link to the hosted API where you can run the same query as before.

## Next steps

With your first project created, learn more about how Hasura handles [data modeling](/data-modeling/overview.mdx) before
diving into advanced features.



--- File: ../ddn-docs/docs/private-ddn/ddn-workspace.mdx ---
# DDN workspace

---
sidebar_position: 6
sidebar_label: DDN workspace
description: "Learn how to develop with DDN workspace"
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
  - create private ddn
  - ddn workspace
seoFrontMatterUpdated: false
---

import Thumbnail from "@site/src/components/Thumbnail";

# Developing with DDN workspace

## Introduction

DDN Workspace provides a browser-based development environment for working with Private DDN, eliminating the need to
install DDN CLI and its dependencies locally. It features a familiar VS Code-like UI, runs directly on your data plane,
and includes all necessary dependencies required to work with DDN pre-installed.

This guide will walk you through the steps for using DDN workspace to develop your API on a Data Plane in
[Private Hasura DDN](/private-ddn/overview.mdx). Before proceeding, you'll first need to be
[invited](/private-ddn/data-plane-collaboration.mdx) to a Data Plane.

:::info Prerequisite

To provision a new workspace in Private DDN, please reach out to us [here](https://hasura.io/contact-us).

:::

We recommend a dedicated workspace for each developer working on Private DDN API development. Once the workspace has
been provisioned, it can be launched from the [Private DDN page](https://console.hasura.io/data-plane).

## Advantages

- Get started with just your browser. No need to install DDN CLI and its dependencies (e.g. Docker Engine) on your local
  machine.
- Network connectivity - Since the workspace runs on your data plane, it has direct access to all data sources and
  services within the data plane network. There is no need to set up complex VPN/network policies to access your DBs
  from your local machine.
- Collaboration - Multiple developers can work on the same project simultaneously within the same workspace, if needed.

## Step 1. Launch a workspace

- To launch a workspace, click on the `Launch Workspace` button on the Private DDN. You can find a list of all available
  Private DDN instances here. This will open a new tab in your browser with the workspace UI.

- Click on the `Copy Password` button to copy the password to your clipboard. Use this password to login to the
  workspace.

<Thumbnail src="/img/data-plane/launch-workspace.png" alt="Launch DDN workspace" width="1000px" />

## Step 2. Create a `<service-account-token>`

:::info Prerequisite

In order to perform a `ddn auth login --access-token <service-account-token>`, you need to have a project created in your data plane under which a `<service-account-token>` has been generated.
If you have already done this, continue to the next step.  Otherwise, proceed with the steps below and ensure that you are executing this on your local machine since a browser login is required
during this step.

**Alternate Approach:**

If you are unable to generate a `<service-account-token>`, you can instead create a Personal Access Token (PAT) by heading on over to [Hasura dashboard](https://cloud.hasura.io/account-settings/access-tokens)
and creating a new access token there.

:::

:::info Get Data Plane ID

You can find all the Data Planes you have access to on this page: https://console.hasura.io/data-plane

:::

:::info Available Plans

- ddn_base
- ddn_advanced

Read more about plans [here](/reference/pricing.mdx)

:::

```sh title="Login & create project:"
# Perform a ddn auth login using your personal Hasura Cloud account
ddn auth login

# Create a project
ddn project create --data-plane-id <data-plane-id> --plan <plan-name>
```

Next, follow [these](/project-configuration/project-management/service-accounts/#how-to-create-service-account) steps to generate a `<service-account-token>`.

## Step 3. Authenticate the DDN CLI

Once you're logged into the DDN Workspace, you will be greeted with the familiar VS Code UI. You can perform all typical development tasks, such as
installing extensions, customizing key bindings, and applying themesâ€”all within your browser.

Bring up the terminal using the shortcut `Ctrl` + `` ` ``.

Run the command below to authenticate the CLI with a service account token. If you don't have a token, follow
[these](/project-configuration/project-management/service-accounts/#how-to-create-service-account) steps to generate
one.

```bash
ddn auth login --access-token <service-account-token>
```

Alternatively, if you have opted to use a Personal Access Token (PAT), run the following command instead:

```bash
ddn auth login --pat <personal-account-token>
```

## Step 4. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

After navigating into the directory, you will see the project structure scaffolded for you. You can view the project
structure by running `ls` in the terminal or by exploring it in the VS Code interface.

## Step 5. Add a data source

:::tip

If you have established private connectivity to your data source, you will need to ensure that you are using a
hostname/IP for your database which is specific to your private connectivity connection.
This hostname/IP will be communicated to you by the Hasura team during the coordination of the private connectivity setup.

:::

The command below launches a wizard in the DDN CLI to guide you through adding a data connector. You can learn more
about adding sources via data connectors [here](/data-sources/connect-to-a-source.mdx).

```sh title="Add a data source using the CLI:"
ddn connector init <connector_name> -i
```

This will provision the necessary files using your configuration for connecting a data source to your supergraph API.

:::info Organize your data into subgraphs

You can also organize your API into **subgraphs**. Subgraphs are generally organized around team functions or business
outcomes and allow for independent ownership and governance of data sources. Learn more
[here](/project-configuration/subgraphs/index.mdx).

:::

## Step 6. Generate your metadata

```sh title="First, introspect your data source:"
ddn connector introspect <connector_name>
```

```sh title="Then, generate the metadata:"
ddn model add <connector_name> '*'
ddn command add <connector_name> '*'
ddn relationship add <connector_name> '*'
```

Metadata files will be generated for each entity in your data source. You can learn more
[here](/data-modeling/overview.mdx).

:::info Note

`ddn supergraph build local`, `ddn run docker-start` and `ddn console --local` are not supported in DDN workspace. Once
the metadata is generated, create a cloud project and deploy your supergraph to your Private DDN to query the API.

:::

## Step 7. Initiate against a project on your Data Plane

:::info

In this step, use the `<project-name>` from Step 2.
:::

```sh title="Initiate against an existing project on your data plane:"
ddn project init --with-project <project-name>
```

## Step 8. Build and deploy your supergraph

```sh title="Build and deploy your supergraph:"
ddn supergraph build create
```

When this process is complete, the CLI will return a link to the hosted API where you can query your data.

## Step 9. Exporting your metadata

To export metadata, right-click the Explorer pane and select `Download`. This will download the entire workspace
directory to your local machine. Since Git is pre-installed in the workspace, you can also configure it and push changes
directly to a remote repository.

<Thumbnail src="/img/data-plane/export-metadata.png" alt="Export metadata" width="1000px" />

## Logout

To log out of the workspace, click the Menu button in the top-left corner and select `Sign out of code-server`. This
will close the workspace and you will be redirected to the workspace login page. The state of your local project will be
preserved and you can login again to continue working on it.

<Thumbnail src="/img/data-plane/workspace-logout.png" alt="Logout from workspace" width="1000px" />



--- File: ../ddn-docs/docs/private-ddn/architecture/dedicated.mdx ---
# Dedicated

---
sidebar_position: 1
sidebar_label: Dedicated
description:
  "Learn how you use a VPC with Hasura DDN and have us host your API without any access to the public internet."
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Private DDN: Dedicated

## Introduction

Hasura manages the deployment, uptime, and availability of the
[Data Plane](/reference/ddn-topology-architecture.mdx#data-plane) for you.

Hasura-hosted DDN is available on GCP, AWS, and Azure. It can be deployed to any region of your choice. You can select
multiple regions to set up a distributed-edge configuration to provide the lowest possible latency to your clients.

The Data Plane operates on a dedicated and isolated compute and network infrastructure, ensuring enhanced security and
compliance for your workloads. The infrastructure is set up using the highest isolation levels provided by the cloud
provider.

Private network connectivity is available through VPC Network Peering, Private Link / Private Service Connect, Transit
Gateway, or any other equivalent connectivity option offered by the cloud provider.

Please reach out to us if you need support for another cloud or configuration that is not mentioned here.

<Thumbnail src="/img/deployment/private_ddn_hasura_hosted.png" alt="Private DDN Architecture Hasura Hosted" />

## Get started

To get started with Hasura DDN in Hasura Hosted VPC deployment mode, [contact sales](https://hasura.io/contact-us).



--- File: ../ddn-docs/docs/private-ddn/architecture/index.mdx ---
# Overview

---
sidebar_position: 1
sidebar_label: Overview
description: ""
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
seoFrontMatterUpdated: true
---

# Private DDN

With Private deployment for Hasura DDN, you can run Hasura and your connectors either on dedicated infrastructure hosted
by Hasura or on your own infrastructure. In both cases, Hasura's automated systems will be managing the infrastructure
ensuring uptime and reliability.

Private Hasura DDN offers enhanced security and isolation by enabling private connectivity for your databases, APIs, and
other connectors. Hasura communicates with your sources over a dedicated private network, bypassing the public internet.

- [Dedicated](/private-ddn/architecture/dedicated) private deployments
- [BYOC](/private-ddn/architecture/byoc) private deployments
- [Fully Self-Hosted](/private-ddn/architecture/self-hosted) private deployments



--- File: ../ddn-docs/docs/private-ddn/architecture/byoc.mdx ---
# BYOC

---
sidebar_position: 2
sidebar_label: BYOC
description: "Run Hasura DDN Private Data Plane in your own cloud account, get all managed Data Plane benefits"
keywords:
  - hasura ddn
  - enterprise ddn
  - private ddn
  - byoc
  - bring your own cloud
---

import Thumbnail from "@site/src/components/Thumbnail";

# Private DDN: Bring Your Own Cloud (BYOC)

## Introduction

Private DDN BYOC allows you to deploy the Data Plane component in your own cloud environment. This approach is useful if
you have specific requirements or constraints which prevent you from using the dedicated Data Plane on Hasura's cloud.

BYOC is currently supported on AWS, GCP and Azure.

If you would like access to Private DDN BYOC, please [contact sales](https://hasura.io/contact-us).

## Architecture

In the BYOC mode, the Data Plane is running on the customer's cloud account, while the Control Plane is running on
Hasura's cloud. The Data Plane is managed by Hasura's Control Plane, ensuing timely updates and maintenance. Uptime and
reliability of the Data Plane is a shared responsibility of the customer's infrastrucure team and Hasura's automated
systems.

:::info Data Plane stability

While essential for managing and updating the Data Plane, the Control Plane is not on the critical path for serving API
requests. Even if the Control Plane becomes unavailable, the Data Plane continues to operate without interruption.

:::

<Thumbnail src="/img/deployment/private_ddn_byoc.png" alt="Private DDN Architecture BYOC " />

## Data Flow and security

All critical data operations occur exclusively within the customer infrastructure. When an API user sends a GraphQL
query, it's received by Hasura in the Data Plane. Hasura then directly accesses the necessary data sources within the
customer infrastructure to fulfill the request. This direct access ensures that sensitive data never leaves the
customer's controlled environment, maintaining data residency and security.

While the Control Plane, situated in the Hasura infrastructure, does communicate with the Data Plane, this communication
is strictly limited to configuration and management purposes. Importantly, this communication does not involve customer
data or API responses, further enhancing data security.

The distinction between the Control and Data Planes creates well-defined security boundaries. By keeping the Data Plane
and all data sources within the customer's security perimeter, the architecture ensures that sensitive information
remains under the customer's control at all times.

## Interactions with the Control Plane

The Data Plane running on your infrastructure communicates with the Control Plane only in very specific scenarios:

1. Metadata Storage: The Data Plane accesses a metadata storage bucket to retrieve build artifacts; these artifacts are
   required to process API requests.
2. Build Metadata: The Data Plane accesses the Control Plane APIs to retrieve information about (applied) builds for the
   purposes of routing.
3. Connector Metadata: The Data Plane gets information about the connectors that needs to be deployed so that controller
   within the Data Plane can deploy them.
4. OTEL Gateway: (Optional) The Data Plane sends observability data to the Control Plane so you can visualize it on the
   console; it does not contain any API response data or variables.
5. BYOC Controller: The Control Plane interacts with the Kubernetes cluster to manage the Data Plane workloads.

<Thumbnail
  src="/img/deployment/private_ddn_byoc_detailed.png"
  alt="Detailed architecture diagram for Private DDN BYOC"
/>

## Upgrade Process

Regular software upgrades are rolled out to the Data Plane automatically. This incudes

- Hasura Engine version upgrades
- Other components on the Data Plane
- Kubernetes and other dependencies

Upgrades are typically seamless, utilizing rolling restarts. However, some upgrades (e.g., Kubernetes node upgrades) may
require customer-specified maintenance windows to minimize disruption.

## FAQ

##### What cloud providers are supported for BYOC deployment?

AWS, GCP and Azure are supported.

##### Which regions are supported for BYOC deployment?

Any region on AWS, GCP, Azure are supported, provided there is enough quota available for the workloads.

## Get started

To get started with Hasura DDN in your own cloud, [contact sales](https://hasura.io/contact-us).



--- File: ../ddn-docs/docs/private-ddn/architecture/self-hosted.mdx ---
# Self Hosted

---
sidebar_position: 3
sidebar_label: Self Hosted
description: "Learn how to self-host your own instance of Hasura DDN."
keywords:
  - hasura ddn
  - enterprise ddn
  - private ddn
---

import Thumbnail from "@site/src/components/Thumbnail";

# Self-Hosted

## Introduction

For customers with strict security and compliance requirements, Private DDN Self-Hosted allows you to host both the
Control Plane and Data Plane on your own infrastructure.

This is a premium offering where the Hasura team will be helping you with setting up the entire DDN Platform
organization-wide. An enterprise license is required for this offering.

If you would like access to Private DDN Self-Hosted, please [contact sales](https://hasura.io/contact-us).

## Architecture

In self-hosted mode, the Control Plane and Data Plane are running on customer's cloud account, managed by customer's own
operations team, with the help of the Hasura team. Uptime and reliability of the Data Plane is the responsibility of the
customer's infrastrucure team.

A Kubernetes cluster is required for installing DDN.

<Thumbnail src="/img/deployment/private_ddn_self_hosted.png" alt="Private DDN Architecture Self Hosted" />

## Get started

To get started with Hasura DDN in your own infrastructure, [contact sales](https://hasura.io/contact-us).



--- File: ../ddn-docs/docs/private-ddn/creating-a-data-plane/dedicated.mdx ---
# Dedicated

---
sidebar_position: 1
sidebar_label: Dedicated
description: "Learn how to create a private Data Plane."
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
  - create private ddn
seoFrontMatterUpdated: false
---

import Thumbnail from "@site/src/components/Thumbnail";

# Dedicated: Create a Data Plane

## Introduction

A [Data Plane](/reference/ddn-topology-architecture.mdx#data-plane) in Hasura provides dedicated infrastructure for
running your GraphQL workloads. This guide walks you through the process of creating a new Data Plane for
[dedicated Private DDN](/private-ddn/architecture/dedicated.mdx).

:::info Prerequisite

To create a Data Plane in Private DDN, you'll need an Enterprise contract. Reach out to sales
[here](https://hasura.io/contact-us) to get started.

:::

## How to create a new Data Plane on Dedicated Private DDN

### Step 1. Create a new Data Plane

Navigate to the `Private DDN` section in your [Hasura console](https://console.hasura.io).

<Thumbnail src="/img/data-plane/private-ddn.png" alt="Data Plane Management" width="1000px" />

<br></br>

Click the `Create Data Plane` button.

<Thumbnail src="/img/data-plane/pending-provision-data-planes.png" alt="Data Plane Creation Pending" width="1000px" />

### Step 2. Complete the Data Plane Configuration Form

- **Name:** Enter a descriptive name using only letters and spaces, between 4-30 characters. This will be used to
  identify your Data Plane.
- **Domain:** This field is automatically generated from your Data Plane name. This is the base domain for the GraphQL
  API on this Data Plane.
- **Cloud Provider:** Select your preferred cloud provider. Currently available options:
  - Amazon Web Services (AWS)
  - Google Cloud Platform (GCP)
  - Microsoft Azure
- **Region:** Select the geographical region for your Data Plane.
- **Zones:** Select availability zones for your Data Plane. The maximum number of zones depends on your Hasura Private DDN subscription.
  - For **AWS**: Use AZ IDs (e.g., use1-az1, use1-az2)
  - For **GCP**: Use zone names (e.g., us-west2-a, us-west2-b)
  - For **Azure**: Use physical zone names (e.g., eastus-az1, eastus-az2)

  Expand below to view available zones for your cloud:

<details>
  <summary>AWS zones</summary>

**Availability Zone (AZ) IDs** are unique identifiers for AWS Availability Zones within a region. Using AZ IDs ensures
consistency across different AWS accounts and optimizes performance by aligning your Data Plane with your database's
physical location.

**Why Use AZ IDs Instead of AZ Names?**

- **Consistency:** AZ IDs are consistent across all AWS accounts, whereas AZ Names can differ.
- **Performance:** Deploying resources in the same physical AZ reduces latency.
- **Cost Efficiency:** Avoids cross-zone network costs by ensuring resources are within the same AZ.

Note: When selecting AZ IDs, ensure they correspond to the AZs where your database resides to maintain low latency and
avoid cross-zone network costs. It is recommended to choose at least two AZs for higher availability and fault
tolerance.

[Learn more about AWS AZ IDs](https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html)

**Steps to View Available Zones:**

Using AWS CLI:
```bash
aws ec2 describe-availability-zones \
  --region us-east-1 \
  --output table \
  --query "AvailabilityZones[?State=='available'] | [].{ZoneName: ZoneName, ZoneId: ZoneId}"
```

Example output:
```
-----------------------------
| DescribeAvailabilityZones |
+-----------+---------------+
| ZoneId    | ZoneName      |
+-----------+---------------+
| use1-az1  | us-east-1a    |
| use1-az2  | us-east-1b    |
| use1-az3  | us-east-1c    |
+-----------+---------------+
```

Note: Replace `us-east-1` in the command with your desired region.
</details>

<details>
  <summary>GCP zones</summary>

**Zones in Google Cloud Platform (GCP)** are identified by their zone names, which follow a consistent pattern across all GCP projects. Unlike AWS, GCP uses straightforward zone names that don't require separate ID mapping.

**Why Use Zone Names?**

- **Consistency:** Zone names are consistent across all GCP projects.
- **Performance:** Deploying resources in the same zone reduces latency.
- **Cost Efficiency:** Avoids cross-zone network costs by ensuring resources are within the same zone.

Note: When selecting zones, ensure they correspond to the zones where your database resides to maintain low latency and
avoid cross-zone network costs. It is recommended to choose at least two zones for higher availability and fault
tolerance.

[Learn more about GCP Zones](https://cloud.google.com/compute/docs/regions-zones)

**Steps to View Available Zones:**

Using gcloud CLI:
```bash
gcloud compute zones list \
  --filter="region:us-west2" \
  --format="table(name,region,status)"
```

Example output:
```
NAME        REGION    STATUS
us-west2-a  us-west2  UP
us-west2-b  us-west2  UP
us-west2-c  us-west2  UP
```

Note: Replace `us-west2` in the command with your desired region.
</details>

<details>
  <summary>Azure zones</summary>

**Physical Zone Names** are unique identifiers for Azure Availability Zones within a region. Using physical zone names ensures
consistency across different Azure subscriptions and optimizes performance by aligning your Data Plane with your database's
physical location.

**Why Use Physical Zone Names Instead of Logical Zones?**

- **Consistency:** Physical zone names are consistent across all Azure subscriptions, whereas logical zones (1,2,3) can map differently.
- **Performance:** Deploying resources in the same physical zone reduces latency.
- **Cost Efficiency:** Avoids cross-zone network costs by ensuring resources are within the same physical zone.

Note: When selecting physical zones, ensure they correspond to the zones where your database resides to maintain low latency and
avoid cross-zone network costs. It is recommended to choose at least two physical zones for higher availability and fault
tolerance.

[Learn more about Azure Physical vs Logical Zone Mapping](https://learn.microsoft.com/en-us/azure/availability-zones/az-overview#physical-and-logical-availability-zones)

**Steps to View Available Zones:**

Using Azure CLI:
```bash
az rest --method get \
  --uri '/subscriptions/{subscriptionId}/locations?api-version=2022-12-01' \
  --query 'value[?name==`eastus` && availabilityZoneMappings != `null`].{displayName: displayName, name: name, availabilityZoneMappings: availabilityZoneMappings}'
```

Example output:
```json
[
  {
    "availabilityZoneMappings": [
      {
        "logicalZone": "1",
        "physicalZone": "eastus-az1"
      },
      {
        "logicalZone": "2",
        "physicalZone": "eastus-az3"
      },
      {
        "logicalZone": "3",
        "physicalZone": "eastus-az2"
      }
    ],
    "displayName": "East US",
    "name": "eastus"
  }
]
```

Note: Replace `eastus` in the command with your desired region name.
</details>

- **VPC CIDR:** A /16 CIDR block that defines the IP address range for your Data Plane's Virtual Private Cloud (VPC). Default value: 10.0.0.0/16.
- **Kubernetes Service CIDR:** A /16-/20 CIDR block used for Kubernetes service cluster IP addresses in your Data Plane. This CIDR range is used internally by Kubernetes to assign IP addresses to services running in the cluster. Default value: 172.20.0.0/16.

:::warning Important When choosing your VPC CIDR and Kubernetes Service CIDR:

- Consider your current and future network topology, ensuring these CIDR ranges don't conflict with existing network
  routes or address spaces, especially for VPC peering or VPN connections.
- Consult with your network administrator if you're unsure about potential conflicts.
- Remember that VPC CIDR and Kubernetes Service CIDR cannot be modified once the Data Plane has been created.
- Kubernetes Service CIDR is only required for AWS.

:::

Data Plane Configuration form for AWS:
<Thumbnail src="/img/data-plane/vpc-form.png" alt="Data Plane Creation Form" width="800px" />

<br></br>

Data Plane Configuration form for GCP and Azure:
<Thumbnail src="/img/data-plane/vpc-form-gcp-azure.png" alt="Data Plane Creation Form" width="800px" />

### Step 3. Create and Monitor your Data Plane

Click the `Create` button after filling out all required fields. The creation process will begin, and you'll see your
Data Plane listed with a `Creating` status. The creation process typically takes 60 minutes to complete for AWS and GCP deployments, and 2-3 business days for Azure deployments.

<Thumbnail src="/img/data-plane/data-plane-page.png" alt="Data Plane Page" width="1000px" />

<br></br>

Click on the `Creating` button in the status column to see detailed progress.

<Thumbnail src="/img/data-plane/provisioning-status-modal.png" alt="Data Plane Creating Status" width="100px" />

## After Creation

Once your data plane is successfully created and the status updates to Active, you and your team can begin
[creating projects](/private-ddn/create-a-project-on-a-data-plane.mdx) on your Data Plane. To access more information
about a specific Data Plane, simply click on its name within the Data Planes table. This action will navigate you to a
detailed view page of the selected Data Plane, as illustrated below.

<Thumbnail src="/img/data-plane/data-plane-detail.png" alt="Data Plane Detail" width="1000px" />

## Next steps

Now that you've created a Data Plane on dedicated Private DDN,
[learn how to add collaborators](/private-ddn/data-plane-collaboration.mdx) so they can create projects.



--- File: ../ddn-docs/docs/private-ddn/creating-a-data-plane/index.mdx ---
# Overview

---
sidebar_position: 1
sidebar_label: Overview
description: ""
keywords:
  - hasura ddn
  - private ddn
  - dedicated vpc
  - enterprise ddn
seoFrontMatterUpdated: true
---

# Introduction

A [Data Plane](/reference/ddn-topology-architecture.mdx#data-plane) in Hasura provides dedicated infrastructure for
running your GraphQL workloads. This guide walks you through the process of creating a new Data Plane.

- [Dedicated](/private-ddn/creating-a-data-plane/dedicated.mdx) private deployment
- [BYOC](/private-ddn/creating-a-data-plane/byoc.mdx) private deployment


--- File: ../ddn-docs/docs/private-ddn/creating-a-data-plane/byoc.mdx ---
# BYOC

---
sidebar_position: 2
sidebar_label: BYOC
description: "Run Hasura DDN Private Data Plane in your own cloud account, get all managed Data Plane benefits"
keywords:
  - hasura ddn
  - enterprise ddn
  - private ddn
  - byoc
  - bring your own cloud
---

import Thumbnail from "@site/src/components/Thumbnail";

# Bring Your Own Cloud (BYOC): Create a Data Plane

## Introduction

Private DDN BYOC allows you to deploy the Data Plane component in your own cloud environment. This approach is useful if
you have specific requirements or constraints which prevent you from using the dedicated Data Plane on Hasura's cloud.

BYOC is currently supported on AWS, GCP and Azure.

If you would like access to Private DDN BYOC, please [contact sales](https://hasura.io/contact-us).

## ðŸ“Œ Onboarding Process

To get started with BYOC, customers are required to have one of the following:

- A dedicated AWS Account
- A dedicated project on Google Cloud
- A dedicated Resource Group on Microsoft Azure

## Instructions

### AWS

The setup involves creating an IAM role in your AWS account that establishes a trust relationship with Hasura's AWS automation role (PulumiDDNCli). This role will be used to deploy and manage workloads necessary for Hasura DDN.

##### Pre-requisites

- Dedicated AWS Account with administrative access
- AWS CLI installed and configured
- `AWS_REGION` environment variable set to your desired region (e.g., `export AWS_REGION=us-east-1`)
- Ensure the AWS region where you plan to deploy is enabled in your account
  ```bash
  aws account get-regions --region-opt-status-contains ENABLED --query 'Regions[*].RegionName'
  ```

##### Setup Instructions

1. Copy the following template and save it as `cloudformation.yaml`

<details>

<summary>cloudformation.yaml</summary>
```bash
Resources:
  BootstrapRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: HasuraCloudBYOC
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: arn:aws:iam::760537944023:role/PulumiDDNCli
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                sts:ExternalId: hasura-cloud
  BootstrapPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: HasuraCloudBYOC
      Roles:
        - !Ref BootstrapRole
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - ec2:DescribeAddresses
              - ec2:DescribeAddressesAttribute
              - ec2:DescribeAvailabilityZones
              - ec2:DescribeInternetGateways
              - ec2:DescribeNatGateways
              - ec2:DescribeNetworkInterfaces
              - ec2:DescribeRegions
              - ec2:DescribeRouteTables
              - ec2:DescribeSecurityGroupRules
              - ec2:DescribeSecurityGroups
              - ec2:DescribeSubnets
              - ec2:DescribeTags
              - ec2:DescribeVpcAttribute
              - ec2:DescribeVpcs
              - eks:DeleteAddon
              - eks:DescribeAddon
              - eks:DescribeCluster
              - eks:DescribeNodegroup
              - eks:ListClusters
              - iam:GetRole
              - iam:GetServiceLinkedRoleDeletionStatus
              - sqs:GetQueueAttributes
            Resource: '*'
          - Effect: Allow
            Action:
              - ec2:AllocateAddress
              - ec2:AssociateAddress
              - ec2:AssociateRouteTable
              - ec2:CreateInternetGateway
              - ec2:CreateNatGateway
              - ec2:CreateRoute
              - ec2:CreateRouteTable
              - ec2:CreateSubnet
              - ec2:CreateTags
              - ec2:CreateVpc
              - eks:CreateCluster
              - eks:CreateNodegroup
              - globalaccelerator:CreateAccelerator
              - globalaccelerator:CreateEndpointGroup
              - globalaccelerator:CreateListener
              - globalaccelerator:TagResource
              - sqs:CreateQueue
              - sqs:TagQueue
              - acm:RequestCertificate
              - events:PutRule
              - events:TagResource
              - iam:CreateOpenIDConnectProvider
              - iam:TagOpenIDConnectProvider
            Resource: '*'
            Condition:
              StringEquals:
                aws:RequestTag/Created-By: HasuraCloud
          - Effect: Allow
            Action:
              - ec2:CreateTags
            Resource:
              - !Sub arn:aws:ec2:*:${AWS::AccountId}:security-group/*
            Condition:
              StringEquals:
                aws:RequestTag/karpenter.sh/discovery: dataplane
          - Effect: Allow
            Action:
              - ec2:DeleteTags
            Resource:
              - !Sub arn:aws:ec2:*:${AWS::AccountId}:security-group/*
            Condition:
              StringEquals:
                aws:ResourceTag/karpenter.sh/discovery: dataplane
          - Effect: Allow
            Action:
              - eks:AssociateAccessPolicy
              - eks:DisassociateAccessPolicy
            Resource:
              - !Sub arn:aws:eks:*:${AWS::AccountId}:access-entry/dataplane/*
          - Effect: Allow
            Action:
              - iam:AttachRolePolicy
              - iam:CreateInstanceProfile
              - iam:CreatePolicy
              - iam:CreateRole
              - iam:DeleteInstanceProfile
              - iam:DeleteOpenIDConnectProvider
              - iam:DeletePolicy
              - iam:DeleteRole
              - iam:DeleteServiceLinkedRole
              - iam:DetachRolePolicy
              - iam:GetInstanceProfile
              - iam:GetOpenIDConnectProvider
              - iam:GetPolicy
              - iam:GetPolicyVersion
              - iam:GetRolePolicy
              - iam:ListAttachedRolePolicies
              - iam:ListInstanceProfilesForRole
              - iam:ListOpenIDConnectProviderTags
              - iam:ListPolicyVersions
              - iam:ListRolePolicies
              - iam:PassRole
              - iam:PutRolePolicy
              - iam:RemoveRoleFromInstanceProfile
              - iam:TagInstanceProfile
              - iam:TagOpenIDConnectProvider
              - iam:TagPolicy
              - iam:TagRole
            Resource:
              # Roles
              - !Sub arn:aws:iam::${AWS::AccountId}:role/KarpenterNodeRole
              - !Sub arn:aws:iam::${AWS::AccountId}:role/eksClusterRole-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/lb-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/autoscaler-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/global-accelerator-operator-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/karpenter-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/HasuraWorkloadAutomationRole-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/vpc-cni-*
              - !Sub arn:aws:iam::${AWS::AccountId}:role/ebsCsiDriverRole-*
              # Instance Profiles
              - !Sub arn:aws:iam::${AWS::AccountId}:instance-profile/dataplane_*
              # Policies
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/lb-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/autoscaler-controller-*
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/global-accelerator-operator-*
              - !Sub arn:aws:iam::${AWS::AccountId}:policy/karpenter-controller-*
              # OIDC Providers
              - !Sub arn:aws:iam::${AWS::AccountId}:oidc-provider/oidc.eks.*
              # Service Roles
              - !Sub arn:aws:iam::${AWS::AccountId}:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot
          - Effect: Allow
            Action:
              - iam:CreateServiceLinkedRole
            Resource: '*'
            Condition:
              StringEquals:
                iam:AWSServiceName:
                  - spot.amazonaws.com
                  - globalaccelerator.amazonaws.com
                  - eks.amazonaws.com
                  - eks-nodegroup.amazonaws.com
          - Effect: Allow
            Action:
              - eks:*
              - globalaccelerator:*
              - sqs:*
              - acm:*
              - events:*
            Resource: '*'
            Condition:
              StringEquals:
                aws:ResourceTag/Created-By: HasuraCloud
          - Effect: Allow
            Action:
              - ec2:*
            Resource: '*'
            Condition:
              StringEquals:
                ec2:ResourceTag/Created-By: HasuraCloud

Outputs:
  RoleArn:
    Description: "ARN of the HasuraCloudBYOC IAM Role"
    Value: !GetAtt BootstrapRole.Arn
```
</details>

2. Deploy the CloudFormation stack:

   First, check if the stack already exists:
   ```bash
   aws cloudformation describe-stacks --stack-name hasura-cloud-byoc
   ```

   Then, based on the result:
   - If you see an error "Stack with id hasura-cloud-byoc does not exist":
     ```bash
     # Create new stack
     aws cloudformation create-stack \
       --stack-name hasura-cloud-byoc \
       --template-body file://cloudformation.yaml \
       --capabilities CAPABILITY_NAMED_IAM

     # Wait for creation to complete
     aws cloudformation wait stack-create-complete \
       --stack-name hasura-cloud-byoc
     ```

   - If the stack exists:
     ```bash
     # Update existing stack
     aws cloudformation update-stack \
       --stack-name hasura-cloud-byoc \
       --template-body file://cloudformation.yaml \
       --capabilities CAPABILITY_NAMED_IAM

     # Wait for update to complete
     aws cloudformation wait stack-update-complete \
       --stack-name hasura-cloud-byoc
     ```

3. Monitor stack status:
   ```bash
   aws cloudformation describe-stacks \
     --stack-name hasura-cloud-byoc \
     --query 'Stacks[0].StackStatus'
   ```

4. Once complete, retrieve the Role ARN:
   ```bash
   aws cloudformation describe-stacks \
     --stack-name hasura-cloud-byoc \
     --query 'Stacks[0].Outputs[?OutputKey==`RoleArn`].OutputValue' \
     --output text
   ```

##### Required Information

Share the following with the Hasura team:

- (Required) Role ARN (From output above)
- (Required) AWS Region
- (Optional) Preferred Availability Zones
  - Use AZ IDs (e.g., use1-az1, use1-az2) instead of AZ names (us-east-1a, us-east-1b)
  - You can get the AZ IDs by running:
    ```bash
    aws ec2 describe-availability-zones \
    --region <region> \
    --output table \
    --query "AvailabilityZones[?State=='available'] | [].{ZoneName: ZoneName, ZoneId: ZoneId}"
    ```
  - If you have specific zones which you'd like to use, please pass it along. Otherwise, Hasura will assign accordingly.
- (Optional) VPC CIDR (/16 CIDR)
  - If you have a specific CIDR in mind for the VPC setup, please pass it along. If not specified, Hasura will assign 10.0.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.
- (Optional) Kubernetes Service CIDR (/16-20 CIDR)
  - A /16-/20 CIDR block used for Kubernetes service cluster IP addresses in your Data Plane. If not specified, Hasura will assign 172.20.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.

***

### GCP

The setup involves enabling APIs & creating IAM policy bindings; The IAM policy bindings establish a trust relationship
with Hasuraâ€™s GCP service account which will be used to deploy and manage workloads necessary for Hasura DDN.

##### Pre-requisites

- Dedicated GCP Project

##### Setup instructions

- Enable APIs within GCP project:

```bash
gcloud services enable \
  compute.googleapis.com \
  dns.googleapis.com \
  gkehub.googleapis.com \
  multiclusterservicediscovery.googleapis.com \
  trafficdirector.googleapis.com \
  multiclusteringress.googleapis.com \
  container.googleapis.com \
  certificatemanager.googleapis.com --project ${GCP_PROJECT_ID}
```

- Create following IAM Policy bindings for Hasura's `ddn-automation` service account:

<details>

<summary>IAM Policy bindings</summary>
```bash
gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/compute.networkAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/dns.admin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/gkehub.editor

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/logging.configWriter

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/container.clusterAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/monitoring.metricsScopesAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/certificatemanager.editor

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member='serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com' \
  --role='roles/resourcemanager.projectIamAdmin' \
  --condition='^:^title=Restrict IAM Granting for ddn-automation:description=Restrict ddn-automation to granting specific roles to specific members:expression=api.getAttribute("iam.googleapis.com/modifiedGrantsByRole", []).hasOnly(["roles/container.defaultNodeServiceAccount"]) || api.getAttribute("iam.googleapis.com/modifiedGrantsByRole", []).hasOnly(["roles/container.admin"]) || api.getAttribute("iam.googleapis.com/modifiedGrantsByRole", []).hasOnly(["roles/compute.networkViewer"])'

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/container.admin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/iam.serviceAccountAdmin

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member "serviceAccount:ddn-automation@hasura-ddn.iam.gserviceaccount.com" \
  --role roles/iam.workloadIdentityPoolAdmin
```
</details>

##### Provide Required Information

Share the following with the Hasura team:

- (Required) Project ID
- (Required) GCP Region
- (Optional) Preferred Availability Zones
  - If you have specific zones which you'd like to use, please pass it along.  Otherwise, Hasura will assign accordingly.
- (Optional) VPC CIDR (/16 CIDR)
  - If you have a specific CIDR in mind for the VPC setup, please pass it along.  If not specified, Hasura will assign 10.0.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.

***

### Azure

##### Pre-requisites

A dedicated resource group.  If you do not have one, create it.

##### Create App Entities for BYOC

###### For infra provisioning:

- Replace `$(customer_tenant_id)` with your Azure Tenant ID in the URL below:
   `https://login.microsoftonline.com/$(customer_tenant_id)/oauth2/authorize?client_id=4f7f1f59-f0b0-4adb-8603-2afacc50552b&response_type=code`
- Then open the URL in your browser to create the app entity named **PrivateDdnInfra** in your Azure account.
  1.  If you encounter a redirect error ("No reply address is registered for the application") after clicking on
      Approve, ignore this. The actual app will still be created behind the scenes successfully.

###### For workload provisioning:

- Replace `$(customer_tenant_id)` with your Azure Tenant ID in the URL below:
   `https://login.microsoftonline.com/$(customer_tenant_id)/oauth2/authorize?client_id=ce413986-b5c3-4e49-b45e-093f07776c14&response_type=code`
- Then open the URL in your browser to create the app entity named **ArgoCDDeployer** in your Azure account.
  1.  If you encounter a redirect error ("No reply address is registered for the application) after clicking on Approve,
      ignore this. The actual app will still be created behind the scenes successfully.

#### Grant Required Roles

Assign the following roles at the resource group level:

- For `PrivateDdnInfra`:
  - Contributor
  - Role Based Access Control (RBAC) Administrator
    1. Allow user to assign all roles except the roles you select
    2. Configure the constrained roles by adding Owner to exclude roles
- For `ArgoCDDeployer`:
  - Azure Kubernetes Service RBAC Cluster Admin

#### Additional requirements

- Register the `EnableAPIServerVnetIntegrationPreview` feature flag using the az feature register command: `az feature register --namespace "Microsoft.ContainerService" --name "EnableAPIServerVnetIntegrationPreview"`
- Hasura provisioning requires that various node pools be created in order to support DDN workloads. Hasura will be
provisioning 2 vCPU/8GB RAM instance types for this purpose (Using x64 VM Architecture). We therefore require that CPU
related quotas are set accordingly by the customer on a desired instance family type in order to allow Hasura the
ability to provision nodes without any issues. A base configuration of worker nodes will consume 20 vCPU. We however
recommend setting an appropriate threshold in order to accommodate workers scaling up and down.

#### Provide Required Information

Get the following details from your
[Azure Portal](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview) and pass them along
to the Hasura team.

- (Required) Subscription ID
- (Required) Tenant ID
- (Required) Resource Group Name
- (Required) Azure Region
- (Optional) Preferred Availability Zones
  - If you have specific zones which you'd like to use, please pass it along.  Otherwise, Hasura will assign accordingly.
- (Optional) VPC CIDR (/16-/19 CIDR)
  - If you have a specific CIDR in mind for the VPC setup, please pass it along.  If not specified, Hasura will assign 10.0.0.0/16.
  - Note: If you are planning to use VPC Peering, this CIDR should not conflict with any networks on your side.



--- File: ../ddn-docs/docs/reference/ddn-topology-architecture.mdx ---
# DDN Topology & Architecture

---
sidebar_label: DDN Topology & Architecture
description: "Learn about the topology and architecture of Hasura DDN"
keywords:
  - hasura ddn
  - architecture
  - topology
---

import Thumbnail from "@site/src/components/Thumbnail";

# DDN Topology & Architecture

## Topology

Compared to Hasura v2, a fundamental architecture improvement introduced Hasura DDN where the buildtime and runtime are
separated into distinct components. The [Control Plane](#control-plane) builds the project's metadata and makes it
available for the Hasura DDN engine running on the [Data Plane](#data-plane).

<Thumbnail src="/img/deployment/control_plane.png" alt="control plane" />

### Control Plane {#control-plane}

Developers interact with the Hasura DDN Control Plane to create [supergraph builds](/project-configuration/overview.mdx)
Each supergraph build produces a unique GraphQL API, hosted by the Data Plane. As a developer, you use the Hasura
console on the Control Plane to visualize the supergraph and interact with the GraphQL API.

The Control Plane also provides monitoring and observability features. The console lets you add collaborators so that
you can invite other developers to contribute to the supergraph and also to share the GraphQL API with consumers.

The Control Plane enables preview deployments, in the form of builds, by accepting your project metadata and making it
available to the Data Plane in a format optimized for the Hasura DDN Engine runtime.

Each build you create will results in a unique, immutable GraphQL API that can be used to execute operations
independently, without affecting any other builds.

The Control Plane processes observability data sent by the Data Plane and makes it available for consumption on the
console. This includes traces for every GraphQL operation executed and overall metrics on API usage.

### Data Plane {#data-plane}

The Hasura DDN Engine and the data connectors run on the Data Plane. The Hasura DDN Data Plane runs an enhanced version
of the DDN engine which is capable of serving multiple GraphQL APIs at the same time. It is a serverless-like runtime
where the configuration for executing a GraphQL request is loaded on-demand and is discarded after processing the
request. This makes the runtime extremely efficient and highly scalable.

Certain connectors also makes use of similar strategies to efficiently handle connection pools, only creating them
on-demand.

The Data Plane can be deployed across multiple regions, enabling API requests to be routed to the nearest client
location for optimal latency through advanced load-balancing with Anycast IP addresses.

This feature is readily available on GCP, AWS, and Azure, and can also be deployed on self-hosted Hasura DDN, provided
the infrastructure supports the necessary network capabilities.

## Architecture

Hasura DDN is a fully managed SaaS product where your API is running on the cloud, and where performance, availability
and security is already taken care of for you. Your API will be running on Hasura's optimized serverless production
infrastructure.

For organizations with advanced security and compliance needs, Hasura DDN can be deployed within isolated compute and
network infrastructure and hosted with a specific cloud provider and regions you choose. Private deployment of Hasura
DDN enables secure connectivity to your data sources, whether they are within your Cloud VPCs or on-prem infrastructure.
You can also configure the API to be exposed over internal IP alone. Advanced firewall rules and API protections are
also available.

Hasura Private DDN is available as a dedicated offering that is fully hosted and managed by Hasura. It can be deployed
to your own cloud provider account in a Bring Your Own Cloud (BYOC) model, where it is still completely managed by
Hasura.

### Hasura DDN

When you create a project on Hasura DDN, your GraphQL API is running on global serverless edge infrastructure hosted by
Hasura. You configure your instance to talk to your databases, APIs, or other connectors that are hosted on your
infrastructure and Hasura communicates with them over the public internet.

<Thumbnail src="/img/deployment/ddn_architecture.png" alt="DDN Architecture" />

With serverless-edge deployment, Hasura and your connectors operate within Hasura's infrastructure. We manage automatic
scaling, provisioning, operations, and maintenance.

This deployment option is production-ready, with the API available **instantly**. There is no base cost: it is free if
you are on the Free plan, or you only pay for the [models](/reference/metadata-reference/models.mdx) if you're on the
Base or Advanced plans. For more details, visit our [pricing page](https://hasura.io/pricing/ddn).

### Hasura Private DDN

Hasura Private DDN provides all the benefits of Hasura DDN with the added advantage of private data connectivity and
isolated infrastructure. We highly recommend Private DDN for any organization using Hasura DDN to connect to private
data, ensuring that sensitive information remains secure and is protected from vulnerabilities associated with public
networks.

<Thumbnail src="/img/deployment/private_ddn.png" alt="Hasura Private DDN Architecture" />

With Private DDN, the Data Plane is running on Dedicated Infrastructure which provides dedicated network and compute
infrastructure for your deployment, offering superior security by isolating traffic from public networks and other
customers. The highest level of isolation provided by cloud providers is used to make sure that the workloads are
dedicated to the customer.

Private Data Source Connectivity enables direct, secure, and private connections to data sources without relying on
public networks, minimizing latency and enhancing data protection. With Private DDN, VPC peering, VPN, Private Link or
any other cloud vendor specific technology is available.

Single Sign-On (SAML) with Private DDN enables effortless and secure access by integrating with identity providers like
Okta or Active Directory. Custom Firewall Rules allows organizations to define custom rules, controlling network traffic
flow based on specific security policies. Private DDN also gives customers Private API Endpoints (For Internal network
use) that are restricted to internal networks, ensuring external traffic cannot access your applications or data.

### Multi-region

Hasura DDN is deployed to many regions around the globe. This deployment model provides lowest latency possible to your
API clients. The API endpoint that Hasura DDN provides is backed by an Anycast IP address, where a single IP is
allocated for serving the incoming request and the request is routed to the nearest Hasura DDN region.

<Thumbnail src="/img/deployment/multi_region_postgres.png" alt="DDN multi-region routing" />

Combined with multi-region databases, Hasura DDN gives you the best performance for your APIs, without the hassle of
setting up any of this infrastructure yourself. Read more about how this works on
[our blog](https://hasura.io/blog/global-reach-local-performance-introducing-hasuras-multi-region-database-routing).



--- File: ../ddn-docs/docs/ai/overview.mdx ---
# Overview

---
sidebar_position: 1
sidebar_label: Overview
description: "Enable Hasura PromptQL in your DDN instance to get AI-powered insights into your data."
keywords:
  - hasura ddn
  - promptql
  - ai
  - promptql
  - hasura ai
hide_table_of_contents: true
seoFrontMatterUpdated: true
---

import { OverviewTopSectionIconNoVideo } from "@site/src/components/OverviewTopSectionIconNoVideo";
import { OverviewPlainCard } from "@site/src/components/OverviewPlainCard";
import Icon from "@site/static/icons/eye.svg";

# AI With PromptQL

## Introduction

You can enable Hasura PromptQL in your DDN instance to get AI-powered insights into your data. Read more about PromptQL
in the dedicated [PromptQL site](https://promptql.hasura.io/).



--- File: ../ddn-docs/docs/reference/overview.mdx ---
# Reference

---
title: Reference
sidebar_position: 1
description:
  "Reference docs for Hasura DDN will help you dive deep into connector configuration, metadata structures, and CLI
  commands."
sidebar_label: Overview
keywords:
  - reference
  - configuration
  - connector configuration
  - metadata
  - cli commands
seoFrontMatterUpdated: false
---

# Reference

## Introduction

In this section of docs, you'll find reference material for:

- [Connectors](/reference/connectors/index.mdx) - How to configure and customize them.

- [Metadata](/reference/metadata-reference/index.mdx) - Configuration reference.

- [CLI](/reference/cli/index.mdx) - How to install and use the Hasura DDN CLI.

This section of documentation is designed to be an exhaustive **reference** of Hasura DDN. To see implementation docs
for these concepts, check out the tutorials and guides in sections like
[How to Build With DDN](/how-to-build-with-ddn/overview.mdx).



--- File: ../ddn-docs/docs/reference/pricing.mdx ---
# Plans and Pricing

---
title: Plans and Pricing
sidebar_label: "Plans, Pricing & Billing"
sidebar_position: 10
description:
  "Learn about Hasura DDN Billing and Pricing - Find details on simple, consistent pricing independent of traffic, with
  no extra costs for growing usage. View pricing for models, zones, and data transfers across free, base, and advanced
  plans. Ideal for developers and teams of all sizes."
keywords:
  - Hasura DDN billing
  - Hasura DDN plan
  - Data transfer
  - DDN Free
  - DDN Base
  - DDN Advanced
  - Dedicated
  - Data passthrough
---

import Thumbnail from "@site/src/components/Thumbnail";

# Hasura DDN Plans, Pricing, and Billing

## Overview

[Hasura DDN Pricing](https://hasura.io/pricing/v3) is designed for simplicity with fewer pricing components while
maintaining consistency across different plans and deployment methods. Additionally, this pricing model remains
independent of traffic, meaning that you won't incur higher costs as your traffic loads grow.

:::tip Start your free trial!

The DDN Base Trial Plan and DDN Advanced Trial Plan offer new users the opportunity to experience our **DDN Base Plan**
and **DDN Advanced Plan** free of charge for 30 days.

The trials are activated immediately and are free for 30 days. Once the trial period ends, the account or project plan
will automatically downgrade to the last active non-trial plan (either DDN Free or DDN Base). You will not be charged or
automatically converted to the DDN Base or Advanced plan after the trial expires. These trials are available only once
per user.

During the DDN Base trial, you'll have access to all features, including: unlimited collaboration for your team,
real-time performance metrics and query tracing, model and field analytics, insights for the last 30 days, and schema
registry and tracking.

During the DDN Advanced trial, you'll have access to all DDN Base features and all features of DDN Advanced, including:
modular metadata management, multi-repo CI/CD, continuous validation, and team-based governance. You can upgrade at any
time before the trial expires by adding your credit card, and we will notify you two weeks before your trial period
ends.

This ensures you can explore the full benefits of both plans without any commitment or automatic billing.

[Try it out now!](https://hasura.io/pricing/v3)

:::

## Pricing units and terminology

| Pricing Unit      | Details                                                                                                                                                                                                                                                                                            |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Model             | The number of models for billing purposes maps directly to the sum of [Model](/reference/metadata-reference/models.mdx) and [Command](/reference/metadata-reference/commands.mdx) objects in the metadata. A model or a command can be created for a database table, view, microservice, API, etc. |
| Active Model      | An active model is defined as a model that is accessed more than 1000 times a month. A model is considered to be accessed if it is queried over the GraphQL API either directly or via a relationship                                                                                              |
| Availability Zone | An Availability Zone is a redundant location within a data center where Hasura is running. These typically map to data center locations on major cloud providers.                                                                                                                                  |
| Data Transfer     | Amount of data in [Gibibytes](https://simple.wikipedia.org/wiki/Gibibyte) (GiB) transferred across various channels while running a Hasura instance.                                                                                                                                               |

## Plans and Pricing

See our [pricing page](https://hasura.io/pricing) for more information.

## Hasura DDN Billing {#billing}

With Hasura DDN, you can setup billing in just a few steps. All you need to do is add a credit card to your plan and
you're off to the races!

### Add a payment method

You can manage the payment methods for your Hasura account by navigating to the
[billing section](https://console.hasura.io/billing "go to billing") of the console. You can access this via the menu in
the top-right of the projects page.

<Thumbnail src="/img/billing/billing-option.png" alt="Go to billing" />

#### Step 1: Add a billing address

If you do not have a billing address associated with your user account, please add one in the
[payment methods management screen](https://console.hasura.io/billing/payment-methods "edit billing address").

<Thumbnail src="/img/billing/add-billing-details.png" alt="add billing info" />

#### Step 2: Add the payment details

To add a credit card, navigate to the `Card Management` section and click on `Add Card` to save a new payment method.

#### Step 3: Choose your plan (for an existing project)

1. Click on the project plan button on the header.

   <Thumbnail src="/img/billing/project-plan-header.png" alt="add card" />

2. Choose your project plan.

   <Thumbnail src="/img/billing/select-subscription.png" alt="select plan" />

3. Confirm the plan details. You can also edit your billing details and add/remove payment methods.

   <Thumbnail src="/img/billing/confirm-plan.png" alt="confirm plan" />

4. You should see a message that confirms that your project has been updated ðŸŽ‰

### Edit an existing payment method

To edit an existing payment method, navigate to the
[payment methods management screen](https://console.hasura.io/billing/payment-methods "edit billing address") and manage
your saved payment methods under `Card Management` section.

<Thumbnail src="/img/billing/edit-existing-payment-methods.png" alt="edit card" />



--- File: ../ddn-docs/docs/reference/metadata-reference/index.mdx ---
# Overview

---
sidebar_position: 1
sidebar_label: Overview
description:
  "Understand the power and application of supergraph modeling to create robust Hasura GraphQL APIs. Learn the
  declarative approach for structuring data and establishing API guidelines, making it reader-friendly and highly
  efficient."
keywords:
  - hasura graphql api
  - data structure
  - api development strategy
  - data permissions
  - api design
  - data schema
  - declarative data modeling
  - hasura data api
  - graphql api specifications
hide_table_of_contents: true
seoFrontMatterUpdated: true
---

# Supergraph Modeling

## Introduction

Hasura believes the foundation for constructing your data supergraph should be in modeling it. Your API should be a
reflection of your data, not the other way around.

By focusing on modeling your data sources and business logic, you can create a robust data layer that is easy to
understand and uses a declarative approach to define data models. This empowers you to create a data layer that is
highly efficient and easy to maintain.

## Find out more

- [Working with metadata](/reference/metadata-reference/introduction.mdx)
- [Build configs](/reference/metadata-reference/build-configs.mdx)
- [Data Connector Links](/reference/metadata-reference/data-connector-links.mdx)
- [Types](/reference/metadata-reference/types.mdx)
- [Models](/reference/metadata-reference/models.mdx)
- [Commands](/reference/metadata-reference/commands.mdx)
- [Boolean Expressions](/reference/metadata-reference/boolean-expressions.mdx)
- [OrderBy Expressions](/reference/metadata-reference/orderby-expressions.mdx)
- [Aggregate Expressions](/reference/metadata-reference/aggregate-expressions.mdx)
- [Relationships](/reference/metadata-reference/relationships.mdx)
- [Permissions](/reference/metadata-reference/permissions.mdx)
- [GraphQL API Configuration](/reference/metadata-reference/graphql-config.mdx)
- [Auth Config](/reference/metadata-reference/auth-config.mdx)
- [Compatibility Config](/reference/metadata-reference/compatibility-config.mdx)
- [Engine Plugins](/reference/metadata-reference/engine-plugins.mdx)



--- File: ../ddn-docs/docs/reference/metadata-reference/introduction.mdx ---
# Working with Metadata

---
sidebar_position: 2
sidebar_label: Working with Metadata
description:
  "Explore the enhanced capabilities of supergraph modeling. Understand how it provides an efficient tool for metadata
  generation, data source management, and lays the foundation for building API data domains. Learn the mechanism of data
  connectors and the corresponding specifications, models, commands, relationships, and permissions."
keywords:
  - hasura ddn
  - data connectivity
  - data management
  - api development
  - metadata generation
  - data connectors
  - graphql api
  - data modeling
  - api permissions
seoFrontMatterUpdated: true
---

# Working with Metadata

## What is metadata?

Your Hasura metadata describes every aspect of your supergraph and is used to build it into the GraphQL schema that
defines your API.

This is done by validating and combining the various objects in your metadata, including subgraphs, connectors, models,
commands, relationships, permissions and more.

Metadata is written in Hasura Metadata Language (HML), which is a declarative extension of YAML.

## How is metadata generated?

When you initialize a new local supergraph project, the [DDN CLI](/reference/cli/index.mdx) scaffolds a set of metadata
files and default subgraphs and configurations for you to start building your API.

The CLI is also used to perform many other metadata creation and management operations such as:

- Creating new subgraphs
- Managing the connection to data sources
- Generating models and commands from existing data sources
- Updating models and commands
- Generating relationships

## How is metadata edited?

[The Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) provides syntax
highlighting and auto-complete for your metadata.

The DDN CLI will also validate your metadata when you use it to create a build.

## Next steps

On each page in this section, you'll find detailed information about the various components of Hasura DDN which you can
author and modify in your metadata. Each page will provide you with an overview of the components, how they work, and
examples of how to use them. Below all of this, you'll find detailed reference information about the metadata structure
and the fields that you can use to define each component.



--- File: ../ddn-docs/docs/reference/metadata-reference/build-configs.mdx ---
# Build configs

---
sidebar_label: Build configs
description:
  "Add as many Supergraph objects as you need to define different configurations for building your supergraph and a
  Connector object for each connector in your supergraph."
seoFrontMatterUpdated: true
toc_max_heading_level: 4
sidebar_position: 3
---

# Build configs

## Introduction

Build configs are used to define the resources and other configurations required to build a **supergraph**, **subgraph**
or a **connector**. They are written in YAML and are defined in individual files.

It's helpful to think of build configs as a blueprint for building your supergraph. The following options are available
to you:

| Config type               | Description                                           |
| ------------------------- | ----------------------------------------------------- |
| [Supergraph](#supergraph) | Defines the configuration used to build a supergraph. |
| [Subgraph](#subgraph)     | Defines the configuration used to build a subgraph.   |
| [Connector](#connector)   | Defines the configuration used to build a connector.  |

### How the Supergraph config works {#supergraph}

#### Lifecycle

The [Supergraph](#supergraph-supergraph) object defines the configuration used to build the supergraph. While projects
are generated with default configs for building for local or Hasura DDN
[when initializing a new supergraph](/reference/cli/commands/ddn_supergraph_init.mdx), you can add as many `Supergraph`
objects as you need to define different configurations for building your supergraph.

You can then use these files to build your supergraph â€” either locally or on Hasura DDN â€”
[using the CLI](/reference/cli/commands/ddn_supergraph_build_local.mdx). These builds â€” when on Hasura DDN â€” can then be
[applied to a project](/reference/cli/commands/ddn_supergraph_build_apply.mdx) which will then serve the API using the
supergraph built from the configuration.

#### Examples

```yaml title="A supergraph.yaml config file:"
kind: Supergraph
version: v2
definition:
  subgraphs:
    - globals/subgraph.yaml
    - app/subgraph.yaml
```

| **Field**              | **Description**                                                                      | **Reference**                                                |
| ---------------------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------ |
| `kind`                 | Specifies the type of configuration, in this case, `Supergraph`.                     | [Supergraph](#supergraph-supergraph)                         |
| `version`              | The version of the `Supergraph` configuration, which is `v2`.                        | [Supergraph](#supergraph-supergraph)                         |
| `definition.subgraphs` | Paths to the subgraph configuration files that are included in the supergraph build. | [SupergraphDefinitionV2](#supergraph-supergraphdefinitionv2) |

### How the Subgraph config works {#subgraph}

#### Lifecycle

**Each** subgraph in your supergraph has its own config. The [Subgraph](#subgraph-subgraph) object defines the
configuration used to build the subgraph. While subgraphs are generated with a default config
[when initializing a new subgraph](/reference/cli/commands/ddn_subgraph_init.mdx), you can add as many `Subgraph`
objects as you need to define different configurations for building a subgraph.

You can then use these files to build a subgraph [using the CLI](/reference/cli/commands/ddn_subgraph_build_create.mdx).
These subgraph builds can then be [applied to a project](/reference/cli/commands/ddn_subgraph_build_apply.mdx).

#### Examples

```yaml title="A supergraph.yaml config file:"
kind: Subgraph
version: v2
definition:
  name: app
  generator:
    rootPath: .
  includePaths:
    - metadata
  envMapping:
    APP_MY_CONNECTOR_AUTHORIZATION_HEADER:
      fromEnv: APP_MY_CONNECTOR_AUTHORIZATION_HEADER
    APP_MY_CONNECTOR_READ_URL:
      fromEnv: APP_MY_CONNECTOR_READ_URL
    APP_MY_CONNECTOR_WRITE_URL:
      fromEnv: APP_MY_CONNECTOR_WRITE_URL
  connectors:
    - path: connector/my_connector/connector.yaml
      connectorLinkName: my_connector
```

| **Field**                                 | **Description**                                                                                                                                                                                                                                                    | **Reference**                                                |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------ |
| `kind`                                    | Specifies the type of configuration, in this case, `Subgraph`.                                                                                                                                                                                                     | [Subgraph](#subgraph-subgraph)                               |
| `version`                                 | The version of the `Subgraph` configuration, which is `v2`.                                                                                                                                                                                                        | [Subgraph](#subgraph-subgraph)                               |
| `definition.name`                         | The name of the subgraph.                                                                                                                                                                                                                                          | [SubgraphDefinitionV2](#subgraph-subgraphdefinitionv2)       |
| `definition.generator.rootPath`           | Path to the directory containing all the subgraph metadata, in this case the current directory.                                                                                                                                                                    | [SubgraphGeneratorConfig](#subgraph-subgraphgeneratorconfig) |
| `definition.includePaths`                 | Paths to be included to construct the subgraph metadata.                                                                                                                                                                                                           | [SubgraphDefinitionV2](#subgraph-subgraphdefinitionv2)       |
| `definition.envMapping`                   | Environment variable mapping configuration. Typically, these will correspond to connector envs. Additionally, when you [initialize a connector](/reference/cli/commands/ddn_connector_init.mdx), the CLI will automatically add the required envs to the subgraph. | [EnvMapping](#subgraph-envmapping)                           |
| `definition.connectors.path`              | Path to the connector configuration file used in the subgraph.                                                                                                                                                                                                     | [SubgraphConnector](#subgraph-subgraphconnector)             |
| `definition.connectors.connectorLinkName` | Name of the connector link associated with the connector.                                                                                                                                                                                                          | [SubgraphConnector](#subgraph-subgraphconnector)             |

### How the Connector config works {#connector}

#### Lifecycle

**Each** [data connector](/data-sources/overview.mdx) in your supergraph has its own config. The
[Connector](#connector-connector) object defines the configuration used to build the connector. This allows you to
configure the capabilities of the connector and tailor it to your needs. While connectors are initialized with default
configs for building for local or Hasura DDN, you can add as many `Connector` objects as you need to define different
configurations for building your connector.

When you [initialize a connector](/reference/cli/commands/ddn_connector_init.mdx), the CLI will automatically create a
connector config file for you. You can then use this file to build your connector
[using the CLI](/reference/cli/commands/ddn_connector_build_create.mdx).

#### Examples

```yaml title="A connector.yaml config file:"
kind: Connector
version: v2
definition:
  name: MY_CONNECTOR
  subgraph: app
  source: hasura/postgres:v1.1.1
  context: .
  envMapping:
    CONNECTION_URI:
      fromEnv: APP_MY_CONNECTOR_CONNECTION_URI
    HASURA_SERVICE_TOKEN_SECRET:
      fromEnv: APP_MY_CONNECTOR_HASURA_SERVICE_TOKEN_SECRET
    OTEL_EXPORTER_OTLP_TRACES_ENDPOINT:
      fromEnv: APP_MY_CONNECTOR_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
    OTEL_SERVICE_NAME:
      fromEnv: APP_MY_CONNECTOR_OTEL_SERVICE_NAME
```

| **Field**               | **Description**                                                     | **Reference**                                             |
| ----------------------- | ------------------------------------------------------------------- | --------------------------------------------------------- |
| `kind`                  | Specifies the type of configuration, in this case, `Connector`.     | [Connector](#connector-connector)                         |
| `version`               | The version of the `Connector` configuration, which is `v2`.        | [Connector](#connector-connector)                         |
| `definition.name`       | The name of the connector.                                          | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.subgraph`   | The name of the DDN project subgraph associated with the connector. | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.source`     | The versioned source of the connector.                              | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.context`    | The path to the context directory used in the connector build.      | [ConnectorDefinitionV2](#connector-connectordefinitionv2) |
| `definition.envMapping` | Environment variable mapping configuration for the connector.       | [EnvMapping](#connector-envmapping)                       |

---

## Metadata structure


### Supergraph {#supergraph-supergraph}

Defines the configuration used to build the Supergraph.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Supergraph` | true |  |
| `version` | `v2` | true |  |
| `definition` | [SupergraphDefinitionV2](#supergraph-supergraphdefinitionv2) | true |  |



#### SupergraphDefinitionV2 {#supergraph-supergraphdefinitionv2}

Supergraph Definition V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `subgraphs` | [string] | true | Paths to subgraph configuration. |

### Subgraph {#subgraph-subgraph}

Defines the configuration used to build the Subgraph.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Subgraph` | true |  |
| `version` | `v2` | true |  |
| `definition` | [SubgraphDefinitionV2](#subgraph-subgraphdefinitionv2) | true |  |



#### SubgraphDefinitionV2 {#subgraph-subgraphdefinitionv2}

Subgraph Definition V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | string | true | Subgraph Name. |
| `generator` | [SubgraphGeneratorConfig](#subgraph-subgraphgeneratorconfig) | true | Subgraph generator Configuration. |
| `envFile` | string | false | Path to the Subgraph .env file. |
| `includePaths` | [string] | true | Paths to be included to construct Subgraph metadata. |
| `envMapping` | [EnvMapping](#subgraph-envmapping) | false | Environment Variable mapping config. |
| `connectors` | [[SubgraphConnector](#subgraph-subgraphconnector)] | false | Connectors used in subgraph. |



#### SubgraphConnector {#subgraph-subgraphconnector}

Subgraph Connector config.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `path` | string | true | Path to connector config file. |
| `connectorLinkName` | string | false | Name of connector link associated with the connector. |



#### EnvMapping {#subgraph-envmapping}

Environment Variables mapping config.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvSource](#subgraph-envsource) | false | Target Environment variable. |



#### EnvSource {#subgraph-envsource}

Environment Variable Source.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fromEnv` | string | true | Source Environment variable. |



#### SubgraphGeneratorConfig {#subgraph-subgraphgeneratorconfig}

Subgraph generator Configuration.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootPath` | string | true | Path to the directory which holds all the Subgraph metadata. |
| `graphqlRootFieldPrefix` | string | false | Prefix to use while generating GraphQL root fields. |
| `graphqlTypeNamePrefix` | string | false | Prefix to use while generating GraphQL type names. |
| `namingConvention` | `none` / `graphql` | false | Naming convention to use while generating GraphQL fields and types. |

### Connector {#connector-connector}

Defines the configuration used to build the connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Connector` | true |  |
| `version` | `v2` | true |  |
| `definition` | [ConnectorDefinitionV2](#connector-connectordefinitionv2) | true |  |



#### ConnectorDefinitionV2 {#connector-connectordefinitionv2}

Connector deployment definition V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | string | true | Connector name. |
| `subgraph` | string | false | DDN project subgraph name. |
| `source` | string | true | Connector Hub ID. |
| `context` | string | true | Path to the context directory. |
| `envFile` | string | false | Path to the shared .env file. |
| `envMapping` | [EnvMapping](#connector-envmapping) | false | Environment Variable mapping config. |
| `regionConfiguration` | [[RegionConfigurationV2](#connector-regionconfigurationv2)] | false | Connector deployment Region configuration |
| `resources` | [Resources](#connector-resources) | false | Connector deployment Resources. |



#### RegionConfigurationV2 {#connector-regionconfigurationv2}

Connector deployment Region Configuration V2.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `region` | string | true | Region to deploy the connector to. |
| `mode` | `ReadOnly` / `ReadWrite` | true | Connector deployment mode. |
| `envMapping` | [EnvMapping](#connector-envmapping) | false | Environment Variable mapping config. |
| `resources` | [Resources](#connector-resources) | false | Connector deployment Resources. |



#### Resources {#connector-resources}

Connector deployment Resources.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `memory` | string | false | Connector deployment memory resource in bytes. Accepted units: k, M, G. Example: 128M, 1G |
| `cpu` | string | false | Connector deployment cpu resource in cores. Example: 1, 1.5 |



#### EnvMapping {#connector-envmapping}

Environment Variables mapping config.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvSource](#connector-envsource) | false | Target Environment variable. |



#### EnvSource {#connector-envsource}

Environment Variable Source.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fromEnv` | string | true | Source Environment variable. |



--- File: ../ddn-docs/docs/reference/metadata-reference/data-connector-links.mdx ---
# Data Connector Links

---
sidebar_position: 4
sidebar_label: Data Connector Links
description:
  "Docs for the configuration of data connectors in Hasura. Learn how to connect to SQL & NoSQL databases, REST APIs,
  GraphQL API and more by specifying a connector URL and schema. Increase flexibility and efficiency by defining schema
  types, leveraging atomic builds, and implementing structured data model configuration."
keywords:
  - hasura
  - data connector url
  - data connector headers
  - data connector schema
  - graphql api configuration
  - rest api configuration
  - database connectivity
  - nosql databases
  - sql databases
toc_max_heading_level: 4
seoFrontMatterUpdated: true
---

# Data Connector Links

## Introduction

A `DataConnectorLink` is used to specify the URLs and NDC schema of a [data connector](/data-sources/overview.mdx)
allowing to link it to [models](/reference/metadata-reference/models.mdx) and
[commands](/reference/metadata-reference/commands.mdx). It can be used to connect to various types of data connectors on
different data sources, like SQL databases, NoSQL databases, REST APIs, GraphQL APIs, files, and more.

## How DataConnectorLinks work

### Lifecycle

A `DataConnectorLink` can be [created using the CLI](/reference/cli/commands/ddn_connector-link_add.mdx). Out of
convenience, the CLI will scaffold this file automatically for you when
[initializing a new connector](/reference/cli/commands/ddn_connector_init.mdx).

A `DataConnectorLink` belongs to a single [subgraph's](/project-configuration/subgraphs/index.mdx) data source. It is
used to link the data source to the subgraph's models, commands, and relationships. The contents can be
[updated using the CLI](/reference/cli/commands/ddn_connector-link_update.mdx). This will introspect the data source and
update the schema of the data connector.

Any time your data source schema changes, you should update the `DataConnectorLink` to reflect those changes. This will
ensure that the schema of the data connector is up to date and that the data connector can be used to serve requests.

This configuration is then used to [generate the metadata](/reference/cli/commands/ddn_connector-link_add-resources.mdx)
representing collections present in the data source.

:::info Be more granular

The example we linked above is for adding all models, commands, and relationships present in a data source. However, you
can add each resource individually after updating the `DataConnectorLink` configuration:

- [Add models](/reference/cli/commands/ddn_model_add.mdx)
- [Add commands](/reference/cli/commands/ddn_command_add.mdx)
- [Add relationships](/reference/cli/commands/ddn_relationship_add.mdx)

:::

To make a new data connector link and it's cascading metadata available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI after adding your resources.

### Examples

```yaml title="A sample DataConnectorLink:"
kind: DataConnectorLink
version: v1
definition:
  name: data_connector
  url:
    singleUrl:
      value: http://data_connector:8100
  headers: {}
  schema:
    version: v0.1
    schema:
      scalar_types: {}
      object_types: {}
      collections: []
      functions: []
      procedures: []
    capabilities:
      version: 0.1.3
      capabilities:
        query:
          nested_fields: {}
          variables: {}
        mutation: {}
```

| **Field**                                              | **Description**                                                                                       | **Reference**                                                           |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| `kind`                                                 | Specifies the type of configuration, in this case, DataConnectorLink.                                 | [DataConnectorLink](#dataconnectorlink-dataconnectorlink)               |
| `version`                                              | The version of the DataConnectorLink configuration, which is `v1`.                                    | [DataConnectorLinkV1](#dataconnectorlink-dataconnectorlinkv1)           |
| `definition.name`                                      | The name given to this data connector configuration.                                                  | [DataConnectorName](#dataconnectorlink-dataconnectorname)               |
| `definition.url.singleUrl.value`                       | The URL used to access the data connector.                                                            | [DataConnectorUrlV1](#dataconnectorlink-dataconnectorurlv1)             |
| `definition.headers`                                   | A key-value map of HTTP headers to be sent with each request to the data connector.                   | [HttpHeaders](#dataconnectorlink-httpheaders)                           |
| `definition.schema.version`                            | The version of the schema that the data connector is using.                                           | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.schema[]`                           | The schema of the data connector, representing various types, collections, functions, and procedures. | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.capabilities.version`               | The version of the capabilities that the data connector supports.                                     | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.capabilities.capabilities.query`    | The query capabilities of the data connector.                                                         | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |
| `definition.schema.capabilities.capabilities.mutation` | The mutation capabilities of the data connector.                                                      | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) |

---

## Metadata structure


### DataConnectorLink {#dataconnectorlink-dataconnectorlink}

Definition of a data connector, used to bring in sources of data and connect them to OpenDD models and commands.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `DataConnectorLink` | true |  |
| `version` | `v1` | true |  |
| `definition` | [DataConnectorLinkV1](#dataconnectorlink-dataconnectorlinkv1) | true | Definition of a data connector - version 1. |

 **Example:**

```yaml
kind: DataConnectorLink
version: v1
definition:
  name: data_connector
  url:
    singleUrl:
      value: http://data_connector:8100
  headers: {}
  schema:
    version: v0.1
    schema:
      scalar_types: {}
      object_types: {}
      collections: []
      functions: []
      procedures: []
    capabilities:
      version: 0.1.3
      capabilities:
        query:
          nested_fields: {}
          variables: {}
        mutation: {}
```


#### DataConnectorLinkV1 {#dataconnectorlink-dataconnectorlinkv1}

Definition of a data connector - version 1.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [DataConnectorName](#dataconnectorlink-dataconnectorname) | true | The name of the data connector. |
| `url` | [DataConnectorUrlV1](#dataconnectorlink-dataconnectorurlv1) | true | The url(s) to access the data connector. |
| `headers` | [HttpHeaders](#dataconnectorlink-httpheaders) | false | Key value map of HTTP headers to be sent with each request to the data connector. This is meant for protocol level use between engine and the data connector. |
| `schema` | [SchemaAndCapabilitiesV01](#dataconnectorlink-schemaandcapabilitiesv01) | true | The schema of the data connector. This schema is used as the source of truth when serving requests and the live schema of the data connector is not looked up. |
| `argumentPresets` | [[DataConnectorArgumentPreset](#dataconnectorlink-dataconnectorargumentpreset)] | false | Argument presets that applies to all functions and procedures of this data connector. Defaults to no argument presets. |
| `responseHeaders` | [ResponseHeaders](#dataconnectorlink-responseheaders) / null | false | HTTP response headers configuration that is forwarded from a data connector to the client. |



#### ResponseHeaders {#dataconnectorlink-responseheaders}

Configuration of what HTTP response headers should be forwarded from a data connector to the client in HTTP response.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headersField` | [DataConnectorColumnName](#dataconnectorlink-dataconnectorcolumnname) | true | Name of the field in the NDC function/procedure's result which contains the response headers |
| `resultField` | [DataConnectorColumnName](#dataconnectorlink-dataconnectorcolumnname) | true | Name of the field in the NDC function/procedure's result which contains the result |
| `forwardHeaders` | [string] | true | List of actual HTTP response headers from the data connector to be set as response headers |



#### DataConnectorColumnName {#dataconnectorlink-dataconnectorcolumnname}

The name of a column in a data connector.


**Value:** string


#### DataConnectorArgumentPreset {#dataconnectorlink-dataconnectorargumentpreset}

An argument preset that can be applied to all functions/procedures of a connector

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [DataConnectorArgumentName](#dataconnectorlink-dataconnectorargumentname) | true | The name of an argument as defined by a data connector. |
| `value` | [DataConnectorArgumentPresetValue](#dataconnectorlink-dataconnectorargumentpresetvalue) | true | The value of a data connector argument preset. |



#### DataConnectorArgumentPresetValue {#dataconnectorlink-dataconnectorargumentpresetvalue}

The value of a data connector argument preset.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `httpHeaders` | [HttpHeadersPreset](#dataconnectorlink-httpheaderspreset) | true | HTTP headers that can be preset from request |



#### HttpHeadersPreset {#dataconnectorlink-httpheaderspreset}

Configuration of what HTTP request headers should be forwarded to a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `forward` | [string] | true | List of HTTP headers that should be forwarded from HTTP requests |
| `additional` | [AdditionalHttpHeaders](#dataconnectorlink-additionalhttpheaders) | true | Additional headers that should be forwarded, from other contexts |



#### AdditionalHttpHeaders {#dataconnectorlink-additionalhttpheaders}

Key value map of HTTP headers to be forwarded in the headers argument of a data connector request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [ValueExpression](#dataconnectorlink-valueexpression) | false |  |



#### ValueExpression {#dataconnectorlink-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false |  |
| `valueFromEnv` | string | false |  |



#### DataConnectorArgumentName {#dataconnectorlink-dataconnectorargumentname}

The name of an argument as defined by a data connector.


**Value:** string


#### SchemaAndCapabilitiesV01 {#dataconnectorlink-schemaandcapabilitiesv01}

Version 0.1 of schema and capabilities for a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `version` | `v0.1` | true |  |
| `schema` | [Schema Response](https://hasura.github.io/ndc-spec/specification/schema/index.html) | true |  |
| `capabilities` | [Capabilities Response](https://hasura.github.io/ndc-spec/specification/capabilities.html) | true |  |



#### HttpHeaders {#dataconnectorlink-httpheaders}

Key value map of HTTP headers to be sent with an HTTP request. The key is the header name and the value is a potential reference to an environment variable.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | false |  |



#### DataConnectorUrlV1 {#dataconnectorlink-dataconnectorurlv1}

A URL to access a data connector. This can be a single URL or a pair of read and write URLs.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `singleUrl` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | false |  |
| `readWriteUrls` | [ReadWriteUrls](#dataconnectorlink-readwriteurls) | false | A pair of URLs to access a data connector, one for reading and one for writing. |



#### ReadWriteUrls {#dataconnectorlink-readwriteurls}

A pair of URLs to access a data connector, one for reading and one for writing.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `read` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | true |  |
| `write` | [EnvironmentValue](#dataconnectorlink-environmentvalue) | true |  |



#### EnvironmentValue {#dataconnectorlink-environmentvalue}

Either a literal string or a reference to a Hasura secret


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | string | false |  |
| `valueFromEnv` | string | false |  |



#### DataConnectorName {#dataconnectorlink-dataconnectorname}

The name of a data connector.


**Value:** string
### DataConnectorScalarRepresentation {#dataconnectorscalarrepresentation-dataconnectorscalarrepresentation}

The representation of a data connector scalar in terms of Open DD types

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `DataConnectorScalarRepresentation` | true |  |
| `version` | `v1` | true |  |
| `definition` | [DataConnectorScalarRepresentationV1](#dataconnectorscalarrepresentation-dataconnectorscalarrepresentationv1) | true | The representation of a data connector scalar in terms of Open DD types. Deprecated in favour of `BooleanExpressionType`. |



#### DataConnectorScalarRepresentationV1 {#dataconnectorscalarrepresentation-dataconnectorscalarrepresentationv1}

The representation of a data connector scalar in terms of Open DD types. Deprecated in favour of `BooleanExpressionType`.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#dataconnectorscalarrepresentation-dataconnectorname) | true | The name of the data connector that this scalar type comes from. |
| `dataConnectorScalarType` | [DataConnectorScalarType](#dataconnectorscalarrepresentation-dataconnectorscalartype) | true | The name of the scalar type coming from the data connector. |
| `representation` | [TypeName](#dataconnectorscalarrepresentation-typename) | true | The name of the Open DD type that this data connector scalar type should be represented as. |
| `graphql` | [DataConnectorScalarGraphQLConfiguration](#dataconnectorscalarrepresentation-dataconnectorscalargraphqlconfiguration) / null | false | Configuration for how this scalar's operators should appear in the GraphQL schema. |

 **Example:**

```yaml
dataConnectorName: data_connector
dataConnectorScalarType: varchar
representation: String
graphql:
  comparisonExpressionTypeName: String_Comparison_Exp
```


#### DataConnectorScalarGraphQLConfiguration {#dataconnectorscalarrepresentation-dataconnectorscalargraphqlconfiguration}

GraphQL configuration of a data connector scalar

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `comparisonExpressionTypeName` | [GraphQlTypeName](#dataconnectorscalarrepresentation-graphqltypename) / null | false |  |



#### GraphQlTypeName {#dataconnectorscalarrepresentation-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### TypeName {#dataconnectorscalarrepresentation-typename}

The name of the Open DD type that this data connector scalar type should be represented as.


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#dataconnectorscalarrepresentation-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#dataconnectorscalarrepresentation-customtypename) |  |



#### CustomTypeName {#dataconnectorscalarrepresentation-customtypename}

The name of a user-defined type.


**Value:** string


#### InbuiltType {#dataconnectorscalarrepresentation-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### DataConnectorScalarType {#dataconnectorscalarrepresentation-dataconnectorscalartype}

The name of a scalar type in a data connector.


**Value:** string


#### DataConnectorName {#dataconnectorscalarrepresentation-dataconnectorname}

The name of a data connector.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/types.mdx ---
# Types

---
sidebar_position: 5
sidebar_label: Types
description:
  "Explore types in Hasura, discover primitive types, custom types and container types. Learn how to define your data
  structure and map existing data connector scalars to types in your data domain. Understand how these elements form the
  foundation of your data, allowing for flexibility and seamless interconnection."
keywords:
  - hasura documentation
  - graphql data structure
  - data domain mapping
  - hasura ddn
  - hasura data specification
  - graphql schema
  - data connector scalars
  - primitive types
  - custom types
toc_max_heading_level: 4
seoFrontMatterUpdated: true
---

# Types

## Introduction

Types serve as the fundamental elements that define the structure of your data.

Being able to define types in your data domain is beneficial because it provides you with the flexibility to define them
separately from the types referenced by a data connector's source.

The specification employs a concrete type system that includes both primitive and user-defined types. All subsequent
layers, such as [models](/reference/metadata-reference/models.mdx),
[commands](/reference/metadata-reference/commands.mdx), and
[relationships](/reference/metadata-reference/relationships.mdx) are defined in terms of these types.

The types can be one of the following:

| Type            | Description                                                                                              |
| --------------- | -------------------------------------------------------------------------------------------------------- |
| Primitive       | These are the basic types `ID`, `Int`, `Float`, `Boolean`, or `String`.                                  |
| Custom          | These are user-defined types, such as ScalarType or ObjectType.                                          |
| Type References | When specifying the types of a field or an argument, you can mark them as required `!` or repeated `[]`. |

The spec also allows you to map existing data connector scalars to types in your data domain.

:::info Primitive types and type references

Primitive types supported are `ID`, `Int`, `Float`, `Boolean` and `String`.

Type references follow [GraphQL type syntax](https://spec.graphql.org/June2018/#sec-Combining-List-and-Non-Null). Fields
and arguments are nullable by default. To represent non-nullability, specify a `!` after the type name. Similarly, array
fields and arguments are wrapped in `[]`.

:::

## How types work

### Lifecycle

Typically, types will be generated from the data connector schema when you
[introspect a data connector](/reference/cli/commands/ddn_connector_introspect.mdx). You can also define custom types to
represent data that doesn't exist in the data connector.

Further, you can define custom types by either aliasing existing types (such as primitives or custom), or you can define
a type with fields. In turn, the fields themselves can be a primitive or another custom type.

Type references are types of fields and arguments that refer to other primitive or custom types and which can be marked
as nullable, required or repeated (in the case of arrays).

To make a new types available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

#### ScalarType

```yaml title="A sample scalar type definition:"
kind: ScalarType
version: v1
definition:
  name: Uuid
  graphql:
    typeName: Uuid
```

| **Field**                     | **Description**                                                                 | **Reference**                                                                |
| ----------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `kind`                        | Indicates that this configuration is for a custom scalar type.                  | [ScalarType](#scalartype-scalartype)                                         |
| `version`                     | The version of the ScalarType configuration.                                    | [ScalarTypeV1](#scalartype-scalartypev1)                                     |
| `definition.name`             | The unique name for your custom scalar type, used throughout your project.      | [CustomTypeName](#scalartype-customtypename)                                 |
| `definition.graphql.typeName` | The name to use for this scalar type in your GraphQL schema.                    | [ScalarTypeGraphQLConfiguration](#scalartype-scalartypegraphqlconfiguration) |
| `definition.description`      | An optional description of this scalar type that appears in the GraphQL schema. | [ScalarTypeV1](#scalartype-scalartypev1)                                     |

#### ObjectType

```yaml title="A sample object type definition:"
kind: ObjectType
version: v1
definition:
  name: CartItems
  fields:
    - name: cartId
      type: Uuid!
    - name: createdAt
      type: Timestamptz
    - name: id
      type: Uuid!
    - name: productId
      type: Uuid!
    - name: quantity
      type: Int4!
    - name: updatedAt
      type: Timestamptz
  graphql:
    typeName: CartItems
    inputTypeName: CartItemsInput
  dataConnectorTypeMapping:
    - dataConnectorName: my_pg
      dataConnectorObjectType: cart_items
      fieldMapping:
        cartId:
          column:
            name: cart_id
        createdAt:
          column:
            name: created_at
        id:
          column:
            name: id
        productId:
          column:
            name: product_id
        quantity:
          column:
            name: quantity
        updatedAt:
          column:
            name: updated_at
```

| **Field**                                              | **Description**                                                                                                   | **Reference**                                                                |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `kind`                                                 | Indicates that this configuration is for a custom object type.                                                    | [ObjectType](#objecttype-objecttype)                                         |
| `version`                                              | The version of the ObjectType configuration.                                                                      | [ObjectTypeV1](#objecttype-objecttypev1)                                     |
| `definition.name`                                      | The unique name for your custom object type, used throughout your project.                                        | [CustomTypeName](#objecttype-customtypename)                                 |
| `definition.fields[].name`                             | The name of a field within your object type.                                                                      | [FieldName](#objecttype-fieldname)                                           |
| `definition.fields[].type`                             | The data type for the field, which can be a primitive, custom type, or a type reference (e.g., `Uuid!`, `Int4!`). | [TypeReference](#objecttype-typereference)                                   |
| `definition.fields[].description`                      | An optional description of this field that appears in the GraphQL schema.                                         | [ObjectFieldDefinition](#objecttype-objectfielddefinition)                   |
| `definition.graphql.typeName`                          | The name to use for this object type in your GraphQL schema.                                                      | [ObjectTypeGraphQLConfiguration](#objecttype-objecttypegraphqlconfiguration) |
| `definition.graphql.inputTypeName`                     | The name to use for this object type in input operations within your GraphQL schema.                              | [ObjectTypeGraphQLConfiguration](#objecttype-objecttypegraphqlconfiguration) |
| `definition.dataConnectorTypeMapping[]`                | The mapping of data connector object types to your object type.                                                   | [DataConnectorTypeMapping](#objecttype-dataconnectortypemapping)             |
| `definition.dataConnectorTypeMapping[].fieldMapping[]` | The mapping of fields in your object type to columns in a data connector.                                         | [ObjectTypeFieldMappings](#objecttype-fieldmappings)                         |

---

## Metadata structure


### ScalarType {#scalartype-scalartype}

Definition of a user-defined scalar type that that has opaque semantics.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ScalarType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ScalarTypeV1](#scalartype-scalartypev1) | true | Definition of a user-defined scalar type that that has opaque semantics. |

 **Example:**

```yaml
kind: ScalarType
version: v1
name: CustomString
graphql:
  typeName: CustomString
description: A custom string type
```


#### ScalarTypeV1 {#scalartype-scalartypev1}

Definition of a user-defined scalar type that that has opaque semantics.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#scalartype-customtypename) | true | The name to give this scalar type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `graphql` | [ScalarTypeGraphQLConfiguration](#scalartype-scalartypegraphqlconfiguration) / null | false | Configuration for how this scalar type should appear in the GraphQL schema. |
| `description` | string / null | false | The description of this scalar. Gets added to the description of the scalar's definition in the graphql schema. |



#### ScalarTypeGraphQLConfiguration {#scalartype-scalartypegraphqlconfiguration}

GraphQL configuration of an Open DD scalar type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#scalartype-graphqltypename) | true | The name of the GraphQl type to use for this scalar. |



#### GraphQlTypeName {#scalartype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### CustomTypeName {#scalartype-customtypename}

The name of a user-defined type.


**Value:** string
### ObjectType {#objecttype-objecttype}

Definition of a user-defined Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ObjectType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ObjectTypeV1](#objecttype-objecttypev1) | true | Definition of a user-defined Open DD object type. |

 **Example:**

```yaml
kind: ObjectType
version: v1
definition:
  name: Author
  fields:
    - name: author_id
      type: Int!
      description: The id of the author
    - name: first_name
      type: String
      description: The first name of the author
    - name: last_name
      type: String
      description: The last name of the author
    - name: biography
      type: String
      description: AI generated biography for the author
      arguments:
        - name: ai_model
          argumentType: String!
          description: The AI model to use for generating the biography
  description: An author of a book
  globalIdFields:
    - author_id
  graphql:
    typeName: Author
  dataConnectorTypeMapping:
    - dataConnectorName: my_db
      dataConnectorObjectType: author
      fieldMapping:
        author_id:
          column:
            name: id
    - dataConnectorName: my_vector_db
      dataConnectorObjectType: author
      fieldMapping:
        biography:
          column:
            name: biography
            argumentMapping:
              ai_model: model
```


#### ObjectTypeV1 {#objecttype-objecttypev1}

Definition of a user-defined Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#objecttype-customtypename) | true | The name to give this object type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `fields` | [[ObjectFieldDefinition](#objecttype-objectfielddefinition)] | true | The list of fields defined for this object type. |
| `globalIdFields` | [[FieldName](#objecttype-fieldname)] / null | false | The subset of fields that uniquely identify this object in the domain. Setting this property will automatically implement the GraphQL Relay Node interface for this object type and add an `id` global ID field. If setting this property, there must not be a field named `id` already present. |
| `graphql` | [ObjectTypeGraphQLConfiguration](#objecttype-objecttypegraphqlconfiguration) / null | false | Configuration for how this object type should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the object. Gets added to the description of the object's definition in the graphql schema. |
| `dataConnectorTypeMapping` | [[DataConnectorTypeMapping](#objecttype-dataconnectortypemapping)] | false | Mapping of this object type to corresponding object types in various data connectors. |



#### DataConnectorTypeMapping {#objecttype-dataconnectortypemapping}

This defines the mapping of the fields of an object type to the corresponding columns of an object type in a data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#objecttype-dataconnectorname) | true |  |
| `dataConnectorObjectType` | [DataConnectorObjectType](#objecttype-dataconnectorobjecttype) | true |  |
| `fieldMapping` | [FieldMappings](#objecttype-fieldmappings) | false |  |



#### FieldMappings {#objecttype-fieldmappings}

Mapping of object fields to their source columns in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [FieldMapping](#objecttype-fieldmapping) | false |  |



#### FieldMapping {#objecttype-fieldmapping}

Source field directly maps to some column in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `column` | [ColumnFieldMapping](#objecttype-columnfieldmapping) | true | The target column in a data connector object that a source field maps to. |



#### ColumnFieldMapping {#objecttype-columnfieldmapping}

The target column in a data connector object that a source field maps to.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [DataConnectorColumnName](#objecttype-dataconnectorcolumnname) | true | The name of the target column |
| `argumentMapping` | [ArgumentMapping](#objecttype-argumentmapping) / null | false | Arguments to the column field |



#### ArgumentMapping {#objecttype-argumentmapping}

Mapping of a comand or model argument name to the corresponding argument name used in the data connector. The key of this object is the argument name used in the command or model and the value is the argument name used in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [DataConnectorArgumentName](#objecttype-dataconnectorargumentname) | false |  |



#### DataConnectorArgumentName {#objecttype-dataconnectorargumentname}

The name of an argument as defined by a data connector.


**Value:** string


#### DataConnectorColumnName {#objecttype-dataconnectorcolumnname}

The name of a column in a data connector.


**Value:** string


#### DataConnectorObjectType {#objecttype-dataconnectorobjecttype}

The name of an object type in a data connector.


**Value:** string


#### DataConnectorName {#objecttype-dataconnectorname}

The name of a data connector.


**Value:** string


#### ObjectTypeGraphQLConfiguration {#objecttype-objecttypegraphqlconfiguration}

GraphQL configuration of an Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#objecttype-graphqltypename) / null | false | The name to use for the GraphQL type representation of this object type when used in an output context. |
| `inputTypeName` | [GraphQlTypeName](#objecttype-graphqltypename) / null | false | The name to use for the GraphQL type representation of this object type when used in an input context. |
| `apolloFederation` | [ObjectApolloFederationConfig](#objecttype-objectapollofederationconfig) / null | false | Configuration for exposing apollo federation related types and directives. |



#### ObjectApolloFederationConfig {#objecttype-objectapollofederationconfig}

Configuration for apollo federation related types and directives.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `keys` | [[ApolloFederationObjectKey](#objecttype-apollofederationobjectkey)] | true |  |



#### ApolloFederationObjectKey {#objecttype-apollofederationobjectkey}

The definition of a key for an apollo federation object.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fields` | [[FieldName](#objecttype-fieldname)] | true |  |



#### GraphQlTypeName {#objecttype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### ObjectFieldDefinition {#objecttype-objectfielddefinition}

The definition of a field in a user-defined object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [FieldName](#objecttype-fieldname) | true | The name of the field. This name is used both when referring to the field elsewhere in the metadata and when creating the corresponding GraphQl type. |
| `type` | [TypeReference](#objecttype-typereference) | true | The type of this field. This uses the GraphQL syntax to represent field types and must refer to one of the inbuilt OpenDd types or another user-defined type. |
| `description` | string / null | false | The description of this field. Gets added to the description of the field's definition in the graphql schema. |
| `deprecated` | [Deprecated](#objecttype-deprecated) / null | false | Whether this field is deprecated. If set, the deprecation status is added to the field's graphql schema. |
| `arguments` | [[FieldArgumentDefinition](#objecttype-fieldargumentdefinition)] | false | The arguments for the field |



#### FieldArgumentDefinition {#objecttype-fieldargumentdefinition}

The definition of an argument for a field in a user-defined object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ArgumentName](#objecttype-argumentname) | true |  |
| `argumentType` | [TypeReference](#objecttype-typereference) | true |  |
| `description` | string / null | false |  |



#### ArgumentName {#objecttype-argumentname}

The name of an argument.


**Value:** string


#### Deprecated {#objecttype-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### TypeReference {#objecttype-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### FieldName {#objecttype-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#objecttype-customtypename}

The name of a user-defined type.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/models.mdx ---
# Models

---
sidebar_position: 6
sidebar_label: Models
description:
  "Explore the concept of Models in Hasura Data Domain, your link between the data sources and the generated API. Learn
  about models, their structure, source configuration, and how they appear in the GraphQL API. Understand filterable
  fields, orderable fields, data connector types, and how to configure them. Gain insights on unique identifiers and
  their role in retrieving unique data objects from the model."
toc_max_heading_level: 4
keywords:
  - hasura models
  - data domain models
  - hasura api
  - graphql api configuration
  - source configuration
  - filterable fields
  - orderable fields
  - data connectors
  - unique identifiers
seoFrontMatterUpdated: true
---

# Models

## Introduction

In Hasura DDN, a **model** represents a collection of data objects within a data domain. Models act as the foundational
elements (or "nouns") that define the data structure and behavior in your API. They can be backed by various data
sources such as database tables, custom SQL queries, materialized views, or even external REST or GraphQL APIs.

Models bridge the gap between [data connectors](/data-sources/overview.mdx) and the
[GraphQL API](/graphql-api/overview.mdx), enabling operations like insertion, updating, deletion, and querying with
features like filtering, pagination, and sorting.

## How models work

### Lifecycle

You can quickly add models to your metadata [using the CLI](/reference/cli/commands/ddn_model_add.mdx).

Models themselves are backed by [types](/reference/metadata-reference/types.mdx), which are derived using the schema of
the data source they represent via a [DataConnectorLink object](/reference/metadata-reference/data-connector-links.mdx).
This means that when you add a model, Hasura DDN automatically generates the necessary types for you.

Once a model is declared, it becomes a central reference point within your metadata. Models are often associated with
[`Relationship`](/reference/metadata-reference/relationships.mdx) objects, allowing them to interact with other models,
and with [`Permissions`](/reference/metadata-reference/permissions.mdx) objects, which control access to the data they
represent. Like all other metadata objects, models are defined in an HML file.

You should [update your models](/reference/cli/commands/ddn_model_update.mdx) whenever you make changes to your data
sources and, in turn, your DataConnectorLink objects. This ensures that your API remains in sync with your data.

To make a new model available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="The following is an example of a model definition for a Users model:"
---
kind: Model
version: v1
definition:
  name: Users
  objectType: Users
  source:
    dataConnectorName: my_pg
    collection: users
  filterExpressionType: UsersBoolExp
  orderableFields:
    - fieldName: id
      orderByDirections:
        enableAll: true
    - fieldName: name
      orderByDirections:
        enableAll: true
    - fieldName: email
      orderByDirections:
        enableAll: true
    - fieldName: createdAt
      orderByDirections:
        enableAll: true
  graphql:
    selectMany:
      queryRootField: users
    selectUniques:
      - queryRootField: usersById
        uniqueIdentifier:
          - id
    orderByExpressionType: UsersOrderBy
```

| **Field**                                        | **Description**                                                                                                                                                                                | **Reference**                                                         |
| ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **kind**                                         | Specifies the type of object being defined. In this case, itâ€™s a model.                                                                                                                        | [Model](#model-model)                                                 |
| **version**                                      | Indicates the version of the model's structure.                                                                                                                                                | [ModelV1](#model-modelv1)                                             |
| **definition.name**                              | The name of the model, representing the collection of data objects within this model.                                                                                                          | [ModelName](#model-modelname)                                         |
| **definition.objectType**                        | Defines the type of objects contained within this model.                                                                                                                                       | [CustomTypeName](#model-customtypename)                               |
| **definition.source.dataConnectorName**          | The name of the data connector that backs this model, linking it to the actual data source.                                                                                                    | [DataConnectorName](#model-dataconnectorname)                         |
| **definition.source.collection**                 | The specific collection within the data connector that this model maps to.                                                                                                                     | [CollectionName](#model-collectionname)                               |
| **definition.filterExpressionType**              | Specifies the type used for filtering the model's data within GraphQL queries.                                                                                                                 | [CustomTypeName](#model-customtypename)                               |
| **definition.orderableFields**                   | A list of fields (in this example: `id`, `name`, `email`, `createdAt`) that can be used to sort the data in this model.                                                                        | [OrderableField](#model-orderablefield)                               |
| **definition.graphql.selectMany.queryRootField** | The root field in the GraphQL API for querying multiple objects from this model. Removing this will disable the ability to query and return an array of this model.                            | [SelectManyGraphQlDefinition](#model-selectmanygraphqldefinition)     |
| **definition.graphql.selectUniques**             | Defines unique query root fields (e.g., usersById) and identifiers used to retrieve unique objects in GraphQL. Removing this will disable the abilty to query a single instance of this model. | [SelectUniqueGraphQlDefinition](#model-selectuniquegraphqldefinition) |
| **definition.graphql.orderByExpressionType**     | The type name used for specifying how to order the data when querying this model in GraphQL.                                                                                                   | [GraphQlTypeName](#model-graphqltypename)                             |

```graphql title="The earlier model definition enables the following query in the API:"
query UsersQuery {
  users(where: { name: { _eq: "Bob" } }, order_by: { createdAt: Asc }, limit: 10) {
    id
    name
    email
    createdAt
  }
}
```

The above example works because the earlier model definition includes the necessary configuration for filtering,
sorting, and pagination. Alternatively, if we'd set `enableAll` to `false` for the `createdAt` field in the
[`orderableFields` section](#model-orderablefield), the `createdAt` field would not be available for sorting in the API.

<details>
<summary>Check out Global IDs for relay:</summary>

A Global ID is a unique identifier for an object across the entire application, not just within a specific table or
type. Think of it as an ID which you can use to fetch any object directly, regardless of what kind of object it is. This
is different from typical database IDs, which are often guaranteed unique only within a particular table.

[//]: # "TODO: As long as it has already been fetched?"

Hasura's Global ID implementation can be used to provide options for GraphQL clients to elegantly handle caching and
data re-fetching in a predictable and standardized way.

The Global ID generated by Hasura DDN follows the
[Relay Global ID spec](https://relay.dev/graphql/objectidentification.htm).

As the example below shows, the `user` object type has a field `user_id` that uniquely identifies a user. The Global ID
for the `user` object type will be generated using the `user_id` field:

For the following request on a model which has enabled Global ID:

```graphql
{
  user_by_id(user_id: 1) {
    id // Global ID
    user_id
    name
  }
}
```

The response obtained should look like the following:

```json
{
  "data": {
    "user_by_id": {
      "id": "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IkFydGljbGUiLCJpZCI6eyJhcnRpY2xlX2lkIjoyfX0=",
      "user_id": 1,
      "name": "Bob"
    }
  }
}
```

Now, with the Global ID received above, the `User` object corresponding to `user_id: 1` can be retrieved, as shown
below.

```graphql
{
  node(id: "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IkFydGljbGUiLCJpZCI6eyJhcnRpY2xlX2lkIjoyfX0=") {
    id
    __typename
    ... on User {
      name
    }
  }
}
```

The response to the above request should identify the `User` with `user_id: 1`.

```json
{
  "node": {
    "id": "eyJ2ZXJzaW9uIjoxLCJ0eXBlbmFtZSI6IkFydGljbGUiLCJpZCI6eyJhcnRpY2xlX2lkIjoyfX0=",
    "__typename": "User",
    "name": "Bob"
  }
}
```

</details>

---

## Metadata structure


#### Model {#model-model}

The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations.


**One of the following values:**

| Value | Description |
|-----|-----|
| [ModelV1](#model-modelv1) |  |
| [ModelV2](#model-modelv2) |  |

 **Example:**

```yaml
kind: Model
version: v1
definition:
  name: Articles
  objectType: article
  globalIdSource: true
  arguments: []
  source:
    dataConnectorName: data_connector
    collection: articles
    argumentMapping: {}
  filterExpressionType: Article_bool_exp
  orderableFields:
    - fieldName: article_id
      orderByDirections:
        enableAll: true
    - fieldName: title
      orderByDirections:
        enableAll: true
    - fieldName: author_id
      orderByDirections:
        enableAll: true
  graphql:
    selectUniques:
      - queryRootField: ArticleByID
        uniqueIdentifier:
          - article_id
        description: Description for the select unique ArticleByID
    selectMany:
      queryRootField: ArticleMany
      description: Description for the select many ArticleMany
    orderByExpressionType: Article_Order_By
    apolloFederation:
      entitySource: true
  description: Description for the model Articles
```


#### ModelV2 {#model-modelv2}

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Model` | true |  |
| `version` | `v2` | true |  |
| `definition` | [ModelV2](#model-modelv2) | true | The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations. ModelV2 implements the changes described in rfcs/open-dd-expression-type-changes.md. |



#### ModelV2 {#model-modelv2}

The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations. ModelV2 implements the changes described in rfcs/open-dd-expression-type-changes.md.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ModelName](#model-modelname) | true | The name of the data model. |
| `objectType` | [CustomTypeName](#model-customtypename) | true | The type of the objects of which this model is a collection. |
| `globalIdSource` | boolean | false | Whether this model should be used as the global ID source for all objects of its type. |
| `arguments` | [[ArgumentDefinition](#model-argumentdefinition)] | false | A list of arguments accepted by this model. Defaults to no arguments. |
| `source` | [ModelSource](#model-modelsource) / null | false | The source configuration for this model. |
| `filterExpressionType` | [CustomTypeName](#model-customtypename) / null | false | The boolean expression type that should be used to perform filtering on this model. |
| `orderByExpression` | [OrderByExpressionName](#model-orderbyexpressionname) / null | false | The order by expression to use for this model. |
| `aggregateExpression` | [AggregateExpressionName](#model-aggregateexpressionname) / null | false | The name of the AggregateExpression that defines how to aggregate over this model |
| `graphql` | [ModelGraphQlDefinitionV2](#model-modelgraphqldefinitionv2) / null | false | Configuration for how this model should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the model. Gets added to the description of the model in the graphql schema. |



#### ModelGraphQlDefinitionV2 {#model-modelgraphqldefinitionv2}

The definition of how a model appears in the GraphQL API. Note: ModelGraphQlDefinitionV2 removed the `order_by_expression_type` property. See rfcs/open-dd-expression-type-changes.md.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `selectUniques` | [[SelectUniqueGraphQlDefinition](#model-selectuniquegraphqldefinition)] | true | For each select unique defined here, a query root field is added to the GraphQL API that can be used to select a unique object from the model. |
| `selectMany` | [SelectManyGraphQlDefinition](#model-selectmanygraphqldefinition) / null | false | Select many configuration for a model adds a query root field to the GraphQl API that can be used to retrieve multiple objects from the model. |
| `argumentsInputType` | [GraphQlTypeName](#model-graphqltypename) / null | false | The type name of the input type used to hold the arguments of the model. |
| `apolloFederation` | [ModelApolloFederationConfiguration](#model-modelapollofederationconfiguration) / null | false | Apollo Federation configuration |
| `filterInputTypeName` | [GraphQlTypeName](#model-graphqltypename) / null | false | The type name of the input type used to hold the filtering settings used by aggregates (etc) to filter their input before processing |
| `aggregate` | [ModelAggregateGraphQlDefinition](#model-modelaggregategraphqldefinition) / null | false | Configures the query root field added to the GraphQL API that can be used to aggregate over the model |

 **Example:**

```yaml
selectUniques:
  - queryRootField: ArticleByID
    uniqueIdentifier:
      - article_id
    description: Description for the select unique ArticleByID
selectMany:
  queryRootField: ArticleMany
  description: Description for the select many ArticleMany
aggregate:
  queryRootField: ArticleAggregate
  description: Aggregate over Articles
```


#### OrderByExpressionName {#model-orderbyexpressionname}

The name of an order by expression.


**Value:** string


#### ModelV1 {#model-modelv1}

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Model` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ModelV1](#model-modelv1) | true | The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations. |



#### ModelV1 {#model-modelv1}

The definition of a data model. A data model is a collection of objects of a particular type. Models can support one or more CRUD operations.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ModelName](#model-modelname) | true | The name of the data model. |
| `objectType` | [CustomTypeName](#model-customtypename) | true | The type of the objects of which this model is a collection. |
| `globalIdSource` | boolean | false | Whether this model should be used as the global ID source for all objects of its type. |
| `arguments` | [[ArgumentDefinition](#model-argumentdefinition)] | false | A list of arguments accepted by this model. Defaults to no arguments. |
| `source` | [ModelSource](#model-modelsource) / null | false | The source configuration for this model. |
| `filterExpressionType` | [CustomTypeName](#model-customtypename) / null | false | The boolean expression type that should be used to perform filtering on this model. |
| `orderableFields` | [[OrderableField](#model-orderablefield)] | true | A list of fields that can be used to order the objects in this model. |
| `aggregateExpression` | [AggregateExpressionName](#model-aggregateexpressionname) / null | false | The name of the AggregateExpression that defines how to aggregate over this model |
| `graphql` | [ModelGraphQlDefinition](#model-modelgraphqldefinition) / null | false | Configuration for how this model should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the model. Gets added to the description of the model in the graphql schema. |



#### ModelGraphQlDefinition {#model-modelgraphqldefinition}

The definition of how a model appears in the GraphQL API.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `selectUniques` | [[SelectUniqueGraphQlDefinition](#model-selectuniquegraphqldefinition)] | true | For each select unique defined here, a query root field is added to the GraphQL API that can be used to select a unique object from the model. |
| `selectMany` | [SelectManyGraphQlDefinition](#model-selectmanygraphqldefinition) / null | false | Select many configuration for a model adds a query root field to the GraphQl API that can be used to retrieve multiple objects from the model. |
| `argumentsInputType` | [GraphQlTypeName](#model-graphqltypename) / null | false | The type name of the input type used to hold the arguments of the model. |
| `orderByExpressionType` | [GraphQlTypeName](#model-graphqltypename) / null | false | The type name of the order by expression input type. |
| `apolloFederation` | [ModelApolloFederationConfiguration](#model-modelapollofederationconfiguration) / null | false | Apollo Federation configuration |
| `filterInputTypeName` | [GraphQlTypeName](#model-graphqltypename) / null | false | The type name of the input type used to hold the filtering settings used by aggregates (etc) to filter their input before processing |
| `aggregate` | [ModelAggregateGraphQlDefinition](#model-modelaggregategraphqldefinition) / null | false | Configures the query root field added to the GraphQL API that can be used to aggregate over the model |

 **Example:**

```yaml
selectUniques:
  - queryRootField: ArticleByID
    uniqueIdentifier:
      - article_id
    description: Description for the select unique ArticleByID
selectMany:
  queryRootField: ArticleMany
  description: Description for the select many ArticleMany
orderByExpressionType: Article_Order_By
aggregate:
  queryRootField: ArticleAggregate
  description: Aggregate over Articles
```


#### ModelAggregateGraphQlDefinition {#model-modelaggregategraphqldefinition}

The definition of the GraphQL API for aggregating over a model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `queryRootField` | [GraphQlFieldName](#model-graphqlfieldname) | true | The name of the query root field for this API. |
| `description` | string / null | false | The description of the aggregate graphql definition of the model. Gets added to the description of the aggregate root field of the model in the graphql schema. |
| `deprecated` | [Deprecated](#model-deprecated) / null | false | Whether this aggregate query field is deprecated. If set, the deprecation status is added to the aggregate root field's graphql schema. |
| `subscription` | [SubscriptionGraphQlDefinition](#model-subscriptiongraphqldefinition) / null | false | Enable subscription on this aggregate root field. |



#### ModelApolloFederationConfiguration {#model-modelapollofederationconfiguration}

Apollo Federation configuration for a model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `entitySource` | boolean | true | Whether this model should be used as the source for fetching _entity for object of its type. |



#### GraphQlTypeName {#model-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### SelectManyGraphQlDefinition {#model-selectmanygraphqldefinition}

The definition of the GraphQL API for selecting rows from a model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `queryRootField` | [GraphQlFieldName](#model-graphqlfieldname) | true | The name of the query root field for this API. |
| `description` | string / null | false | The description of the select many graphql definition of the model. Gets added to the description of the select many root field of the model in the graphql schema. |
| `deprecated` | [Deprecated](#model-deprecated) / null | false | Whether this select many query field is deprecated. If set, the deprecation status is added to the select many root field's graphql schema. |
| `subscription` | [SubscriptionGraphQlDefinition](#model-subscriptiongraphqldefinition) / null | false | Enable subscription on this select many root field. |



#### SelectUniqueGraphQlDefinition {#model-selectuniquegraphqldefinition}

The definition of the GraphQL API for selecting a unique row/object from a model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `queryRootField` | [GraphQlFieldName](#model-graphqlfieldname) | true | The name of the query root field for this API. |
| `uniqueIdentifier` | [[FieldName](#model-fieldname)] | true | A set of fields which can uniquely identify a row/object in the model. |
| `description` | string / null | false | The description of the select unique graphql definition of the model. Gets added to the description of the select unique root field of the model in the graphql schema. |
| `deprecated` | [Deprecated](#model-deprecated) / null | false | Whether this select unique query field is deprecated. If set, the deprecation status is added to the select unique root field's graphql schema. |
| `subscription` | [SubscriptionGraphQlDefinition](#model-subscriptiongraphqldefinition) / null | false | Enable subscription on this select unique root field. |



#### SubscriptionGraphQlDefinition {#model-subscriptiongraphqldefinition}

The definition of the GraphQL API for enabling subscription on query root fields.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootField` | [GraphQlFieldName](#model-graphqlfieldname) | true | The name of the subscription root field. |
| `description` | string / null | false | The description of the subscription graphql definition. Gets added to the description of the subscription root field in the graphql schema. |
| `deprecated` | [Deprecated](#model-deprecated) / null | false | Whether this subscription root field is deprecated. If set, the deprecation status is added to the subscription root field's graphql schema. |
| `pollingIntervalMs` | integer | false | Polling interval in milliseconds for the subscription. |



#### Deprecated {#model-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### GraphQlFieldName {#model-graphqlfieldname}

The name of a GraphQL object field.


**Value:** string


#### AggregateExpressionName {#model-aggregateexpressionname}

The name of an aggregate expression.


**Value:** string


#### OrderableField {#model-orderablefield}

A field that can be used to order the objects in a model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#model-fieldname) | true |  |
| `orderByDirections` | [EnableAllOrSpecific](#model-enableallorspecific) | true | Enable all or specific values. |



#### EnableAllOrSpecific {#model-enableallorspecific}

Enable all or specific values.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableAll` | boolean | false |  |
| `enableSpecific` | [[OrderByDirection](#model-orderbydirection)] | false |  |



#### OrderByDirection {#model-orderbydirection}


**Value:** `Asc` / `Desc`


#### FieldName {#model-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### ModelSource {#model-modelsource}

Description of how a model maps to a particular data connector

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#model-dataconnectorname) | true | The name of the data connector backing this model. |
| `collection` | [CollectionName](#model-collectionname) | true | The collection in the data connector that backs this model. |
| `argumentMapping` | [ArgumentMapping](#model-argumentmapping) | false | Mapping from model argument names to data connector collection argument names. |

 **Example:**

```yaml
dataConnectorName: data_connector
collection: articles
```


#### ArgumentMapping {#model-argumentmapping}

Mapping of a comand or model argument name to the corresponding argument name used in the data connector. The key of this object is the argument name used in the command or model and the value is the argument name used in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [DataConnectorArgumentName](#model-dataconnectorargumentname) | false |  |



#### DataConnectorArgumentName {#model-dataconnectorargumentname}

The name of an argument as defined by a data connector.


**Value:** string


#### CollectionName {#model-collectionname}

The collection in the data connector that backs this model.


**Value:** string


#### DataConnectorName {#model-dataconnectorname}

The name of the data connector backing this model.


**Value:** string


#### ArgumentDefinition {#model-argumentdefinition}

The definition of an argument for a field, command, or model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ArgumentName](#model-argumentname) | true | The name of an argument. |
| `type` | [TypeReference](#model-typereference) | true |  |
| `description` | string / null | false |  |



#### TypeReference {#model-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### ArgumentName {#model-argumentname}

The name of an argument.


**Value:** string


#### CustomTypeName {#model-customtypename}

The name of a user-defined type.


**Value:** string


#### ModelName {#model-modelname}

The name of data model.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/commands.mdx ---
# Commands

---
sidebar_position: 7
sidebar_label: Commands
description:
  "Begin executing business logic directly from your GraphQL API using Hasura Data Domain Specification commands.
  Connect to REST endpoints, custom servers or serverless functions and better manage your back-end functions."
keywords:
  - hasura commands
  - hasura dds
  - graphql api
  - business logic
  - data connector
  - rest endpoint
  - custom server
  - serverless function
  - graphql instruction
  - command configuration
seoFrontMatterUpdated: true
toc_max_heading_level: 4
---

# Commands

## Introduction

In Hasura DDN, a **command** represents an action that can be executed within your data domain. Commands act as the
verbs or operations that modify or interact with the data managed by your
[models](/reference/metadata-reference/models.mdx). They can be triggered via GraphQL queries or mutations.

Commands have a fixed definition and are backed by [functions](#command-functionname) (exposed as queries) or
[procedures](#command-procedurename) (exposed as mutations) allowing you to add top-level methods to your API, or, add a
custom field to an existing model.

Commands also allow you to execute [custom business logic](/business-logic/overview.mdx) directly from your GraphQL API
and are useful in validating, processing or enriching data, calling another API, or, for example, logging a user in.

## How commands work

### Lifecycle

Commands can be added to your metadata [using the CLI](/reference/cli/commands/ddn_command_add.mdx). The CLI will use a
connector's [DataConnectorLink object](/reference/metadata-reference/data-connector-links.mdx) to determine which
functions or procedures are available to be added as commands.

You should [update your commands](/reference/cli/commands/ddn_command_update.mdx) whenever you make changes to your data
sources and, in turn, your DataConnectorLink objects. This ensures that your API remains in sync with your data.

As an example, if you're usings the [TypesScript connector](/business-logic/overview.mdx) for business logic and add a
new function, you'll need to update your DataConnectorLink to ensure that the new function is present in the connector's
configuration. Then, add the new command to your metadata.

To make a new command available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="The following is an example of a command for a function:"
---
kind: Command
version: v1
definition:
  name: Hello
  outputType: String!
  arguments:
    - name: name
      type: String
  source:
    dataConnectorName: my_data_connector
    dataConnectorCommand:
      function: hello
  graphql:
    rootFieldName: hello
    rootFieldKind: Query
```

| **Field**                                  | **Description**                                                                                                        | **Reference**                                         |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| **kind**                                   | Specifies the type of object being defined, which is a command in this context.                                        | [Command](#command-command)                           |
| **version**                                | Indicates the version of the command's structure.                                                                      | [CommandV1](#command-commandv1)                       |
| **definition.name**                        | The name of the command, representing the action to be executed. You can configure this to whatever you wish.          | [CommandName](#command-commandname)                   |
| **definition.outputType**                  | Defines the return type of the command, specifying what kind of data is returned after execution.                      | [TypeReference](#command-typereference)               |
| **definition.arguments**                   | Lists the arguments the command can take, allowing customization of the command's execution based on input parameters. | [ArgumentDefinition](#command-argumentdefinition)     |
| **definition.source.dataConnectorName**    | The name of the data connector that backs this command, linking it to the data source or function.                     | [DataConnectorName](#command-dataconnectorname)       |
| **definition.source.dataConnectorCommand** | Specifies the function or procedure in the data connector's configuration file(s) that implements the command's logic. | [DataConnectorCommand](#command-dataconnectorcommand) |
| **definition.graphql.rootFieldName**       | The name of the root field in the GraphQL API for invoking this command.                                               | [GraphQlFieldName](#command-graphqlfieldname)         |
| **definition.graphql.rootFieldKind**       | Determines whether the command should be part of the Query or Mutation root in the GraphQL API.                        | [GraphQlRootFieldKind](#command-graphqlrootfieldkind) |

```graphql title="The HML example above results in this query:"
query {
  hello(name: "Hasura")
}
```

---

## Metadata structure


### Command {#command-command}

The definition of a command. A command is a user-defined operation which can take arguments and returns an output. The semantics of a command are opaque to the Open DD specification.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Command` | true |  |
| `version` | `v1` | true |  |
| `definition` | [CommandV1](#command-commandv1) | true | Definition of an OpenDD Command, which is a custom operation that can take arguments and returns an output. The semantics of a command are opaque to OpenDD. |

 **Example:**

```yaml
kind: Command
version: v1
definition:
  name: get_latest_article
  outputType: commandArticle
  arguments: []
  source:
    dataConnectorName: data_connector
    dataConnectorCommand:
      function: latest_article
    argumentMapping: {}
  graphql:
    rootFieldName: getLatestArticle
    rootFieldKind: Query
  description: Get the latest article
```


#### CommandV1 {#command-commandv1}

Definition of an OpenDD Command, which is a custom operation that can take arguments and returns an output. The semantics of a command are opaque to OpenDD.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CommandName](#command-commandname) | true | The name of the command. |
| `outputType` | [TypeReference](#command-typereference) | true | The return type of the command. |
| `arguments` | [[ArgumentDefinition](#command-argumentdefinition)] | false | The list of arguments accepted by this command. Defaults to no arguments. |
| `source` | [CommandSource](#command-commandsource) / null | false | The source configuration for this command. |
| `graphql` | [CommandGraphQlDefinition](#command-commandgraphqldefinition) / null | false | Configuration for how this command should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the command. Gets added to the description of the command's root field in the GraphQL schema. |



#### CommandGraphQlDefinition {#command-commandgraphqldefinition}

The definition of how a command should appear in the GraphQL API.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootFieldName` | [GraphQlFieldName](#command-graphqlfieldname) | true | The name of the graphql root field to use for this command. |
| `rootFieldKind` | [GraphQlRootFieldKind](#command-graphqlrootfieldkind) | true | Whether to put this command in the Query or Mutation root of the GraphQL API. |
| `deprecated` | [Deprecated](#command-deprecated) / null | false | Whether this command root field is deprecated. If set, this will be added to the graphql schema as a deprecated field. |

 **Example:**

```yaml
rootFieldName: getLatestArticle
rootFieldKind: Query
```


#### Deprecated {#command-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### GraphQlRootFieldKind {#command-graphqlrootfieldkind}

Whether to put this command in the Query or Mutation root of the GraphQL API.


**Value:** `Query` / `Mutation`


#### GraphQlFieldName {#command-graphqlfieldname}

The name of a GraphQL object field.


**Value:** string


#### CommandSource {#command-commandsource}

Description of how a command maps to a particular data connector

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#command-dataconnectorname) | true | The name of the data connector backing this command. |
| `dataConnectorCommand` | [DataConnectorCommand](#command-dataconnectorcommand) | true | The function/procedure in the data connector that backs this command. |
| `argumentMapping` | [ArgumentMapping](#command-argumentmapping) | false | Mapping from command argument names to data connector function or procedure argument names. |

 **Example:**

```yaml
dataConnectorName: data_connector
dataConnectorCommand:
  function: latest_article
argumentMapping: {}
```


#### ArgumentMapping {#command-argumentmapping}

Mapping of a comand or model argument name to the corresponding argument name used in the data connector. The key of this object is the argument name used in the command or model and the value is the argument name used in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [DataConnectorArgumentName](#command-dataconnectorargumentname) | false |  |



#### DataConnectorArgumentName {#command-dataconnectorargumentname}

The name of an argument as defined by a data connector.


**Value:** string


#### DataConnectorCommand {#command-dataconnectorcommand}

The function/procedure in the data connector that backs this command.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `function` | [FunctionName](#command-functionname) | false | The name of a function backing the command. |
| `procedure` | [ProcedureName](#command-procedurename) | false | The name of a procedure backing the command. |



#### ProcedureName {#command-procedurename}

The name of a procedure backing the command.


**Value:** string


#### FunctionName {#command-functionname}

The name of a function backing the command.


**Value:** string


#### DataConnectorName {#command-dataconnectorname}

The name of a data connector.


**Value:** string


#### ArgumentDefinition {#command-argumentdefinition}

The definition of an argument for a field, command, or model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ArgumentName](#command-argumentname) | true | The name of an argument. |
| `type` | [TypeReference](#command-typereference) | true |  |
| `description` | string / null | false |  |



#### ArgumentName {#command-argumentname}

The name of an argument.


**Value:** string


#### TypeReference {#command-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### CommandName {#command-commandname}

The name of a command.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/boolean-expressions.mdx ---
# Boolean Expressions

---
sidebar_position: 8
sidebar_label: Boolean Expressions
description:
  "Explore boolean expression types in Hasura. Learn how to define comparisons between different data types and expose
  them in your API."
keywords:
  - hasura documentation
  - graphql data structure
  - data domain mapping
  - hasura ddn
  - hasura data specification
  - graphql schema
  - boolean expressions
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# Boolean Expressions

## Introduction

Hasura provides powerful tools to control filtering and selecting data. Boolean expression types let you control which
filters are available for a [model](/reference/metadata-reference/models.mdx) or
[command](/reference/metadata-reference/commands.mdx). They can be used to configure filters on models, such as in the
[filtering](graphql-api/queries/filters/index.mdx) section, or as the types of arguments to commands or models.

## How Boolean expressions work

### Lifecycle

There are two types of boolean expressions:

| Type              | Description                                               |
| ----------------- | --------------------------------------------------------- |
| [Scalar](#scalar) | This specifies how a user able to compare a scalar field. |
| [Object](#object) | This specifies how fields of a type can be filtered.      |

Regardless of the type, boolean expressions are used to define how a user is able to filter data and are defined in your
metadata.

### Examples

#### Scalar

This specifies how a user is able to compare a scalar field. For instance, you might want to say that a user can only
check if a `String` type is equals to another, or whether it is null or not. You can do that with the following
metadata:

```yaml
kind: BooleanExpressionType
version: v1
definition:
  name: String_comparison_exp_with_eq_and_is_null
  operand:
    scalar:
      type: String
      comparisonOperators:
        - name: equals
          argumentType: String!
      dataConnectorOperatorMapping:
        - dataConnectorName: postgres
          dataConnectorScalarType: varchar
          operatorMapping:
            equals: _eq
  logicalOperators:
    enable: true
  isNull:
    enable: true
  graphql:
    typeName: String_comparison_exp_with_eq_and_is_null
```

Note the `dataConnectorOperatorMapping`. This allows us to define what these operators mean in zero or more data
connectors. Here, we want our `equals` operator to use Postgres's `_eq` operator.

This would allow us to write filters like:

```json
{ "first_name": { "_is_null": true } }
```

```json
{ "last_name": { "equals": "Bruce" } }
```

#### Object

An object `BooleanExpressionType` is used to define how fields of a type can be filtered. Note that nothing here talks
about specific data connectors - instead you can specify which `BooleanExpressionType` is used to filter each field or
relationship, and then defer the mappings of individual scalar types to those `BooleanExpressionType`s.

```yaml
kind: BooleanExpressionType
version: v2
definition:
  name: Album_bool_exp
  operand:
    object:
      type: Album
      comparableFields:
        - fieldName: AlbumId
          booleanExpressionType: Int_comparison_exp
        - fieldName: ArtistId
          booleanExpressionType: Int_comparison_exp_with_is_null
        - fieldName: Address
          booleanExpressionType: Address_bool_exp
      comparableRelationships:
        - relationshipName: artist
          booleanExpressionType: Artist_bool_exp
  logicalOperators:
    enable: true
  isNull:
    enable: true
  graphql:
    typeName: Album_bool_exp
```

Note here that we can specify different comparison operators for `AlbumId` and `ArtistId` by using different
`BooleanExpressionType`s for them. We are also able to define filtering on nested objects such as `Address`.

The above would let us write filters on `Album` types like:

```json
{ "album": { "AlbumId": { "equals": 100 } } }
```

```json
{ "album": { "Address": { "postcode": { "like": "N1" } } } }
```

```json
{ "album": { "artist": { "name": { "equals": "Madonna" } } } }
```

---

## Metadata structure


### BooleanExpressionType {#booleanexpressiontype-booleanexpressiontype}

Definition of a type representing a boolean expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `BooleanExpressionType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [BooleanExpressionTypeV1](#booleanexpressiontype-booleanexpressiontypev1) | true | Definition of a type representing a boolean expression on an OpenDD object type. |

 **Example:**

```yaml
kind: BooleanExpressionType
version: v1
definition:
  name: Album_bool_exp
  operand:
    object:
      type: Album
      comparableFields:
        - fieldName: AlbumId
          booleanExpressionType: pg_Int_Comparison_exp
        - fieldName: ArtistId
          booleanExpressionType: pg_Int_Comparison_exp_with_is_null
        - fieldName: Address
          booleanExpressionType: Address_bool_exp
      comparableRelationships:
        - relationshipName: artist
          booleanExpressionType: Artist_bool_exp
  logicalOperators:
    enable: true
  isNull:
    enable: true
  graphql:
    typeName: App_Album_bool_exp
```


#### BooleanExpressionTypeV1 {#booleanexpressiontype-booleanexpressiontypev1}

Definition of a type representing a boolean expression on an OpenDD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#booleanexpressiontype-customtypename) | true | The name to give this boolean expression type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `operand` | [BooleanExpressionOperand](#booleanexpressiontype-booleanexpressionoperand) | true | The type that this boolean expression applies to. |
| `logicalOperators` | [BooleanExpressionLogicalOperators](#booleanexpressiontype-booleanexpressionlogicaloperators) | true | Whether to enable _and / _or / _not |
| `isNull` | [BooleanExpressionIsNull](#booleanexpressiontype-booleanexpressionisnull) | true | Whether to enable _is_null |
| `graphql` | [BooleanExpressionTypeGraphQlConfiguration](#booleanexpressiontype-booleanexpressiontypegraphqlconfiguration) / null | false | Configuration for how this object type should appear in the GraphQL schema. |



#### BooleanExpressionTypeGraphQlConfiguration {#booleanexpressiontype-booleanexpressiontypegraphqlconfiguration}

GraphQL configuration of an OpenDD boolean expression type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#booleanexpressiontype-graphqltypename) | true | The name to use for the GraphQL type representation of this boolean expression type. |



#### GraphQlTypeName {#booleanexpressiontype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### BooleanExpressionIsNull {#booleanexpressiontype-booleanexpressionisnull}

Configuration for is_null in boolean expressions

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enable` | boolean | true |  |



#### BooleanExpressionLogicalOperators {#booleanexpressiontype-booleanexpressionlogicaloperators}

Configuration for logical operators in boolean expressions

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enable` | boolean | true |  |



#### BooleanExpressionOperand {#booleanexpressiontype-booleanexpressionoperand}

Configuration for object or scalar boolean expression


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `object` | [BooleanExpressionObjectOperand](#booleanexpressiontype-booleanexpressionobjectoperand) | false | Definition of an object type representing a boolean expression on an OpenDD object type. |
| `scalar` | [BooleanExpressionScalarOperand](#booleanexpressiontype-booleanexpressionscalaroperand) | false | Definition of a scalar type representing a boolean expression on an OpenDD scalar type. |



#### BooleanExpressionScalarOperand {#booleanexpressiontype-booleanexpressionscalaroperand}

Definition of a scalar type representing a boolean expression on an OpenDD scalar type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | [TypeName](#booleanexpressiontype-typename) | true | The OpenDD type name of the scalar type that this boolean expression applies to. |
| `comparisonOperators` | [[ComparisonOperator](#booleanexpressiontype-comparisonoperator)] | true | The list of comparison operators that can used on this scalar type |
| `dataConnectorOperatorMapping` | [[DataConnectorOperatorMapping](#booleanexpressiontype-dataconnectoroperatormapping)] | true | The list of mappings between OpenDD operator names and the names used in the data connector schema |



#### DataConnectorOperatorMapping {#booleanexpressiontype-dataconnectoroperatormapping}

Mapping between OpenDD operator names and the names used in the data connector schema

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#booleanexpressiontype-dataconnectorname) | true | Name of the data connector this mapping applies to |
| `dataConnectorScalarType` | [DataConnectorScalarType](#booleanexpressiontype-dataconnectorscalartype) | true | Name of the scalar type according to the data connector's schema |
| `operatorMapping` | [operator_mapping](#booleanexpressiontype-operator_mapping) | true | Mapping between OpenDD operator names and the data connector's operator names Defaults to the same operator name (e.g. "_eq: _eq") if no explicit mapping is present. |



#### operator_mapping {#booleanexpressiontype-operator_mapping}

Mapping between OpenDD operator names and the data connector's operator names Defaults to the same operator name (e.g. "_eq: _eq") if no explicit mapping is present.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [DataConnectorOperatorName](#booleanexpressiontype-dataconnectoroperatorname) | false | The name of an operator in a data connector. |



#### DataConnectorOperatorName {#booleanexpressiontype-dataconnectoroperatorname}

The name of an operator in a data connector.


**Value:** string


#### DataConnectorScalarType {#booleanexpressiontype-dataconnectorscalartype}

The name of a scalar type in a data connector.


**Value:** string


#### DataConnectorName {#booleanexpressiontype-dataconnectorname}

The name of a data connector.


**Value:** string


#### ComparisonOperator {#booleanexpressiontype-comparisonoperator}

Definition of a comparison operator for a scalar type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [OperatorName](#booleanexpressiontype-operatorname) | true | Name you want to give the operator in OpenDD / GraphQL |
| `argumentType` | [TypeReference](#booleanexpressiontype-typereference) | true | An OpenDD type |



#### TypeReference {#booleanexpressiontype-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### OperatorName {#booleanexpressiontype-operatorname}

The name of an operator


**Value:** string


#### TypeName {#booleanexpressiontype-typename}

The OpenDD type name of the scalar type that this boolean expression applies to.


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#booleanexpressiontype-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#booleanexpressiontype-customtypename) |  |



#### InbuiltType {#booleanexpressiontype-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### BooleanExpressionObjectOperand {#booleanexpressiontype-booleanexpressionobjectoperand}

Definition of an object type representing a boolean expression on an OpenDD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | [CustomTypeName](#booleanexpressiontype-customtypename) | true | The name of the object type that this boolean expression applies to. |
| `comparableFields` | [[BooleanExpressionComparableField](#booleanexpressiontype-booleanexpressioncomparablefield)] | true | The list of fields of the object type that can be used for comparison when evaluating this boolean expression. |
| `comparableRelationships` | [[BooleanExpressionComparableRelationship](#booleanexpressiontype-booleanexpressioncomparablerelationship)] | true | The list of relationships of the object type that can be used for comparison when evaluating this boolean expression. |



#### BooleanExpressionComparableRelationship {#booleanexpressiontype-booleanexpressioncomparablerelationship}

Definition of a relationship that can be used for a comparison

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `relationshipName` | [RelationshipName](#booleanexpressiontype-relationshipname) | true | The name of the relationship to use for comparison |
| `booleanExpressionType` | [CustomTypeName](#booleanexpressiontype-customtypename) / null | false | The boolean expression type to use for comparison. This is optional for relationships to models, and defaults to the filterExpressionType of the model |



#### RelationshipName {#booleanexpressiontype-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### BooleanExpressionComparableField {#booleanexpressiontype-booleanexpressioncomparablefield}

Comparison configuration definition for a field that can be used for a comparison

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#booleanexpressiontype-fieldname) | true | The name of the field that can be compared. |
| `booleanExpressionType` | [CustomTypeName](#booleanexpressiontype-customtypename) | true | The boolean expression type that can be used for comparison against the type of the field. |



#### FieldName {#booleanexpressiontype-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#booleanexpressiontype-customtypename}

The name of a user-defined type.


**Value:** string
### ObjectBooleanExpressionType {#objectbooleanexpressiontype-objectbooleanexpressiontype}

Definition of a type representing a boolean expression on an Open DD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ObjectBooleanExpressionType` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ObjectBooleanExpressionTypeV1](#objectbooleanexpressiontype-objectbooleanexpressiontypev1) | true | Definition of a type representing a boolean expression on an Open DD object type. Deprecated in favour of `BooleanExpressionType`. |

 **Example:**

```yaml
kind: ObjectBooleanExpressionType
version: v1
definition:
  name: AuthorBoolExp
  objectType: Author
  dataConnectorName: my_db
  dataConnectorObjectType: author
  comparableFields:
    - fieldName: article_id
      operators:
        enableAll: true
    - fieldName: title
      operators:
        enableAll: true
    - fieldName: author_id
      operators:
        enableAll: true
  graphql:
    typeName: Author_bool_exp
```


#### ObjectBooleanExpressionTypeV1 {#objectbooleanexpressiontype-objectbooleanexpressiontypev1}

Definition of a type representing a boolean expression on an Open DD object type. Deprecated in favour of `BooleanExpressionType`.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CustomTypeName](#objectbooleanexpressiontype-customtypename) | true | The name to give this object boolean expression type, used to refer to it elsewhere in the metadata. Must be unique across all types defined in this subgraph. |
| `objectType` | [CustomTypeName](#objectbooleanexpressiontype-customtypename) | true | The name of the object type that this boolean expression applies to. |
| `dataConnectorName` | [DataConnectorName](#objectbooleanexpressiontype-dataconnectorname) | true | The data connector this boolean expression type is based on. |
| `dataConnectorObjectType` | [DataConnectorObjectType](#objectbooleanexpressiontype-dataconnectorobjecttype) | true | The object type in the data connector's schema this boolean expression type is based on. |
| `comparableFields` | [[ComparableField](#objectbooleanexpressiontype-comparablefield)] | true | The list of fields of the object type that can be used for comparison when evaluating this boolean expression. |
| `graphql` | [ObjectBooleanExpressionTypeGraphQlConfiguration](#objectbooleanexpressiontype-objectbooleanexpressiontypegraphqlconfiguration) / null | false | Configuration for how this object type should appear in the GraphQL schema. |



#### ObjectBooleanExpressionTypeGraphQlConfiguration {#objectbooleanexpressiontype-objectbooleanexpressiontypegraphqlconfiguration}

GraphQL configuration of an Open DD boolean expression type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [GraphQlTypeName](#objectbooleanexpressiontype-graphqltypename) | true | The name to use for the GraphQL type representation of this boolean expression type. |



#### GraphQlTypeName {#objectbooleanexpressiontype-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### ComparableField {#objectbooleanexpressiontype-comparablefield}

A field of an object type that can be used for comparison when evaluating a boolean expression.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#objectbooleanexpressiontype-fieldname) | true |  |
| `operators` | [EnableAllOrSpecific](#objectbooleanexpressiontype-enableallorspecific) | true | Enable all or specific values. |



#### EnableAllOrSpecific {#objectbooleanexpressiontype-enableallorspecific}

Enable all or specific values.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableAll` | boolean | false |  |
| `enableSpecific` | [[OperatorName](#objectbooleanexpressiontype-operatorname)] | false |  |



#### OperatorName {#objectbooleanexpressiontype-operatorname}

The name of an operator


**Value:** string


#### FieldName {#objectbooleanexpressiontype-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### DataConnectorObjectType {#objectbooleanexpressiontype-dataconnectorobjecttype}

The name of an object type in a data connector.


**Value:** string


#### DataConnectorName {#objectbooleanexpressiontype-dataconnectorname}

The name of a data connector.


**Value:** string


#### CustomTypeName {#objectbooleanexpressiontype-customtypename}

The name of a user-defined type.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/aggregate-expressions.mdx ---
# Aggregate Expressions

---
sidebar_position: 9
sidebar_label: Aggregate Expressions
description:
  "Explore aggregate expression types in Hasura. Learn how to aggregate different data types and expose them in your
  API."
keywords:
  - hasura documentation
  - graphql data structure
  - data domain mapping
  - hasura ddn
  - hasura data specification
  - graphql schema
  - aggregate expressions
  - aggregate
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# Aggregate Expressions

## Introduction

Aggregate expression types allow you to summarize and calculate collective properties of your data. These expressions
enable you to define how to aggregate over different data types, facilitating efficient data manipulation and retrieval.

Aggregate expressions can be used to configure how data is summarized in your models, or as the types of arguments to
commands or models.

## How AggregateExpressions work

### Lifecycle

:::tip Examples below

Below this description of the lifecycle are examples for each step and how to define AggregateExpressions in your
metadata files.

:::

You'll need to ensure you've generated [ScalarTypes](/reference/metadata-reference/types.mdx#scalartype-scalartype) and
[DataConnectorScalarRepresentations](/reference/metadata-reference/data-connector-links.mdx#dataconnectorscalarrepresentation-dataconnectorscalarrepresentation)
for the data types you want to aggregate over. We recommend saving these in your connector's type definition file.

Once you have these, you can create AggregateExpression objects for the types you want to aggregate over. We recommend
these be saved in a model's metadata file.

Finally, you'll need to update your `graphql-config.hml` in the `globals` subgraph to
[include an `aggregate` field in the `definition`](/reference/metadata-reference/graphql-config.mdx#graphqlconfig-aggregategraphqlconfig).

Once these are included, you'll need to [create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx)
using the CLI.

### Examples

In the following example, we'll show the snippets necessary to implement the example explained above: an
AggregateExpression for the `Reviews` model of our sample application.

```yaml title="metadata/my_connector-types.hml"
# Existing definitions above
---
kind: ScalarType
version: v1
definition:
  name: Numeric
  graphql:
    typeName: Numeric

---
kind: DataConnectorScalarRepresentation
version: v1
definition:
  dataConnectorName: my_connector
  dataConnectorScalarType: numeric
  representation: Numeric
  graphql:
    comparisonExpressionTypeName: NumericComparisonExp
```

```yaml title="metadata/Reviews.hml"
# Existing objects above
---
kind: AggregateExpression
version: v1
definition:
  name: Int4_aggregate_exp
  operand:
    scalar:
      aggregatedType: Int4
      aggregationFunctions:
        - name: avg
          returnType: Numeric
      dataConnectorAggregationFunctionMapping:
        - dataConnectorName: my_connector
          dataConnectorScalarType: int4
          functionMapping:
            avg:
              name: avg
  graphql:
    selectTypeName: Int4_aggregate_fields

---
kind: AggregateExpression
version: v1
definition:
  name: Reviews_aggregate_exp
  operand:
    object:
      aggregatedType: Reviews
      aggregatableFields:
        - fieldName: rating
          aggregateExpression: Int4_aggregate_exp
  graphql:
    selectTypeName: Reviews_aggregate_fields
  description: Aggregate over Reviews
```

```yaml title="globals/graphql-config.hml"
kind: GraphqlConfig
version: v1
definition:
  query:
    rootOperationTypeName: Query
    argumentsInput:
      fieldName: args
    limitInput:
      fieldName: limit
    offsetInput:
      fieldName: offset
    filterInput:
      fieldName: where
      operatorNames:
        and: _and
        or: _or
        not: _not
        isNull: _is_null
    orderByInput:
      fieldName: order_by
      enumDirectionValues:
        asc: Asc
        desc: Desc
      enumTypeNames:
        - directions:
            - Asc
            - Desc
          typeName: OrderBy
    #highlight-start
    aggregate:
      filterInputFieldName: filter_input
      countFieldName: _count
      countDistinctFieldName: _count_distinct
    #highlight-end
  mutation:
    rootOperationTypeName: Mutation
```

---

## Metadata structure


### AggregateExpression {#aggregateexpression-aggregateexpression}

Definition of an aggregate expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AggregateExpression` | true |  |
| `version` | `v1` | true |  |
| `definition` | [AggregateExpressionV1](#aggregateexpression-aggregateexpressionv1) | true | Definition of how to aggregate over a particular operand type |

 **Example:**

```yaml
kind: AggregateExpression
version: v1
definition:
  name: Invoice_aggregate_exp
  operand:
    object:
      aggregatedType: Invoice
      aggregatableFields:
        - fieldName: Total
          aggregateExpression: Float_aggregate_exp
  graphql:
    selectTypeName: Invoice_aggregate_fields
  description: Aggregate over Invoices
```


#### AggregateExpressionV1 {#aggregateexpression-aggregateexpressionv1}

Definition of how to aggregate over a particular operand type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [AggregateExpressionName](#aggregateexpression-aggregateexpressionname) | true | The name of the aggregate expression. |
| `operand` | [AggregateOperand](#aggregateexpression-aggregateoperand) | true | The type this aggregation expression aggregates over, and its associated configuration |
| `count` | [AggregateCountDefinition](#aggregateexpression-aggregatecountdefinition) / null | false | Configuration for the count aggregate function used over the operand |
| `countDistinct` | [AggregateCountDefinition](#aggregateexpression-aggregatecountdefinition) / null | false | Configuration for the count distinct aggregate function used over the operand |
| `graphql` | [AggregateExpressionGraphQlDefinition](#aggregateexpression-aggregateexpressiongraphqldefinition) / null | false | Configuration for how this command should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the aggregate expression. Gets added to the description of the command's root field in the GraphQL schema. |



#### AggregateExpressionGraphQlDefinition {#aggregateexpression-aggregateexpressiongraphqldefinition}

The definition of how an aggregate expression should appear in the GraphQL API.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `selectTypeName` | [GraphQlTypeName](#aggregateexpression-graphqltypename) | true | The type name to use for the aggregate selection type |
| `deprecated` | [Deprecated](#aggregateexpression-deprecated) / null | false | Whether this command root field is deprecated. If set, this will be added to the graphql schema as a deprecated field. |

 **Example:**

```yaml
selectTypeName: Invoice_aggregate_fields
```


#### Deprecated {#aggregateexpression-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### GraphQlTypeName {#aggregateexpression-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### AggregateCountDefinition {#aggregateexpression-aggregatecountdefinition}

Definition of a count aggregation function

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enable` | boolean | true | Whether or not the aggregate function is available for use or not |
| `description` | string / null | false | A description of the aggregation function. Gets added to the description of the field in the GraphQL schema. |
| `returnType` | [TypeName](#aggregateexpression-typename) / null | false | The scalar type that the count aggregation function returns. Must be an integer type. If omitted, Int is used as the default. |



#### AggregateOperand {#aggregateexpression-aggregateoperand}

Definition of an aggregate expression's operand


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `object` | [ObjectAggregateOperand](#aggregateexpression-objectaggregateoperand) | false | Definition of an aggregate over an object-typed operand |
| `scalar` | [ScalarAggregateOperand](#aggregateexpression-scalaraggregateoperand) | false | Definition of an aggregate over a scalar-typed operand |



#### ScalarAggregateOperand {#aggregateexpression-scalaraggregateoperand}

Definition of an aggregate over a scalar-typed operand

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregatedType` | [TypeName](#aggregateexpression-typename) | true | The name of the scalar type the aggregate expression is aggregating |
| `aggregationFunctions` | [[AggregationFunctionDefinition](#aggregateexpression-aggregationfunctiondefinition)] | true | The aggregation functions that operate over the scalar type |
| `dataConnectorAggregationFunctionMapping` | [[DataConnectorAggregationFunctionMapping](#aggregateexpression-dataconnectoraggregationfunctionmapping)] | true | Mapping of aggregation functions to corresponding aggregation functions in various data connectors |



#### DataConnectorAggregationFunctionMapping {#aggregateexpression-dataconnectoraggregationfunctionmapping}

Definition of how to map an aggregate expression's aggregation functions to data connector aggregation functions.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `dataConnectorName` | [DataConnectorName](#aggregateexpression-dataconnectorname) | true | The data connector being mapped to |
| `dataConnectorScalarType` | [DataConnectorScalarType](#aggregateexpression-dataconnectorscalartype) | true | The matching scalar type in the data connector for the operand scalar type |
| `functionMapping` | [AggregationFunctionMappings](#aggregateexpression-aggregationfunctionmappings) | true | Mapping from Open DD aggregation function to data connector aggregation function |



#### AggregationFunctionMappings {#aggregateexpression-aggregationfunctionmappings}

Mapping of aggregation functions to their matching aggregation functions in the data connector.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [AggregateFunctionMapping](#aggregateexpression-aggregatefunctionmapping) | false | Definition of how to map the aggregation function to a function in the data connector |



#### AggregateFunctionMapping {#aggregateexpression-aggregatefunctionmapping}

Definition of how to map the aggregation function to a function in the data connector

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [DataConnectorAggregationFunctionName](#aggregateexpression-dataconnectoraggregationfunctionname) | true | The name of the aggregation function in the data connector |



#### DataConnectorAggregationFunctionName {#aggregateexpression-dataconnectoraggregationfunctionname}

The name of an aggregation function in a data connector


**Value:** string


#### DataConnectorScalarType {#aggregateexpression-dataconnectorscalartype}

The name of a scalar type in a data connector.


**Value:** string


#### DataConnectorName {#aggregateexpression-dataconnectorname}

The name of a data connector.


**Value:** string


#### AggregationFunctionDefinition {#aggregateexpression-aggregationfunctiondefinition}

Definition of an aggregation function

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [AggregationFunctionName](#aggregateexpression-aggregationfunctionname) | true | The name of the aggregation function |
| `description` | string / null | false | A description of the aggregation function. Gets added to the description of the field in the GraphQL schema. |
| `returnType` | [TypeReference](#aggregateexpression-typereference) | true |  |



#### TypeReference {#aggregateexpression-typereference}

A reference to an Open DD type including nullable values and arrays.
Suffix '!' to indicate a non-nullable reference, and wrap in '[]' to indicate an array.
Eg: '[String!]!' is a non-nullable array of non-nullable strings.


**Value:** string


#### AggregationFunctionName {#aggregateexpression-aggregationfunctionname}

The name of an aggregation function.


**Value:** string


#### TypeName {#aggregateexpression-typename}

The name of the scalar type the aggregate expression is aggregating


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#aggregateexpression-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#aggregateexpression-customtypename) |  |



#### InbuiltType {#aggregateexpression-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### ObjectAggregateOperand {#aggregateexpression-objectaggregateoperand}

Definition of an aggregate over an object-typed operand

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregatedType` | [CustomTypeName](#aggregateexpression-customtypename) | true | The name of the object type the aggregate expression is aggregating |
| `aggregatableFields` | [[AggregatableFieldDefinition](#aggregateexpression-aggregatablefielddefinition)] | true | The fields on the object that are aggregatable |



#### AggregatableFieldDefinition {#aggregateexpression-aggregatablefielddefinition}

Definition of an aggregatable field on an object type

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#aggregateexpression-fieldname) | true | The name of the field on the operand aggregated type that is aggregatable |
| `description` | string / null | false | A description of the aggregatable field. Gets added to the description of the field in the GraphQL schema. |
| `aggregateExpression` | [AggregateExpressionName](#aggregateexpression-aggregateexpressionname) | true | The aggregate expression used to aggregate the type of the field |



#### FieldName {#aggregateexpression-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#aggregateexpression-customtypename}

The name of a user-defined type.


**Value:** string


#### AggregateExpressionName {#aggregateexpression-aggregateexpressionname}

The name of an aggregate expression.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/orderby-expressions.mdx ---
# Orderby Expressions

---
sidebar_position: 10
sidebar_label: Orderby Expressions
description:
  "Explore OrderBy Expressions in Hasura, designed for sorting data. Discover how to configure sorting rules for various
  data types and expose them through your API."
keywords:
  - hasura documentation
  - graphql data structure
  - data domain mapping
  - hasura ddn
  - hasura data specification
  - graphql schema
  - orderby expressions
toc_max_heading_level: 4
seoFrontMatterUpdated: false
---

# OrderBy Expressions

## Introduction

Hasura provides powerful tools to control sorting and ordering data. OrderByExpression types let you define customizable
sorting rules for a [type](/reference/metadata-reference/types.mdx).

## How OrderByExpressions work

There are two types of OrderByExpressions:

| Type              | Description                                                |
| ----------------- | ---------------------------------------------------------- |
| [Scalar](#scalar) | This specifies how a user is able to order a scalar field. |
| [Object](#object) | This specifies how fields of a type can be ordered.        |

### Examples

#### Scalar

This specifies how a user is able to order a scalar field. For instance, you might want to say that a user can only sort
a `String` type in ascending order instead of both ascending and descending order. You can do that with the following
metadata:

```yaml
kind: OrderByExpression
version: v1
definition:
  name: String_orderby_exp_asc
  operand:
    scalar:
      orderedType: String
      enableOrderByDirections:
        enableSpecific:
          - Asc
  graphql:
    expressionTypeName: StringOrderByExp
```

#### Object

An object `OrderByExpression` is used to define how fields of a type can be ordered. You can specify which
`OrderByExpression` is used to order each field or relationship, and then defer the mappings of individual scalar types
to those `OrderByExpression`s.

```yaml
kind: OrderByExpression
version: v1
definition:
  name: Album_orderby_exp
  operand:
    object:
      orderedType: Album
      orderableFields:
        - fieldName: AlbumId
          orderByExpression: Int_orderby_exp
        - fieldName: ArtistId
          orderByExpression: Int_orderby_exp_asc
        - fieldName: Address
          orderByExpression: Address_orderby_exp
      orderableRelationships:
        - relationshipName: artist
          orderByExpression: Artist_orderby_exp
        - relationshipName: genre
  graphql:
    expressionTypeName: Album_orderby_Exp
```

Note here that we can specify different orderBy operators for `AlbumId` and `ArtistId` by using different
`OrderByExpression`s for them. We are also able to define ordering on nested objects such as `Address`.

---

## Metadata structure


### OrderByExpression {#orderbyexpression-orderbyexpression}

Definition of an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `OrderByExpression` | true |  |
| `version` | `v1` | true |  |
| `definition` | [OrderByExpressionV1](#orderbyexpression-orderbyexpressionv1) | true | Definition of a type representing an order by expression on an OpenDD type. |

 **Examples:**

```yaml
kind: OrderByExpression
version: v1
definition:
  name: Album_order_by_exp
  operand:
    object:
      orderedType: Album
      orderableFields:
        - fieldName: AlbumId
          orderByExpression: Int_order_by_exp
        - fieldName: ArtistId
          orderByExpression: Int_order_by_exp
        - fieldName: Address
          orderByExpression: Address_order_by_default_exp
      orderableRelationships:
        - relationshipName: artist
          orderByExpression: Artist_order_by_default_exp
  graphql:
    expressionTypeName: App_Album_order_by_exp
  description: Order by expression for Albums
```



```yaml
kind: OrderByExpression
version: v1
definition:
  name: Int_order_by_exp
  operand:
    scalar:
      orderedType: Int
      enableOrderByDirections:
        enableAll: true
  graphql:
    expressionTypeName: App_Int_order_by_exp
  description: Order by expression for Int
```


#### OrderByExpressionV1 {#orderbyexpression-orderbyexpressionv1}

Definition of a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [OrderByExpressionName](#orderbyexpression-orderbyexpressionname) | true | The name used to refer to this expression. This name is unique only in the context of the `orderedType` |
| `operand` | [OrderByExpressionOperand](#orderbyexpression-orderbyexpressionoperand) | true | The type that this expression applies to. |
| `graphql` | [OrderByExpressionGraphQlConfiguration](#orderbyexpression-orderbyexpressiongraphqlconfiguration) / null | false | Configuration for how this order by expression should appear in the GraphQL schema. |
| `description` | string / null | false | The description of the order by expression. |



#### OrderByExpressionGraphQlConfiguration {#orderbyexpression-orderbyexpressiongraphqlconfiguration}

GraphQL configuration settings for a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `expressionTypeName` | [GraphQlTypeName](#orderbyexpression-graphqltypename) | true | The name of a GraphQL type. |



#### GraphQlTypeName {#orderbyexpression-graphqltypename}

The name of a GraphQL type.


**Value:** string


#### OrderByExpressionOperand {#orderbyexpression-orderbyexpressionoperand}

Configuration for object or scalar order by expression


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `object` | [OrderByExpressionObjectOperand](#orderbyexpression-orderbyexpressionobjectoperand) | false | Definition of an type representing an order by expression on an OpenDD object type. |
| `scalar` | [OrderByExpressionScalarOperand](#orderbyexpression-orderbyexpressionscalaroperand) | false | Definition of a type representing an order by expression on an OpenDD scalar type. |



#### OrderByExpressionScalarOperand {#orderbyexpression-orderbyexpressionscalaroperand}

Definition of a type representing an order by expression on an OpenDD scalar type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `orderedType` | [TypeName](#orderbyexpression-typename) | true | The type that this expression applies to. |
| `enableOrderByDirections` | [EnableAllOrSpecific](#orderbyexpression-enableallorspecific) | true | Order by directions supported by this scalar type. |



#### EnableAllOrSpecific {#orderbyexpression-enableallorspecific}

Enable all or specific values.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableAll` | boolean | false |  |
| `enableSpecific` | [[OrderByDirection](#orderbyexpression-orderbydirection)] | false |  |



#### OrderByDirection {#orderbyexpression-orderbydirection}


**Value:** `Asc` / `Desc`


#### TypeName {#orderbyexpression-typename}

The type that this expression applies to.


**One of the following values:**

| Value | Description |
|-----|-----|
| [InbuiltType](#orderbyexpression-inbuilttype) | An inbuilt primitive OpenDD type. |
| [CustomTypeName](#orderbyexpression-customtypename) |  |



#### InbuiltType {#orderbyexpression-inbuilttype}

An inbuilt primitive OpenDD type.


**Value:** `ID` / `Int` / `Float` / `Boolean` / `String`


#### OrderByExpressionObjectOperand {#orderbyexpression-orderbyexpressionobjectoperand}

Definition of an type representing an order by expression on an OpenDD object type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `orderedType` | [CustomTypeName](#orderbyexpression-customtypename) | true | The type that this expression applies to. |
| `orderableFields` | [[OrderByExpressionOrderableField](#orderbyexpression-orderbyexpressionorderablefield)] | true | Orderable fields of the `orderedType` |
| `orderableRelationships` | [[OrderByExpressionOrderableRelationship](#orderbyexpression-orderbyexpressionorderablerelationship)] | true | Orderable relationships |



#### OrderByExpressionOrderableRelationship {#orderbyexpression-orderbyexpressionorderablerelationship}

Definition of a relationship in a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `relationshipName` | [RelationshipName](#orderbyexpression-relationshipname) | true | The name of the relationship. |
| `orderByExpression` | [OrderByExpressionName](#orderbyexpression-orderbyexpressionname) / null | false | The OrderByExpression to use for this relationship. This is optional for model relationships. If not specified we use the model's OrderByExpression configuration. For local command relationships this is required. |



#### RelationshipName {#orderbyexpression-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### OrderByExpressionOrderableField {#orderbyexpression-orderbyexpressionorderablefield}

Definition of a field in a type representing an order by expression on an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#orderbyexpression-fieldname) | true |  |
| `orderByExpression` | [OrderByExpressionName](#orderbyexpression-orderbyexpressionname) | true | OrderByExpression to use for this field. |



#### FieldName {#orderbyexpression-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### CustomTypeName {#orderbyexpression-customtypename}

The name of a user-defined type.


**Value:** string


#### OrderByExpressionName {#orderbyexpression-orderbyexpressionname}

The name of an order by expression.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/relationships.mdx ---
# Relationships

---
sidebar_position: 11
sidebar_label: Relationships
description:
  "Expand your understanding of relationships in Hasura, learn how to define linked or nested data queries, create
  relationships between types and models. Explore mapping relationships with in-depth annotated examples and explore
  connecting data across namespaces."
keywords:
  - hasura relationships
  - data mapping
  - graphql queries
  - linked data queries
  - nested data queries
  - hasura data modeling
  - data connectivity
  - hasura data sources
  - hasura api configuration
toc_max_heading_level: 4
seoFrontMatterUpdated: true
---

import GraphiQLIDE from "@site/src/components/GraphiQLIDE";

# Relationships

## Introduction

Defining a relationship allows you to make queries across linked information.

Relationships are defined in metadata **from an [object type](./types.mdx#objecttype-objecttype), to a
[model](./models.mdx) or [command](./commands.mdx)**.

This enables you to create complex queries, where you can fetch related data in a single query.

By defining a `Relationship` object, **all [models](./models.mdx) or [commands](./commands.mdx) whose output type is the
source object type defined in the relationship will now have a connection to the target model or command.**

So from this relationship definition, all models or commands in the supergraph whose output type is the `orders` object
type will now have a connection to the `customers` model via the `customer` relationship field on the `orders` type.

So, as in example 1 below, you can now fetch the customer when you query orders. Note that also, if you had, for
instance an arbitrary command such as `getCustomerLastOrder` which also returned an order from a customer id, you can
now also return the customer's details for that returned order in the same single query from this relationship.

## How relationships work

### Lifecycle

You can automatically add foreign-key relationships to your metadata
[using the CLI](/reference/cli/commands/ddn_relationship_add.mdx).

Additionally, you can manually define relationships from an object type to a model or command in your metadata using the
VS Code extension. The extension knows about your data sources and can help you write relationships more efficiently.

Simply start by typing a delimiter (`---`) at the end of a model's metadata file and start typing `Relationship`. The
extension will provide you with auto-completion, syntax highlighting, and error checking as you write your
relationships. At any point in the process, you can type `Ctrl + Space` to see the available options and tab through the
fields to fill in the required information.

To make a relationship available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: Relationship
version: v1
definition:
  name: customer
  sourceType: orders
  target:
    model:
      name: customers
      subgraph: customers
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: customerId
      target:
        modelField:
          - fieldName: customerId
```

| **Field**                                    | **Description**                                                                                                        | **Reference**                                                    |
| -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **kind**                                     | Indicates that this object is defining a relationship between two data types or models.                                | [Relationship](#relationship-relationship)                       |
| **version**                                  | Specifies the version of the relationship's structure.                                                                 | [RelationshipV1](#relationship-relationshipv1)                   |
| **definition.name**                          | The name of the relationship, representing the link between the source and target.                                     | [RelationshipName](#relationship-relationshipname)               |
| **definition.sourceType**                    | Defines the source object type that the relationship starts from.                                                      | [CustomTypeName](#relationship-customtypename)                   |
| **definition.target.model.name**             | The name of the target model to which the source is related.                                                           | [ModelName](#relationship-modelname)                             |
| **definition.target.model.subgraph**         | Specifies the subgraph in which the target model resides.                                                              | [ModelRelationshipTarget](#relationship-modelrelationshiptarget) |
| **definition.target.model.relationshipType** | Indicates the type of relationship (Object or Array), determining whether one or many related items can be fetched.    | [RelationshipType](#relationship-relationshiptype)               |
| **definition.mapping**                       | Defines how fields from the source map to fields or arguments in the target, establishing the connection between them. | [RelationshipMapping](#relationship-relationshipmapping)         |

#### Object Type to a Model

As mentioned above, in an e-commerce context, you will likely have customers and orders, and you want to relate them so
that when you query orders you can easily fetch the customer on that order.

To do this in DDN metadata for this example, you might have an `orders` **model** which returns an `orders` **object
type** and you want to relate that to a `customers` **model** and whatever object type it returns.

**Example:** Fetch a list of orders along with the customer details for each order:

<GraphiQLIDE
  query={`query OrdersAndCustomers {
  orders {
    orderId
    orderDate
    customer {
      customerId
      name
      email
    }
  }
}`}
  response={`{
  "data": {
    "orders": [
      {
        "orderId": "ORD001",
        "orderDate": "2024-05-10",
        "customer": {
          "customerId": "CUST001",
          "name": "John Doe",
          "email": "john.doe@example.com"
        }
      },
      {
        "orderId": "ORD002",
        "orderDate": "2024-05-11",
        "customer": {
          "customerId": "CUST002",
          "name": "Jane Smith",
          "email": "jane.smith@example.com"
        }
      }
    ]
  }
}`}
/>

Here is the corresponding relationship configuration which enables this query:

```yaml
kind: Relationship
version: v1
definition:
  name: customer
  sourceType: orders
  target:
    model:
      name: customers
      subgraph: customers
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: customerId
      target:
        modelField:
          - fieldName: customerId
  description: The customer details for an order
```

Now, you can also retrieve the customer details for any model or command in your schema that returns the `orders` object
type.

For convenience, this relationship configuration would best be located with your `orders` object type and model.

#### Object Type to a Command

Let's say you have a `user` object type in your graph, and also a command which responds with the current logged-in user
information (say `getLoggedInUserInfo`). Now you can link these two, by defining a relationship from the `user` object
type to `getLoggedInUserInfo`.

Let's say we name the relationship `currentSession`. Now you can make a single query from your client to get a user's
data and their current session information.

**Example:** fetch a list of users and the current session information of each user:

<GraphiQLIDE
  query={`query UsersAndCurrentSession {
  users {
    id
    username
    currentSession {
      activeSince
    }
  }
}`}
  response={`{
  "data": {
    "users": [
      {
        "id": 1,
        "username": "sit_amet",
        "currentSession": {
          "activeSince": "2024-04-01T07:08:22+0000"
        }
      },
      {
        "id": 2,
        "username": "fancy_nibh",
        "currentSession": {
          "activeSince": "2024-04-01T07:08:22+0000"
        }
      },
      {
        "id": 3,
        "username": "just_joe",
        "currentSession": {
          "activeSince": "2024-04-01T07:08:22+0000"
        }
      }
    ]
  }
}`}
/>

Here is the corresponding relationship configuration which enables this query:

```yaml
kind: Relationship
version: v1
definition:
  name: currentSession
  sourceType: user
  target:
    command:
      name: getLoggedInUserInfo
      subgraph: users
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        argument:
          argumentName: user_id
  description: The current session information for the user
```

#### Command to Command

Hasura DDN also allows you to link commands together from a source type to query related data.

**Example:** fetch the result of one command and use it as input for another command:

<GraphiQLIDE
  query={`query TrackOrder {
    trackOrder(orderId: "ORD12345") {
      trackingNumber
      shippingDetails {
        carrier
        estimatedDeliveryDate
        currentStatus
      }
    }
  }
`}
  response={`{
  "data": {
    "trackOrder": {
      "trackingNumber": "1Z9999999999999999",
      "shippingDetails": {
        "carrier": "UPS",
        "estimatedDeliveryDate": "2024-05-25",
        "currentStatus": "In Transit"
      }
    }
  }
}
`}
/>

And the corresponding relationship configuration which enables this query:

```yaml
kind: Relationship
version: v1
definition:
  name: shippingDetails
  sourceType: trackOrder
  target:
    command:
      name: getShippingDetails
      subgraph: orders
  mapping:
    - source:
        fieldPath:
          - fieldName: trackingNumber
      target:
        argument:
          argumentName: trackingNumber
  description: The shipping details for an order based on its tracking number
```

:::info VS Code extension assisted authoring

Relationships are written in your various HML files. If you're using a compatible editor (such as VS Code with the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura)), assisted authoring
will help you create relationships more efficiently. It will provide you with auto-completion, syntax highlighting, and
error checking as you write your relationships.

The CLI also works to automatically track your relationships for you whenever you add or update a data connector.

:::

---

## Metadata structure


### Relationship {#relationship-relationship}

Definition of a relationship on an OpenDD type which allows it to be extended with related models or commands.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `Relationship` | true |  |
| `version` | `v1` | true |  |
| `definition` | [RelationshipV1](#relationship-relationshipv1) | true | Definition of a relationship on an OpenDD type which allows it to be extended with related models or commands. |

 **Example:**

```yaml
kind: Relationship
version: v1
definition:
  name: Articles
  sourceType: author
  target:
    model:
      name: Articles
      subgraph: null
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: author_id
      target:
        modelField:
          - fieldName: author_id
  description: Articles written by an author
```


#### RelationshipV1 {#relationship-relationshipv1}

Definition of a relationship on an OpenDD type which allows it to be extended with related models or commands.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [RelationshipName](#relationship-relationshipname) | true | The name of the relationship. |
| `sourceType` | [CustomTypeName](#relationship-customtypename) | true | The source type of the relationship. |
| `target` | [RelationshipTarget](#relationship-relationshiptarget) | true | The target of the relationship. |
| `mapping` | [[RelationshipMapping](#relationship-relationshipmapping)] | true | The mapping configuration of source to target for the relationship. |
| `description` | string / null | false | The description of the relationship. Gets added to the description of the relationship in the graphql schema. |
| `deprecated` | [Deprecated](#relationship-deprecated) / null | false | Whether this relationship is deprecated. If set, the deprecation status is added to the relationship field's graphql schema. |
| `graphql` | [RelationshipGraphQlDefinition](#relationship-relationshipgraphqldefinition) / null | false | Configuration for how this relationship should appear in the GraphQL schema. |



#### RelationshipGraphQlDefinition {#relationship-relationshipgraphqldefinition}

The definition of how a relationship appears in the GraphQL API

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregateFieldName` | [FieldName](#relationship-fieldname) / null | false | The field name to use for the field that represents an aggregate over the relationship |



#### Deprecated {#relationship-deprecated}

OpenDd configuration to indicate whether an object type field, relationship, model root field or command root field is deprecated.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `reason` | string / null | false | The reason for deprecation. |



#### RelationshipMapping {#relationship-relationshipmapping}

Definition of a how a particular field in the source maps to a target field or argument.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `source` | [RelationshipMappingSource](#relationship-relationshipmappingsource) | true | The source configuration for this relationship mapping. |
| `target` | [RelationshipMappingTarget](#relationship-relationshipmappingtarget) | true | The target configuration for this relationship mapping. |

 **Example:**

```yaml
source:
  fieldPath:
    - fieldName: author_id
target:
  modelField:
    - fieldName: author_id
```


#### RelationshipMappingTarget {#relationship-relationshipmappingtarget}

The target configuration for a relationship mapping.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [ArgumentMappingTarget](#relationship-argumentmappingtarget) | false | An argument target for a relationship mapping. |
| `modelField` | [[RelationshipSourceFieldAccess](#relationship-relationshipsourcefieldaccess)] | false |  |



#### ArgumentMappingTarget {#relationship-argumentmappingtarget}

An argument target for a relationship mapping.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argumentName` | [ArgumentName](#relationship-argumentname) | true |  |



#### ArgumentName {#relationship-argumentname}

The name of an argument.


**Value:** string


#### RelationshipMappingSource {#relationship-relationshipmappingsource}

The source configuration for a relationship mapping.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | [ValueExpression](#relationship-valueexpression) | false | An expression which evaluates to a value that can be used in permissions and various presets. |
| `fieldPath` | [[RelationshipSourceFieldAccess](#relationship-relationshipsourcefieldaccess)] | false |  |



#### RelationshipSourceFieldAccess {#relationship-relationshipsourcefieldaccess}

A field access in a relationship mapping.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [FieldName](#relationship-fieldname) | true |  |



#### FieldName {#relationship-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### ValueExpression {#relationship-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false |  |
| `valueFromEnv` | string | false |  |



#### RelationshipTarget {#relationship-relationshiptarget}

The target for a relationship.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `model` | [ModelRelationshipTarget](#relationship-modelrelationshiptarget) | false | The target model for a relationship. |
| `command` | [CommandRelationshipTarget](#relationship-commandrelationshiptarget) | false | The target command for a relationship. |

 **Example:**

```yaml
model:
  name: Articles
  subgraph: null
  relationshipType: Array
```


#### CommandRelationshipTarget {#relationship-commandrelationshiptarget}

The target command for a relationship.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [CommandName](#relationship-commandname) | true | The name of the command. |
| `subgraph` | string / null | false | The subgraph of the target command. Defaults to the current subgraph. |



#### CommandName {#relationship-commandname}

The name of a command.


**Value:** string


#### ModelRelationshipTarget {#relationship-modelrelationshiptarget}

The target model for a relationship.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [ModelName](#relationship-modelname) | true | The name of the data model. |
| `subgraph` | string / null | false | The subgraph of the target model. Defaults to the current subgraph. |
| `relationshipType` | [RelationshipType](#relationship-relationshiptype) | true | Type of the relationship - object or array. |
| `aggregate` | [ModelRelationshipTargetAggregate](#relationship-modelrelationshiptargetaggregate) / null | false | How to aggregate over the relationship. Only valid for array relationships |



#### ModelRelationshipTargetAggregate {#relationship-modelrelationshiptargetaggregate}

Which aggregate expression to use to aggregate the array relationship.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `aggregateExpression` | [AggregateExpressionName](#relationship-aggregateexpressionname) | true | The name of the aggregate expression that defines how to aggregate across the relationship. |
| `description` | string / null | false | The description of the relationship aggregate. Gets added to the description of the aggregate field in the GraphQL schema |



#### AggregateExpressionName {#relationship-aggregateexpressionname}

The name of an aggregate expression.


**Value:** string


#### RelationshipType {#relationship-relationshiptype}

Type of the relationship.


**One of the following values:**

| Value | Description |
|-----|-----|
| `Object` | Select one related object from the target. |
| `Array` | Select multiple related objects from the target. |



#### ModelName {#relationship-modelname}

The name of data model.


**Value:** string


#### CustomTypeName {#relationship-customtypename}

The name of a user-defined type.


**Value:** string


#### RelationshipName {#relationship-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/permissions.mdx ---
# Permissions

---
sidebar_position: 12
sidebar_label: Permissions
description:
  "This page provides an understanding of how to define permissions, control access or set authorization rules on output
  types, models, and commands in your supergraph. It essentially makes management of roles and accessibility simple and
  efficient."
toc_max_heading_level: 4
keywords:
  - hasura
  - hasura permissions guide
  - api authorization
  - define permissions
  - model permissions
  - command permissions
  - data domains
  - api control rules
  - access management
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Permissions

## Introduction

Access control rules are essential for securing your data and ensuring that only authorized users can access it. In
Hasura, these are referred to as **permissions**. You can use permissions to control access to certain rows or columns
in your database, or to restrict access to certain operations or fields in your GraphQL API.

The following types of permissions can be defined:

| Type                                                         | Description                                                                       |
| ------------------------------------------------------------ | --------------------------------------------------------------------------------- |
| [TypePermissions](#typepermissions)                          | Define which fields are allowed to be accessed by a role on an output type.       |
| [ModelPermissions](#modelpermissions)                        | Define which objects or rows within a model are allowed to be accessed by a role. |
| [CommandPermissions](#commandpermissions-commandpermissions) | Define whether the command is executable by a role.                               |

## How TypePermissions work {#typepermissions}

### Lifecycle

By default, whenever a new type is created in your supergraph, each field is only accessible to the `admin` role. You
can think of these as permissions on columns in a database table.

You can quickly generate new roles by adding a new item to the `permissions` array in the TypePermissions object. Each
item in the array should have a `role` field and an `output` field. The `output` field should contain an `allowedFields`
array, which lists the fields that are accessible to the role when the type is used in an output context.

To make a new TypePermission or role available in your supergraph, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: TypePermissions
version: v1
definition:
  typeName: article
  permissions:
    - role: admin
      output:
        allowedFields:
          - article_id
          - author_id
          - title
    - role: user
      output:
        allowedFields:
          - article_id
          - author_id
```

| **Field**                                         | **Description**                                                                                   | **Reference**                                                 |
| ------------------------------------------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| **kind**                                          | Indicates that this object is defining permissions for a specific type.                           | [TypePermissions](#typepermissions-typepermissions)           |
| **version**                                       | Specifies the version of the type permissions structure.                                          | [TypePermissionsV1](#typepermissions-typepermissionsv1)       |
| **definition.typeName**                           | The name of the type for which permissions are being defined.                                     | [CustomTypeName](#typepermissions-customtypename)             |
| **definition.permissions**                        | A list of permissions specifying which fields are accessible for different roles.                 | [TypePermission](#typepermissions-typepermission)             |
| **definition.permissions[].role**                 | The role for which permissions are being defined.                                                 | [Role](#typepermissions-role)                                 |
| **definition.permissions[].output.allowedFields** | The fields of the type that are accessible for a role when the type is used in an output context. | [TypeOutputPermission](#typepermissions-typeoutputpermission) |

## How ModelPermissions work {#modelpermissions}

### Lifecycle

By default, whenever a new model is created in your supergraph, all records are only accessible to the `admin` role. You
can think of these as permissions on rows in a database table.

You can restrict access to certain rows by adding a new item to the `permissions` array in the `ModelPermissions`
object. Each item in the array should have a `role` field and a `select` field. The `select` field should contain a
`filter` expression that determines which rows are accessible to the role when selecting from the model.

Most commonly, you'll use session variables â€” provided via your configured
[authentication mechanism](/auth/overview.mdx) â€” to restrict access to rows based on the user's identity or other
criteria. You can also use argument presets to enforce row-level security. You can see an example of this below.

To make a new ModelPermission or role available in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          fieldComparison:
            field: author_id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
```

| **Field**                                  | **Description**                                                                                            | **Reference**                                              |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| **kind**                                   | Indicates that this object is defining permissions for a specific model.                                   | [ModelPermissions](#modelpermissions-modelpermissions)     |
| **version**                                | Specifies the version of the model permissions structure.                                                  | [ModelPermissionsV1](#modelpermissions-modelpermissionsv1) |
| **definition.modelName**                   | The name of the model for which permissions are being defined.                                             | [ModelName](#modelpermissions-modelname)                   |
| **definition.permissions**                 | A list of permissions specifying which rows or objects are accessible for different roles.                 | [ModelPermission](#modelpermissions-modelpermission)       |
| **definition.permissions[].role**          | The role for which permissions are being defined.                                                          | [Role](#modelpermissions-role)                             |
| **definition.permissions[].select.filter** | A filter expression that determines which rows are accessible for this role when selecting from the model. | [ModelPredicate](#modelpermissions-modelpredicate)         |

## How CommandPermissions work {#commandpermissions}

### Lifecycle

By default, whenever a new command is created in your supergraph, it is only executable by the `admin` role.

You can enable or restrict access to commands by adding a new item to the `permissions` array in the
`CommandPermissions` object. Each item in the array should have a `role` field and an `allowExecution` field. The
`allowExecution` field should be set to `true` if the command is executable by the role.

Further, you can use argument presets to pass actual logical expressions to your data sources to control how they do
things.

For example, a data connector might expose a `Command` called `delete_user_by_id` with two arguments - `user_id` and
`pre_check`. `user_id` is the primary key of the user you'd like to remove, and `pre_check` lets you provide a custom
boolean expression.

```yaml
kind: CommandPermissions
version: v1
definition:
  commandName: delete_user_by_id
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: pre_check
          value:
            booleanExpression:
              fieldComparison:
                field: is_invincible
                operator: _eq
                value:
                  literal: false
```

Now, when `admin` runs this command, once again, they can do what they want, and provide their own `pre_check` if they
want. The `user` however, is able to pass a `user_id` argument, but the `pre_check` expression is passed to the data
connector which will only let them delete the row if the row's `is_invincible` value is set to `false`.

To make a execution of a command available to a role in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml
---
kind: CommandPermissions
version: v1
definition:
  commandName: get_article_by_id
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
      argumentPresets:
        - argument: id
          value:
            literal: 100
```

| **Field**                                    | **Description**                                                                         | **Reference**                                                    |
| -------------------------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **kind**                                     | Indicates that this object is defining permissions for a specific command.              | [CommandPermissions](#commandpermissions-commandpermissions)     |
| **version**                                  | Specifies the version of the command permissions structure.                             | [CommandPermissionsV1](#commandpermissions-commandpermissionsv1) |
| **definition.commandName**                   | The name of the command for which permissions are being defined.                        | [CommandName](#commandpermissions-commandname)                   |
| **definition.permissions**                   | A list of permissions specifying whether the command is executable by different roles.  | [CommandPermission](#commandpermissions-commandpermission)       |
| **definition.permissions[].role**            | The role for which permissions are being defined.                                       | [Role](#commandpermissions-role)                                 |
| **definition.permissions[].allowExecution**  | Indicates whether the command is executable by the role.                                | [CommandPermission](#commandpermissions-commandpermission)       |
| **definition.permissions[].argumentPresets** | Preset values for arguments that are enforced when the command is executed by the role. | [ArgumentPreset](#commandpermissions-argumentpreset)             |

---

## Metadata structure


### TypePermissions {#typepermissions-typepermissions}

Definition of permissions for an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `TypePermissions` | true |  |
| `version` | `v1` | true |  |
| `definition` | [TypePermissionsV1](#typepermissions-typepermissionsv1) | true | Definition of permissions for an OpenDD type. |

 **Example:**

```yaml
kind: TypePermissions
version: v1
definition:
  typeName: article
  permissions:
    - role: admin
      output:
        allowedFields:
          - article_id
          - author_id
          - title
    - role: user
      output:
        allowedFields:
          - article_id
          - author_id
```


#### TypePermissionsV1 {#typepermissions-typepermissionsv1}

Definition of permissions for an OpenDD type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `typeName` | [CustomTypeName](#typepermissions-customtypename) | true | The name of the type for which permissions are being defined. Must be an object type. |
| `permissions` | [[TypePermission](#typepermissions-typepermission)] | true | A list of type permissions, one for each role. |



#### TypePermission {#typepermissions-typepermission}

Defines permissions for a particular role for a type.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#typepermissions-role) | true | The role for which permissions are being defined. |
| `output` | [TypeOutputPermission](#typepermissions-typeoutputpermission) / null | false | Permissions for this role when this type is used in an output context. If null, this type is inaccessible for this role in an output context. |
| `input` | [TypeInputPermission](#typepermissions-typeinputpermission) / null | false | Permissions for this role when this type is used in an input context. If null, this type is accessible for this role in an input context. |

 **Example:**

```yaml
role: user
output:
  allowedFields:
    - article_id
    - author_id
input:
  fieldPresets:
    - field: author_id
      value:
        sessionVariable: x-hasura-user-id
```


#### TypeInputPermission {#typepermissions-typeinputpermission}

Permissions for a type for a particular role when used in an input context.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldPresets` | [[FieldPreset](#typepermissions-fieldpreset)] | false | Preset values for fields of the type |



#### FieldPreset {#typepermissions-fieldpreset}

Preset value for a field

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#typepermissions-fieldname) | true | Field name for preset |
| `value` | [ValueExpression](#typepermissions-valueexpression) | true | Value for preset |



#### ValueExpression {#typepermissions-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false |  |
| `valueFromEnv` | string | false |  |



#### TypeOutputPermission {#typepermissions-typeoutputpermission}

Permissions for a type for a particular role when used in an output context.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `allowedFields` | [[FieldName](#typepermissions-fieldname)] | true | Fields of the type that are accessible for a role |



#### FieldName {#typepermissions-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### Role {#typepermissions-role}

The role for which permissions are being defined.


**Value:** string


#### CustomTypeName {#typepermissions-customtypename}

The name of a user-defined type.


**Value:** string
### ModelPermissions {#modelpermissions-modelpermissions}

Definition of permissions for an OpenDD model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `ModelPermissions` | true |  |
| `version` | `v1` | true |  |
| `definition` | [ModelPermissionsV1](#modelpermissions-modelpermissionsv1) | true | Definition of permissions for an OpenDD model. |

 **Examples:**

```yaml
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          fieldComparison:
            field: author_id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
```



```yaml
kind: ModelPermissions
version: v1
definition:
  modelName: Articles
  permissions:
    - role: admin
      select:
        filter: null
    - role: user
      select:
        filter:
          relationship:
            name: author
            predicate:
              fieldComparison:
                field: id
                operator: _eq
                value:
                  sessionVariable: x-hasura-user-id
```


#### ModelPermissionsV1 {#modelpermissions-modelpermissionsv1}

Definition of permissions for an OpenDD model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `modelName` | [ModelName](#modelpermissions-modelname) | true | The name of the model for which permissions are being defined. |
| `permissions` | [[ModelPermission](#modelpermissions-modelpermission)] | true | A list of model permissions, one for each role. |



#### ModelPermission {#modelpermissions-modelpermission}

Defines the permissions for an OpenDD model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#modelpermissions-role) | true | The role for which permissions are being defined. |
| `select` | [SelectPermission](#modelpermissions-selectpermission) / null | false | The permissions for selecting from this model for this role. If this is null, the role is not allowed to query the model. |

 **Example:**

```yaml
role: user
select:
  filter:
    fieldComparison:
      field: author_id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
  argument_presets:
    - field: likes_dogs
      value:
        literal: true
```


#### SelectPermission {#modelpermissions-selectpermission}

Defines the permissions for selecting a model for a role.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `filter` | null / [ModelPredicate](#modelpermissions-modelpredicate) | true | Filter expression when selecting rows for this model. Null filter implies all rows are selectable. |
| `argumentPresets` | [[ArgumentPreset](#modelpermissions-argumentpreset)] | false | Preset values for arguments for this role |
| `allowSubscriptions` | boolean | false | Whether the role is allowed to subscribe to the root fields of this model. |



#### ArgumentPreset {#modelpermissions-argumentpreset}

Preset value for an argument

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [ArgumentName](#modelpermissions-argumentname) | true | Argument name for preset |
| `value` | [ValueExpressionOrPredicate](#modelpermissions-valueexpressionorpredicate) | true | Value for preset |



#### ValueExpressionOrPredicate {#modelpermissions-valueexpressionorpredicate}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false | Used to represent the name of a session variable, like "x-hasura-role". |
| `booleanExpression` | [ModelPredicate](#modelpermissions-modelpredicate) | false |  |
| `valueFromEnv` | string | false |  |



#### ArgumentName {#modelpermissions-argumentname}

The name of an argument.


**Value:** string


#### ModelPredicate {#modelpermissions-modelpredicate}

A predicate that can be used to restrict the objects returned when querying a model.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldComparison` | [FieldComparisonPredicate](#modelpermissions-fieldcomparisonpredicate) | false | Field comparison predicate filters objects based on a field value. |
| `fieldIsNull` | [FieldIsNullPredicate](#modelpermissions-fieldisnullpredicate) | false | Predicate to check if the given field is null. |
| `relationship` | [RelationshipPredicate](#modelpermissions-relationshippredicate) | false | Relationship predicate filters objects of a source model based on a predicate on the related model. |
| `and` | [[ModelPredicate](#modelpermissions-modelpredicate)] | false |  |
| `or` | [[ModelPredicate](#modelpermissions-modelpredicate)] | false |  |
| `not` | [ModelPredicate](#modelpermissions-modelpredicate) | false |  |

 **Examples:**

```yaml
fieldComparison:
  field: author_id
  operator: _eq
  value:
    sessionVariable: x-hasura-user-id
```



```yaml
relationship:
  name: author
  predicate:
    fieldComparison:
      field: id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
```



```yaml
and:
  - fieldComparison:
      field: author_id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
  - fieldComparison:
      field: title
      operator: _eq
      value:
        literal: Hello World
```



```yaml
not:
  fieldComparison:
    field: author_id
    operator: _eq
    value:
      sessionVariable: x-hasura-user-id
```


#### RelationshipPredicate {#modelpermissions-relationshippredicate}

Relationship predicate filters objects of a source model based on a predicate on the related model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [RelationshipName](#modelpermissions-relationshipname) | true | The name of the relationship of the object type of the model to follow. |
| `predicate` | [ModelPredicate](#modelpermissions-modelpredicate) / null | false | The predicate to apply on the related objects. If this is null, then the predicate evaluates to true as long as there is at least one related object present. |



#### RelationshipName {#modelpermissions-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### FieldIsNullPredicate {#modelpermissions-fieldisnullpredicate}

Predicate to check if the given field is null.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#modelpermissions-fieldname) | true | The name of the field that should be checked for a null value. |



#### FieldComparisonPredicate {#modelpermissions-fieldcomparisonpredicate}

Field comparison predicate filters objects based on a field value.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#modelpermissions-fieldname) | true | The field name of the object type of the model to compare. |
| `operator` | [OperatorName](#modelpermissions-operatorname) | true | The name of the operator to use for comparison. |
| `value` | [ValueExpression](#modelpermissions-valueexpression) | true | The value expression to compare against. |



#### ValueExpression {#modelpermissions-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false |  |
| `valueFromEnv` | string | false |  |



#### OperatorName {#modelpermissions-operatorname}

The name of an operator


**Value:** string


#### FieldName {#modelpermissions-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### Role {#modelpermissions-role}

The role for which permissions are being defined.


**Value:** string


#### ModelName {#modelpermissions-modelname}

The name of data model.


**Value:** string
### CommandPermissions {#commandpermissions-commandpermissions}

Definition of permissions for an OpenDD command.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `CommandPermissions` | true |  |
| `version` | `v1` | true |  |
| `definition` | [CommandPermissionsV1](#commandpermissions-commandpermissionsv1) | true | Definition of permissions for an OpenDD command. |

 **Example:**

```yaml
kind: CommandPermissions
version: v1
definition:
  commandName: get_article_by_id
  permissions:
    - role: admin
      allowExecution: true
    - role: user
      allowExecution: true
```


#### CommandPermissionsV1 {#commandpermissions-commandpermissionsv1}

Definition of permissions for an OpenDD command.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `commandName` | [CommandName](#commandpermissions-commandname) | true | The name of the command for which permissions are being defined. |
| `permissions` | [[CommandPermission](#commandpermissions-commandpermission)] | true | A list of command permissions, one for each role. |



#### CommandPermission {#commandpermissions-commandpermission}

Defines the permissions for a role for a command.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#commandpermissions-role) | true | The role for which permissions are being defined. |
| `allowExecution` | boolean | true | Whether the command is executable by the role. |
| `argumentPresets` | [[ArgumentPreset](#commandpermissions-argumentpreset)] | false | Preset values for arguments for this role |

 **Example:**

```yaml
role: user
allowExecution: true
argumentPresets:
  - argument: user_id
    value:
      session_variable: x-hasura-user_id
```


#### ArgumentPreset {#commandpermissions-argumentpreset}

Preset value for an argument

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `argument` | [ArgumentName](#commandpermissions-argumentname) | true | Argument name for preset |
| `value` | [ValueExpressionOrPredicate](#commandpermissions-valueexpressionorpredicate) | true | Value for preset |



#### ValueExpressionOrPredicate {#commandpermissions-valueexpressionorpredicate}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false | Used to represent the name of a session variable, like "x-hasura-role". |
| `booleanExpression` | [ModelPredicate](#commandpermissions-modelpredicate) | false |  |
| `valueFromEnv` | string | false |  |



#### ModelPredicate {#commandpermissions-modelpredicate}

A predicate that can be used to restrict the objects returned when querying a model.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldComparison` | [FieldComparisonPredicate](#commandpermissions-fieldcomparisonpredicate) | false | Field comparison predicate filters objects based on a field value. |
| `fieldIsNull` | [FieldIsNullPredicate](#commandpermissions-fieldisnullpredicate) | false | Predicate to check if the given field is null. |
| `relationship` | [RelationshipPredicate](#commandpermissions-relationshippredicate) | false | Relationship predicate filters objects of a source model based on a predicate on the related model. |
| `and` | [[ModelPredicate](#commandpermissions-modelpredicate)] | false |  |
| `or` | [[ModelPredicate](#commandpermissions-modelpredicate)] | false |  |
| `not` | [ModelPredicate](#commandpermissions-modelpredicate) | false |  |

 **Examples:**

```yaml
fieldComparison:
  field: author_id
  operator: _eq
  value:
    sessionVariable: x-hasura-user-id
```



```yaml
relationship:
  name: author
  predicate:
    fieldComparison:
      field: id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
```



```yaml
and:
  - fieldComparison:
      field: author_id
      operator: _eq
      value:
        sessionVariable: x-hasura-user-id
  - fieldComparison:
      field: title
      operator: _eq
      value:
        literal: Hello World
```



```yaml
not:
  fieldComparison:
    field: author_id
    operator: _eq
    value:
      sessionVariable: x-hasura-user-id
```


#### RelationshipPredicate {#commandpermissions-relationshippredicate}

Relationship predicate filters objects of a source model based on a predicate on the related model.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `name` | [RelationshipName](#commandpermissions-relationshipname) | true | The name of the relationship of the object type of the model to follow. |
| `predicate` | [ModelPredicate](#commandpermissions-modelpredicate) / null | false | The predicate to apply on the related objects. If this is null, then the predicate evaluates to true as long as there is at least one related object present. |



#### RelationshipName {#commandpermissions-relationshipname}

The name of the GraphQL relationship field.


**Value:** string


#### FieldIsNullPredicate {#commandpermissions-fieldisnullpredicate}

Predicate to check if the given field is null.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#commandpermissions-fieldname) | true | The name of the field that should be checked for a null value. |



#### FieldComparisonPredicate {#commandpermissions-fieldcomparisonpredicate}

Field comparison predicate filters objects based on a field value.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `field` | [FieldName](#commandpermissions-fieldname) | true | The field name of the object type of the model to compare. |
| `operator` | [OperatorName](#commandpermissions-operatorname) | true | The name of the operator to use for comparison. |
| `value` | [ValueExpression](#commandpermissions-valueexpression) | true | The value expression to compare against. |



#### ValueExpression {#commandpermissions-valueexpression}

An expression which evaluates to a value that can be used in permissions and various presets.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` |  | false |  |
| `sessionVariable` | string | false |  |
| `valueFromEnv` | string | false |  |



#### OperatorName {#commandpermissions-operatorname}

The name of an operator


**Value:** string


#### FieldName {#commandpermissions-fieldname}

The name of a field in a user-defined object type.


**Value:** string


#### ArgumentName {#commandpermissions-argumentname}

The name of an argument.


**Value:** string


#### Role {#commandpermissions-role}

The role for which permissions are being defined.


**Value:** string


#### CommandName {#commandpermissions-commandname}

The name of a command.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/graphql-config.mdx ---
# GraphQL API Configuration

---
sidebar_position: 13
sidebar_label: GraphQL API Configuration
description:
  "Configure the global field/type names for the GraphQL API generated by Hasura for your data domain models."
toc_max_heading_level: 4
keywords:
  - data domain models
  - hasura api
  - graphql api configuration
  - graphql configuration
seoFrontMatterUpdated: true
---

# Configuration for GraphQL API

## Introduction

The `GraphqlConfig` object allows you to configure the GraphQL schema generated by Hasura. This object helps you
configure the common GraphQL schema applicable to all subgraphs within a project.

**The GraphqlConfig object belongs to the supergraph** and can be defined **once** across your supergraph **in any
subgraph** of your choice.

## How GraphqlConfig works

### Lifecycle

By default, all projects are created with a default `GraphqlConfig` object in the **globals** subgraph.

Specifically, it allows you to configure:

1. The root type names for query, mutation and subscription operations.
2. The name for [model](/reference/metadata-reference/models.mdx) selection input field names (`where`, `limit`,
   `offset`, `args`, `order_by`)
3. The enum values for `order_by` directions and the `order_by` enum type name.

An `GraphqlConfig` object is required to be defined in the supergraph metadata. If not defined, any attempted builds
will not be successful.

### Examples

```yaml
kind: GraphqlConfig
version: v1
definition:
  query:
    rootOperationTypeName: Query
    argumentsInput:
      fieldName: args
    limitInput:
      fieldName: limit
    offsetInput:
      fieldName: offset
    filterInput:
      fieldName: where
      operatorNames:
        and: _and
        or: _or
        not: _not
        isNull: _is_null
    orderByInput:
      fieldName: order_by
      enumDirectionValues:
        asc: Asc
        desc: Desc
      enumTypeNames:
        - directions:
            - Asc
            - Desc
          typeName: OrderBy
  mutation:
    rootOperationTypeName: Mutation
  apolloFederation:
    enableRootFields: false
```

| **Field**                                             | **Description**                                                                                | **Reference**                                                                 |
| ----------------------------------------------------- | ---------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| `kind`                                                | The type of configuration you're defining, in this case, it's for GraphQL API settings.        | [GraphqlConfig](#graphqlconfig-graphqlconfig)                                 |
| `version`                                             | The version of this configuration object.                                                      | [GraphqlConfigV1](#graphqlconfig-graphqlconfigv1)                             |
| `definition.query.rootOperationTypeName`              | The name used for the root query type in your GraphQL schema, usually `Query`.                 | [QueryGraphqlConfig](#graphqlconfig-querygraphqlconfig)                       |
| `definition.query.argumentsInput.fieldName`           | The field name used for passing arguments in queries, usually `args`.                          | [ArgumentsInputGraphqlConfig](#graphqlconfig-argumentsinputgraphqlconfig)     |
| `definition.query.limitInput.fieldName`               | The field name used to limit the number of results, typically `limit`.                         | [LimitInputGraphqlConfig](#graphqlconfig-limitinputgraphqlconfig)             |
| `definition.query.offsetInput.fieldName`              | The field name used to set the starting point for results, typically `offset`.                 | [OffsetInputGraphqlConfig](#graphqlconfig-offsetinputgraphqlconfig)           |
| `definition.query.filterInput.fieldName`              | The field name used for filtering results, usually `where`.                                    | [FilterInputGraphqlConfig](#graphqlconfig-filterinputgraphqlconfig)           |
| `definition.query.filterInput.operatorNames[]`        | The names of the built-in filter operators.                                                    | [FilterInputOperatorNames](#graphqlconfig-filterinputoperatornames)           |
| `definition.query.orderByInput.fieldName`             | The field name used for sorting results, usually `order_by`.                                   | [OrderByInputGraphqlConfig](#graphqlconfig-orderbyinputgraphqlconfig)         |
| `definition.query.orderByInput.enumDirectionValues[]` | The names of the direction parameters, like `asc` and `desc`.                                  | [OrderByDirectionValues](#graphqlconfig-orderbydirectionvalues)               |
| `definition.query.orderByInput.enumTypeNames`         | The name of the enum type used for sorting, like `OrderBy`.                                    | [OrderByEnumTypeName](#graphqlconfig-orderbyenumtypename)                     |
| `definition.mutation.rootOperationTypeName`           | The name used for the root mutation type in your GraphQL schema, usually `Mutation`.           | [MutationGraphqlConfig](#graphqlconfig-mutationgraphqlconfig)                 |
| `definition.apolloFederation.enableRootFields`        | A setting to enable or disable fields needed for Apollo Federation (`_entities`, `_services`). | [GraphqlApolloFederationConfig](#graphqlconfig-graphqlapollofederationconfig) |

---

## Metadata structure


### GraphqlConfig {#graphqlconfig-graphqlconfig}

GraphqlConfig object tells us two things:

1. How the Graphql schema should look like for the features (`where`, `order_by` etc) Hasura provides 2. What features should be enabled/disabled across the subgraphs

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `GraphqlConfig` | true |  |
| `version` | `v1` | true |  |
| `definition` | [GraphqlConfigV1](#graphqlconfig-graphqlconfigv1) | true | GraphqlConfig object tells us two things:  1. How the Graphql schema should look like for the features (`where`, `order_by` etc) Hasura provides 2. What features should be enabled/disabled across the subgraphs |



#### GraphqlConfigV1 {#graphqlconfig-graphqlconfigv1}

GraphqlConfig object tells us two things:

1. How the Graphql schema should look like for the features (`where`, `order_by` etc) Hasura provides 2. What features should be enabled/disabled across the subgraphs

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `query` | [QueryGraphqlConfig](#graphqlconfig-querygraphqlconfig) | true | Configuration for the GraphQL schema of Hasura features for queries. `None` means disable the feature. |
| `mutation` | [MutationGraphqlConfig](#graphqlconfig-mutationgraphqlconfig) | true | Configuration for the GraphQL schema of Hasura features for mutations. |
| `subscription` | [SubscriptionGraphqlConfig](#graphqlconfig-subscriptiongraphqlconfig) / null | false |  |
| `apolloFederation` | [GraphqlApolloFederationConfig](#graphqlconfig-graphqlapollofederationconfig) / null | false |  |



#### GraphqlApolloFederationConfig {#graphqlconfig-graphqlapollofederationconfig}

Configuration for the GraphQL schema of Hasura features for Apollo Federation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `enableRootFields` | boolean | true | Adds the `_entities` and `_services` root fields required for Apollo Federation. |



#### SubscriptionGraphqlConfig {#graphqlconfig-subscriptiongraphqlconfig}

Configuration for the GraphQL schema of Hasura features for subscriptions.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootOperationTypeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true | The name of the root operation type name for subscriptions. Usually `subscription`. |



#### MutationGraphqlConfig {#graphqlconfig-mutationgraphqlconfig}

Configuration for the GraphQL schema of Hasura features for mutations.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootOperationTypeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true | The name of the root operation type name for mutations. Usually `mutation`. |



#### QueryGraphqlConfig {#graphqlconfig-querygraphqlconfig}

Configuration for the GraphQL schema of Hasura features for queries. `None` means disable the feature.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `rootOperationTypeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true | The name of the root operation type name for queries. Usually `query`. |
| `argumentsInput` | [ArgumentsInputGraphqlConfig](#graphqlconfig-argumentsinputgraphqlconfig) / null | false | Configuration for the arguments input. |
| `limitInput` | [LimitInputGraphqlConfig](#graphqlconfig-limitinputgraphqlconfig) / null | false | Configuration for the limit operation. |
| `offsetInput` | [OffsetInputGraphqlConfig](#graphqlconfig-offsetinputgraphqlconfig) / null | false | Configuration for the offset operation. |
| `filterInput` | [FilterInputGraphqlConfig](#graphqlconfig-filterinputgraphqlconfig) / null | false | Configuration for the filter operation. |
| `orderByInput` | [OrderByInputGraphqlConfig](#graphqlconfig-orderbyinputgraphqlconfig) / null | false | Configuration for the sort operation. |
| `aggregate` | [AggregateGraphqlConfig](#graphqlconfig-aggregategraphqlconfig) / null | false | Configuration for aggregates |



#### AggregateGraphqlConfig {#graphqlconfig-aggregategraphqlconfig}

Configuration for the GraphQL schema for aggregates.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `filterInputFieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the filter input parameter of aggregate fields and field name in predicates |
| `countFieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the _count field used for the count aggregate function |
| `countDistinctFieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the _count_distinct field used for the count distinct aggregate function |



#### OrderByInputGraphqlConfig {#graphqlconfig-orderbyinputgraphqlconfig}

Configuration for the sort operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the filter operation field. Usually `order_by`. |
| `enumDirectionValues` | [OrderByDirectionValues](#graphqlconfig-orderbydirectionvalues) | true | The names of the direction parameters. |
| `enumTypeNames` | [[OrderByEnumTypeName](#graphqlconfig-orderbyenumtypename)] | true |  |



#### OrderByEnumTypeName {#graphqlconfig-orderbyenumtypename}

Type name for a sort directions enum, with the given set of possible directions.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `directions` | [[OrderByDirection](#graphqlconfig-orderbydirection)] | true |  |
| `typeName` | [GraphQlTypeName](#graphqlconfig-graphqltypename) | true |  |



#### OrderByDirection {#graphqlconfig-orderbydirection}

Sort direction.


**One of the following values:**

| Value | Description |
|-----|-----|
| `Asc` | Ascending. |
| `Desc` | Descending. |



#### OrderByDirectionValues {#graphqlconfig-orderbydirectionvalues}

The names of the direction parameters.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `asc` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the ascending parameter. Usually `Asc`. |
| `desc` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the descending parameter. Usually `Desc`. |



#### FilterInputGraphqlConfig {#graphqlconfig-filterinputgraphqlconfig}

Configuration for the filter operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the filter operation field. Usually `where`. |
| `operatorNames` | [FilterInputOperatorNames](#graphqlconfig-filterinputoperatornames) | true | The names of built-in filter operators. |



#### FilterInputOperatorNames {#graphqlconfig-filterinputoperatornames}

The names of built-in filter operators.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `and` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `and` operator. Usually `_and`. |
| `or` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `or` operator. Usually `_or`. |
| `not` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `not` operator. Usually `_not`. |
| `isNull` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the `is null` operator. Usually `_is_null`. |



#### OffsetInputGraphqlConfig {#graphqlconfig-offsetinputgraphqlconfig}

Configuration for the offset operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the offset operation field. Usually `offset`. |



#### LimitInputGraphqlConfig {#graphqlconfig-limitinputgraphqlconfig}

Configuration for the limit operation.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of the limit operation field. Usually `limit`. |



#### ArgumentsInputGraphqlConfig {#graphqlconfig-argumentsinputgraphqlconfig}

Configuration for the arguments input.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fieldName` | [GraphQlFieldName](#graphqlconfig-graphqlfieldname) | true | The name of arguments passing field. Usually `args`. |



#### GraphQlFieldName {#graphqlconfig-graphqlfieldname}

The name of a GraphQL object field.


**Value:** string


#### GraphQlTypeName {#graphqlconfig-graphqltypename}

The name of a GraphQL type.


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/auth-config.mdx ---
# AuthConfig

---
sidebar_label: AuthConfig
description: "Configure the authentication configuration used by the API server in Hasura."
seoFrontMatterUpdated: true
toc_max_heading_level: 4
sidebar_position: 14
---

# AuthConfig

## Introduction

The `AuthConfig` object is used to define the authentication configuration used by the API. With Hasura, you can utilize
either webhooks or JWTs for authentication. **The AuthConfig object belongs to the supergraph** and can be defined
**once** across your supergraph in **any subgraph** of your choice using one of the following modes:

| Mode                | Description                                                                                                                            |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| [noAuth](#noAuth)   | This is enabled by default and allows you to connect to your API without any authentication. **It is not recommended for production.** |
| [JWT](#jwt)         | This allows you to use JWTs for authentication.                                                                                        |
| [Webhook](#webhook) | This allows you to use a custom webhook for authentication.                                                                            |

You can learn more about Hasura's approach to using existing authentication systems in the
[auth section](/auth/overview.mdx).

## How AuthConfig works

### Lifecycle

By default, all projects are created with an `AuthConfig` object in the **globals** subgraph in a `noAuth` mode that
doesn't use any auth service and doesn't allow setting session variables with API requests. Meaning that your API is, by
default, fully public and exposed if deployed live.

However, you would want update the `AuthConfig` to use a custom webhook or JWT service for authentication to restrict
access to your API and make use of Hasura's powerful [authorization](/auth/overview.mdx) features. The metadata examples
below can help you configure your `AuthConfig` object to use your own custom webhook or JWT service.

An AuthConfig object is required to be defined in the supergraph metadata. If not defined, any attempted builds will not
be successful.

To make an update AuthConfig available in your supergraph, after updating your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

Each example below offers numerous customizations. However, we've provided a basic example for each mode to get you
started. Check the reference table for more information on each field.

#### noAuth {#noAuth}

```yaml title="An AuthConfig for noAuth mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    noAuth:
      role: admin
      sessionVariables: {}
```

| **Field**          | **Description**                                                                                                                                                                                                                  | **Reference**                                    |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| `role`             | The role to be assumed while running the engine in `noAuth` mode.                                                                                                                                                                | [Role](#authconfig-role)                         |
| `sessionVariables` | Static session variables that will be used while running the engine without authentication. This is helpful when you want to test requests using particular session variables, such as `x-hasura-user-id` with a non-admin role. | [SessionVariables](#authconfig-sessionvariables) |

#### JWT

```yaml title="An AuthConfig for JWT mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    jwt:
      claimsConfig:
        namespace:
          claimsFormat: Json
          location: /claims.jwt.hasura.io
      tokenLocation:
        type: BearerAuthorization
      key:
        fixed:
          algorithm: HS256
          key:
            valueFromEnv: AUTH_SECRET
```

| **Field**       | **Description**                                                                                                                                                                                                                                                                     | **Reference**                                    |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| `claimsConfig`  | Configuration to describe how and where the engine should look for the claims within the decoded token. You can vary the format and location of the claims.                                                                                                                         | [JWTClaimsConfig](#authconfig-jwtclaimsconfig)   |
| `tokenLocation` | Specifies the source of the JWT authentication token and how it should be parsed.                                                                                                                                                                                                   | [JWTTokenLocation](#authconfig-jwttokenlocation) |
| `key`           | Configuration for the JWT key, specifying how the incoming JWT will be verified and decoded. In this example, we've used a fixed key that's stored as an environment variable in our root-level `.env` file that is also mapped to the `subgraph.yaml` in our `/globals` directory. | [JWTKey](#authconfig-jwtkey)                     |

#### Webhook

```yaml title="An AuthConfig for Webhook mode:"
kind: AuthConfig
version: v3
definition:
  mode:
    webhook:
      url:
        value: http://auth_hook:3050/validate-request
      method: POST
      customHeadersConfig:
        body:
          headers:
            forward:
              - authorization
              - content-type
        headers:
          additional:
            user-agent: "Hasura DDN"

```

| **Field** | **Description**                                                                                                                                                                       | **Reference**                                                          |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `url`     | The URL of the authentication webhook that will be called to validate authentication. This can be a static URL or an environment variable.                                            | [EnvironmentValue](#authconfig-environmentvalue)                       |
| `method`  | The HTTP method (`GET` or `POST`) that will be used to make the request to the auth hook.                                                                                             | [AuthHookConfigV3](#authconfig-authhookconfigv3)                       |
| `customHeadersConfig` | Configuration for the headers and body to be sent to the auth hook. This can be used to forward headers from the client request or add additional headers to the request. | [AuthHookConfigV3POSTHeaders](#authconfig-authhookconfigv3postheaders) |

---

## Metadata structure

#### AuthConfig {#authconfig-authconfig}

Definition of the authentication configuration used by the API server.

**One of the following values:**

| Value                                    | Description                                                                |
| ---------------------------------------- | -------------------------------------------------------------------------- |
| [AuthConfigV1](#authconfig-authconfigv1) | Definition of the authentication configuration v1, used by the API server. |
| [AuthConfigV2](#authconfig-authconfigv2) | Definition of the authentication configuration v2, used by the API server. |
| [AuthConfigV3](#authconfig-authconfigv3) | Definition of the authentication configuration v3, used by the API server.                                                                           |

#### AuthConfigV3 {#authconfig-authconfigv3}

| Key          | Value                                    | Required | Description                                                                |
| ------------ | ---------------------------------------- | -------- | -------------------------------------------------------------------------- |
| `kind`       | `AuthConfig`                             | true     |                                                                            |
| `version`    | `v3`                                     | true     |                                                                            |
| `definition` | [AuthConfigV3](#authconfig-authconfigv3) | true     | Definition of the authentication configuration v3, used by the API server. |

#### AuthConfigV2 {#authconfig-authconfigv2}

Definition of the authentication configuration v2, used by the API server.

| Key          | Value                                    | Required | Description                                                                |
| ------------ | ---------------------------------------- | -------- | -------------------------------------------------------------------------- |
| `kind`       | `AuthConfig`                             | true     |                                                                            |
| `version`    | `v2`                                     | true     |                                                                            |
| `definition` | [AuthConfigV2](#authconfig-authconfigv2) | true     | Definition of the authentication configuration v2, used by the API server. |

#### AuthConfigV1 {#authconfig-authconfigv1}

Definition of the authentication configuration v1, used by the API server.

| Key          | Value                                    | Required | Description                                                                |
| ------------ | ---------------------------------------- | -------- | -------------------------------------------------------------------------- |
| `kind`       | `AuthConfig`                             | true     |                                                                            |
| `version`    | `v1`                                     | true     |                                                                            |
| `definition` | [AuthConfigV1](#authconfig-authconfigv1) | true     | Definition of the authentication configuration v1, used by the API server. |




#### AuthConfigV3 {#authconfig-authconfigv3}

Definition of the authentication configuration v3, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `mode` | [AuthModeConfigV3](#authconfig-authmodeconfigv3) | true | The configuration for the authentication mode to use - webhook, JWT or NoAuth. |



#### AuthModeConfigV3 {#authconfig-authmodeconfigv3}

The configuration for the authentication mode to use - webhook, JWT or NoAuth.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `webhook` | [AuthHookConfigV3](#authconfig-authhookconfigv3) | false | The configuration of the authentication webhook. |
| `jwt` | [JWTConfig](#authconfig-jwtconfig) | false |  |
| `noAuth` | [NoAuthConfig](#authconfig-noauthconfig) | false |  |



#### AuthHookConfigV3 {#authconfig-authhookconfigv3}

The configuration of the authentication webhook.


**One of the following values:**

| Value | Description |
|-----|-----|
| [AuthHookConfigV3GET](#authconfig-authhookconfigv3get) | The configuration of the GET authentication webhook. |
| [AuthHookConfigV3POST](#authconfig-authhookconfigv3post) | The configuration of the POST authentication webhook. |

 **Example:**

```yaml
method: GET
url:
  value: http://auth_hook:3050/validate-request
customHeadersConfig:
  headers:
    forward:
      - Authorization
    additional:
      user-agent: hasura-ddn
```


#### AuthHookConfigV3POST {#authconfig-authhookconfigv3post}

The configuration of the POST authentication webhook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `method` | `POST` | true |  |
| `url` | [EnvironmentValue](#authconfig-environmentvalue) | true | The URL of the POST authentication webhook. |
| `customHeadersConfig` | [AuthHookConfigV3POSTHeaders](#authconfig-authhookconfigv3postheaders) / null | false | The configuration for the headers to be sent to the POST auth hook. |

 **Example:**

```yaml
url:
  value: http://auth_hook:3050/validate-request
customHeadersConfig:
  headers:
    forward: '*'
    additional:
      user-agent: hasura-ddn
  body:
    headers:
      forward:
        - Authorization
      additional: {}
```


#### AuthHookConfigV3POSTHeaders {#authconfig-authhookconfigv3postheaders}

The configuration for the headers and body to be sent to the POST auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [AuthHookConfigV3Headers](#authconfig-authhookconfigv3headers) / null | false | The configuration for the headers to be sent to the POST auth hook. |
| `body` | [AuthHookConfigV3Body](#authconfig-authhookconfigv3body) / null | false | The configuration for the body to be sent to the POST auth hook. |

 **Example:**

```yaml
headers:
  forward: '*'
  additional:
    user-agent: hasura-ddn
body:
  headers:
    forward:
      - Authorization
    additional: {}
```


#### AuthHookConfigV3Body {#authconfig-authhookconfigv3body}

The configuration for the body to be sent to the POST auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [AuthHookConfigV3Headers](#authconfig-authhookconfigv3headers) / null | false | The configuration for the headers to be sent as part of the body to the POST auth hook. |

 **Example:**

```yaml
headers:
  forward:
    - Authorization
  additional: {}
```


#### AuthHookConfigV3GET {#authconfig-authhookconfigv3get}

The configuration of the GET authentication webhook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `method` | `GET` | true |  |
| `url` | [EnvironmentValue](#authconfig-environmentvalue) | true | The URL of the GET authentication webhook. |
| `customHeadersConfig` | [AuthHookConfigV3GETHeaders](#authconfig-authhookconfigv3getheaders) / null | false | The configuration for the headers to be sent to the GET auth hook. |

 **Example:**

```yaml
url:
  value: http://auth_hook:3050/validate-request
customHeadersConfig:
  headers:
    forward:
      - Authorization
    additional:
      user-agent: hasura-ddn
```


#### AuthHookConfigV3GETHeaders {#authconfig-authhookconfigv3getheaders}

The configuration for the headers to be sent to the GET auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [AuthHookConfigV3Headers](#authconfig-authhookconfigv3headers) / null | false |  |

 **Example:**

```yaml
headers:
  forward:
    - Authorization
  additional:
    user-agent: hasura-ddn
```


#### AuthHookConfigV3Headers {#authconfig-authhookconfigv3headers}

The configuration for the headers to be sent to the auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `forward` | [AllOrList](#authconfig-allorlist) | false | The headers to be forwarded from the client request. |
| `additional` | [AdditionalHeaders](#authconfig-additionalheaders) | false | The additional headers to be sent to the auth hook. |

 **Example:**

```yaml
forward:
  - Authorization
additional:
  user-agent: hasura-ddn
```


#### AdditionalHeaders {#authconfig-additionalheaders}

The additional headers to be sent to the auth hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | string | false |  |



#### AllOrList {#authconfig-allorlist}

A list of items or a wildcard.


**One of the following values:**

| Value | Description |
|-----|-----|
| [All](#authconfig-all) | Wildcard: match all items |
| [string] |  |

 **Example:**

```yaml
'*'
```


#### All {#authconfig-all}

Wildcard: match all items


**Value:** `*`


#### AuthConfig2 {#authconfig-authconfig2}

Definition of the authentication configuration v2, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AuthConfig` | true |  |
| `version` | `v2` | true |  |
| `definition` | [AuthConfigV2](#authconfig-authconfigv2) | true | Definition of the authentication configuration v2, used by the API server. |



#### AuthConfigV2 {#authconfig-authconfigv2}

Definition of the authentication configuration v2, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `mode` | [AuthModeConfig](#authconfig-authmodeconfig) | true |  |



#### AuthConfig1 {#authconfig-authconfig1}

Definition of the authentication configuration v1, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `AuthConfig` | true |  |
| `version` | `v1` | true |  |
| `definition` | [AuthConfigV1](#authconfig-authconfigv1) | true | Definition of the authentication configuration v1, used by the API server. |



#### AuthConfigV1 {#authconfig-authconfigv1}

Definition of the authentication configuration v1, used by the API server.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `allowRoleEmulationBy` | [Role](#authconfig-role) / null | false |  |
| `mode` | [AuthModeConfig](#authconfig-authmodeconfig) | true | The configuration for the authentication mode to use - webhook, JWT or NoAuth. |



#### AuthModeConfig {#authconfig-authmodeconfig}

The configuration for the authentication mode to use - webhook, JWT or NoAuth.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `webhook` | [AuthHookConfig](#authconfig-authhookconfig) | false | The configuration of the authentication webhook. |
| `jwt` | [JWTConfig](#authconfig-jwtconfig) | false | JWT config according to which the incoming JWT will be verified and decoded to extract the session variable claims. |
| `noAuth` | [NoAuthConfig](#authconfig-noauthconfig) | false | Configuration used when running engine without authentication |



#### NoAuthConfig {#authconfig-noauthconfig}

Configuration used when running engine without authentication

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `role` | [Role](#authconfig-role) | true | role to assume whilst running the engine |
| `sessionVariables` | [SessionVariables](#authconfig-sessionvariables) | true | static session variables to use whilst running the engine |

 **Example:**

```yaml
role: admin
sessionVariables:
  x-hasura-user-id: '100'
```


#### SessionVariables {#authconfig-sessionvariables}

static session variables to use whilst running the engine

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` |  | false | JSON value of a session variable |



#### JWTConfig {#authconfig-jwtconfig}

JWT config according to which the incoming JWT will be verified and decoded to extract the session variable claims.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `audience` | [string] / null | false | Optional validation to check that the `aud` field is a member of the `audience` received, otherwise will throw error. |
| `issuer` | string / null | false | Optional validation to check that the `iss` field is a member of the `iss` received, otherwise will throw error. |
| `allowedSkew` | integer / null | false | Allowed leeway (in seconds) to the `exp` validation to account for clock skew. |
| `claimsConfig` | [JWTClaimsConfig](#authconfig-jwtclaimsconfig) | true | Claims config. Either specified via `claims_mappings` or `claims_namespace_path` |
| `tokenLocation` | [JWTTokenLocation](#authconfig-jwttokenlocation) | true | Source of the JWT authentication token. |
| `key` | [JWTKey](#authconfig-jwtkey) | true | Mode according to which the JWT auth is configured. |

 **Example:**

```yaml
audience: null
issuer: null
allowedSkew: null
claimsConfig:
  namespace:
    claimsFormat: Json
    location: /claims.jwt.hasura.io
tokenLocation:
  type: BearerAuthorization
key:
  fixed:
    algorithm: HS256
    key:
      value: token
```


#### JWTKey {#authconfig-jwtkey}

JWT key configuration according to which the incoming JWT will be decoded.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `fixed` | [JWTKeyConfig](#authconfig-jwtkeyconfig) | false | JWT Secret config according to which the incoming JWT will be decoded. |
| `jwkFromUrl` | string | false |  |



#### JWTKeyConfig {#authconfig-jwtkeyconfig}

JWT Secret config according to which the incoming JWT will be decoded.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `algorithm` | [JWTAlgorithm](#authconfig-jwtalgorithm) | true | The algorithm used to decode the JWT. |
| `key` | [EnvironmentValue](#authconfig-environmentvalue) | true | The key to use for decoding the JWT. |



#### EnvironmentValue {#authconfig-environmentvalue}

Either a literal string or a reference to a Hasura secret


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | string | false |  |
| `valueFromEnv` | string | false |  |



#### JWTAlgorithm {#authconfig-jwtalgorithm}

The algorithm used to decode the JWT.


**One of the following values:**

| Value | Description |
|-----|-----|
| `HS256` | HMAC using SHA-256 |
| `HS384` | HMAC using SHA-384 |
| `HS512` | HMAC using SHA-512 |
| `ES256` | ECDSA using SHA-256 |
| `ES384` | ECDSA using SHA-384 |
| `RS256` | RSASSA-PKCS1-v1_5 using SHA-256 |
| `RS384` | RSASSA-PKCS1-v1_5 using SHA-384 |
| `RS512` | RSASSA-PKCS1-v1_5 using SHA-512 |
| `PS256` | RSASSA-PSS using SHA-256 |
| `PS384` | RSASSA-PSS using SHA-384 |
| `PS512` | RSASSA-PSS using SHA-512 |
| `EdDSA` | Edwards-curve Digital Signature Algorithm (EdDSA) |



#### JWTTokenLocation {#authconfig-jwttokenlocation}

Source of the Authorization token


**One of the following values:**

| Value | Description |
|-----|-----|
| [JWTBearerAuthorizationLocation](#authconfig-jwtbearerauthorizationlocation) | Get the bearer token from the `Authorization` header. |
| [JWTCookieLocation](#authconfig-jwtcookielocation) | Get the token from the Cookie header under the specified cookie name. |
| [JWTHeaderLocation](#authconfig-jwtheaderlocation) | Custom header from where the header should be parsed from. |



#### JWTHeaderLocation {#authconfig-jwtheaderlocation}

Custom header from where the header should be parsed from.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | `Header` | true |  |
| `name` | string | true |  |



#### JWTCookieLocation {#authconfig-jwtcookielocation}

Get the token from the Cookie header under the specified cookie name.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | `Cookie` | true |  |
| `name` | string | true |  |



#### JWTBearerAuthorizationLocation {#authconfig-jwtbearerauthorizationlocation}

Get the bearer token from the `Authorization` header.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `type` | `BearerAuthorization` | true |  |



#### JWTClaimsConfig {#authconfig-jwtclaimsconfig}

Config to describe how/where the engine should look for the claims within the decoded token.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `locations` | [JWTClaimsMap](#authconfig-jwtclaimsmap) | false | Can be used when Hasura claims are not all present in the single object, but individual claims are provided a JSON pointer within the decoded JWT and optionally a default value. |
| `namespace` | [JWTClaimsNamespace](#authconfig-jwtclaimsnamespace) | false | Used when all of the Hasura claims are present in a single object within the decoded JWT. |



#### JWTClaimsNamespace {#authconfig-jwtclaimsnamespace}

Used when all of the Hasura claims are present in a single object within the decoded JWT.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `claimsFormat` | [JWTClaimsFormat](#authconfig-jwtclaimsformat) | true | Format in which the Hasura claims will be present. |
| `location` | string | true | Pointer to lookup the Hasura claims within the decoded claims. |



#### JWTClaimsFormat {#authconfig-jwtclaimsformat}

Format in which the Hasura claims will be present.


**One of the following values:**

| Value | Description |
|-----|-----|
| `Json` | Claims will be in the JSON format. |
| `StringifiedJson` | Claims will be in the Stringified JSON format. |



#### JWTClaimsMap {#authconfig-jwtclaimsmap}

Can be used when Hasura claims are not all present in the single object, but individual claims are provided a JSON pointer within the decoded JWT and optionally a default value.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `x-hasura-default-role` | [JWTClaimsMappingEntry](#authconfig-jwtclaimsmappingentry) | true | JSON pointer to lookup the default role within the decoded JWT. |
| `x-hasura-allowed-roles` | [JWTClaimsMappingEntry](#authconfig-jwtclaimsmappingentry) | true | JSON pointer to lookup the allowed roles within the decoded JWT. |
| `<customKey>` | [JWTClaimsMappingEntry](#authconfig-jwtclaimsmappingentry) | false |  |



#### JWTClaimsMappingEntry {#authconfig-jwtclaimsmappingentry}

JSON pointer to lookup the default role within the decoded JWT.


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `literal` | [Role](#authconfig-role) | false |  |
| `path` | [JWTClaimsMappingPathEntry](#authconfig-jwtclaimsmappingpathentry) | false | Entry to lookup the Hasura claims at the specified JSON Pointer |



#### JWTClaimsMappingPathEntry {#authconfig-jwtclaimsmappingpathentry}

Entry to lookup the Hasura claims at the specified JSON Pointer

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `path` | string | true | JSON pointer to find the particular claim in the decoded JWT token. |
| `default` | [Role](#authconfig-role) / null | false | Default value to be used when no value is found when looking up the value using the `path`. |



#### AuthHookConfig {#authconfig-authhookconfig}

The configuration of the authentication webhook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `url` | string | true | The URL of the authentication webhook. |
| `method` | [AuthHookMethod](#authconfig-authhookmethod) | true | The HTTP method to be used to make the request to the auth hook. |

 **Example:**

```yaml
url: http://auth_hook:3050/validate-request
method: Post
```


#### AuthHookMethod {#authconfig-authhookmethod}

The HTTP method to be used to make the request to the auth hook.


**Value:** `Get` / `Post`


#### Role {#authconfig-role}


**Value:** string


--- File: ../ddn-docs/docs/reference/metadata-reference/compatibility-config.mdx ---
# CompatibilityConfig

---
sidebar_position: 15
sidebar_label: CompatibilityConfig
description:
  "CompatibilityConfig is a metadata object that defines the compatibility configuration of the Hasura metadata."
keywords:
  - compatibility config
toc_max_heading_level: 4
---

# Compatibility Config

## Introduction

The CompatibilityConfig object is a metadata object that defines the compatibility configuration of the Hasura metadata.

## How CompatibilityConfig works

### Lifecycle

By default, all projects are created with a default CompatibilityConfig object in the **globals** subgraph. It can be
defined in any subgraph of your choice, but only once across your supergraph.

The `date` field in the CompatibilityConfig object specifies the date after which any new backwards-incompatible changes
made to Hasura DDN will be disabled and will not impact the Hasura project. For example, if your project has its `date`
set to `2024-10-16`, then any new features added after that date that would result in breaking changes to your project
will be disabled. To enable these features, simply increase your `date` to a newer date.

### Compatibility dates and breaking changes

The following is a list of dates at which backwards-incompatible changes were added to Hasura DDN. Projects with
CompatibilityConfig `date`s prior to these dates will have these features disabled.

#### 2025-02-08

##### Disallow unsupported orderable relationships

Build errors are now raised if unsupported orderable relationships are defined on OrderByExpressions. Specifically:

- If the relationship is a remote relationship
- If the data connector does not support performing relationships
- If the target Model or Command of the relationship does not have a source defined

##### Disallow relationships where the data connector does not support relationships or remote relationships

A build error is now raised if a Relationship is defined between an ObjectType and a Model or Command that are sourced
from the same data connector, but that data connector does not support performing relationships or support remote
relationships.

#### 2025-02-04

##### Command and argument preset typechecking

A build error is now raised if an argument preset defined in CommandPermissions or ModelPermissions or a field preset in
TypePermissions specifies a literal value that does not match the type of the argument or field it is supposed to be
presetting.

##### Disallow scalar type mismatch in object type fields

A build error is now raised if an ObjectType's field's scalar type does not match the scalar type of the underlying data
connector's object type field. The field's specified scalar type must have a DataConnectorScalarRepresentation defined
that specifies it as the representation for the data connector's scalar type.

##### Disallow unknown ObjectType field types

A build error is now raised if an ObjectType's field's type is an unknown type.

##### Disallow orderable fields that have field arguments

A build error is now raised if an orderable field on an OrderByExpression has field arguments. This scenario is not
supported.

#### 2025-01-25

##### Disallow duplicate scalar type aggregate functions

A build error is now raised if a data connector scalar type has multiple aggregate functions of the same type (for
example, two sum functions for the same scalar type). This ensures that there is no ambiguity when choosing an aggregate
function for a particular purpose (for example, performing a sum aggregate).

#### 2025-01-07

##### Disallow duplicate operator definitions for scalar type

A build error is now raised when a scalar type has multiple operator definitions with the same name. For example, if you
have a custom scalar type and define multiple operators with the same name in its boolean expression configuration, the
build will fail. This ensures that operator definitions for scalar types are unique and prevents ambiguity in boolean
expressions.

##### Disallow multidimensional arrays in boolean expressions

A build error is now raised when multidimensional arrays (arrays of arrays) are used in boolean expressions. This
restriction applies to both array comparison operators and array relationship fields within boolean expressions.
Previously, such configurations might have been allowed but could lead to runtime errors or undefined behavior. This
change ensures that only single-dimensional arrays can be used in boolean expressions, making the behavior more
predictable and preventing potential runtime issues.

##### Disallow duplicate names across types and expressions

A build error is now raised when:

- Duplicate names exist within a subgraph for the following definitions:

  - [`BooleanExpressionType`](/reference/metadata-reference/boolean-expressions.mdx#booleanexpressiontype-booleanexpressiontype)
  - [`OrderByExpression`](/reference/metadata-reference/orderby-expressions.mdx)

- Type name conflicts exist within a subgraph across the following type definitions:
  - [`ScalarType`](/reference/metadata-reference/types.mdx#scalartype)
  - [`ObjectType`](/reference/metadata-reference/types.mdx#objecttype)
  - [`BooleanExpressionType`](/reference/metadata-reference/boolean-expressions.mdx#booleanexpressiontype-booleanexpressiontype)

#### 2024-12-18

##### Disallow non-scalar fields in Model v1 `orderableFields`

`orderableFields` in `Model` v1 now only allows fields of scalar types. Previously this was allowed, but queries that
tried to order against the non-scalar fields would fail at runtime or produce unexpected results. To order over nested
object fields, upgrade to `Model` v2 and use `OrderByExpression`s instead.

##### Disallow nested fields and nested relationships in OrderByExpressions when the data connector does not support them

Previously `OrderByExpression`s that incorporated nested fields and nested relationships were allowed to be used with
`Model`s that sourced from data connectors that did not support ordering by nested fields and nested relationships. Such
a configuration will now result in a build error.

##### Prevent usage of array relationships in OrderByExpressions

Previously `OrderByExpression`s could specify array relationships in `orderableRelationships`. This was incorrect
behaviour, as only object relationships can be ordered over, and now results in a build error.

#### 2024-12-10

##### Disallow multiple fields on input objects in GraphQL in `order_by`

GraphQL queries with `order_by` arguments that contain multiple properties set on one input object now properly return
an error. For example `order_by: { location: { city: Asc, country: Asc } }` is no longer allowed. This is because the
order of input object fields in GraphQL is not defined, so it is unclear whether ordering should be first by city or
first by country. Instead, write this query like so:
`order_by: [{ location: { city: Asc } }, { location: { country: Asc } }]`.

Additionally, ordering by nested fields using a nested array is no longer allowed (for example:
`order_by: { location: [{ city: Asc }, { country: Asc }] }`). Instead, write this query like so:
`order_by: [{ location: { city: Asc } }, { location: { country: Asc } }]`.

#### 2024-12-05

##### Prevent conflicts between boolean expression fields in GraphQL

Conflicts between fields on boolean expression types that have the same name (such as conflicts between fields and
logical operators) are now detected and a build error is raised.

#### 2024-11-26

##### Logical operators in scalar boolean expression types

Previously, if logical operators (ie. `_and`, `_or`, `_not`) were enabled in scalar `BooleanExpressionType`s, they would
_not_ appear in the GraphQL API. Now, if they are enabled, they will correctly appear in the GraphQL API as configured.

#### 2024-11-18

##### Disallow object boolean expression type

A build error is now raised if `ObjectBooleanExpressionType` is used to define a boolean expression. To resolve this,
you can upgrade all object boolean expression types in your metadata by running the following DDN codemod command.

```bash
ddn codemod upgrade-object-boolean-expression-types
```

Learn more about the [command here](/reference/cli/commands/ddn_codemod_upgrade-object-boolean-expression-types.mdx).

#### 2024-11-15

##### Require unique model GraphQL names

A build error is now raised if a root field defined in a `Model`'s GraphQL config conflicts with an existing root field.
Previously, the conflicting root field was silently excluded from the GraphQL schema.

##### Require a valid NDC version in data connector capabilities

A build error is now raised when there is an invalid version in the capabilities of a data connector's schema. To fix
the error, please consider upgrading to the latest version of the data connector and running `ddn connector introspect`
to correct the invalid version. Prior to this change, it was assumed that the data connector with the invalid version
was an NDC v0.1.x connector.

#### 2024-11-13

##### Allow resolving boolean expression fields without GraphQL config

Previously, comparable fields, relationships, and aggregate fields of a `BooleanExpressionType` were validated only if
the `graphql` field was configured in the metadata. These fields are now validated regardless of whether the `graphql`
configuration is specified.

#### 2024-10-31

##### Disallow comparisons against scalar array fields in predicates

A build error is now raised if a scalar array field is configured in a BooleanExpressionType as a comparable field.
Comparisons against scalar array fields are not supported as at this compatibility date.

#### 2024-10-16

##### Enable JSON session variable support

Session variables provided in JWT claims, webhook responses or in the NoAuth settings can now be any JSON value, instead
of just a string. This enable scenarios such as being able to put lists of values into a single session variable.
However, this now means that if you use the session variable in a comparison against a field, the field's type must
match the type of the session variable. For example, if you are comparing against an integer field, your session
variable must be set as a JSON number and not a string.

#### 2024-10-07

##### Require unique command GraphQL names

A build error is now raised when there is a conflict with a GraphQL root field or GraphQL type name that is already in
use. Before this, the command would be silently dropped from the GraphQL schema.

#### 2024-09-26

##### Propagate boolean expression deprecation status

The `deprecated` status for relationship fields is now propagated to boolean expressions that use them. The default
GraphQL introspection behavior is to hide such fields from the schema, so this behavior is considered a breaking change.

##### Disallow scalar types names conflicting with inbuilt types

A build error is now raised when a `ScalarType` metadata type is defined that conflicts with any of the built-in type
names (`ID`, `Int`, `String`, `Boolean`, `Float`).

#### 2024-09-18

##### Require nested array filtering capability

A build error is now raised when a nested array field is defined on an ObjectType used in a Model whose underlying data
connector does not have the `query.exists.nested_collections` capability defined.

#### 2024-09-03

##### Enable relationships in predicates to be used even if the data connector does not support it

Previously, only data connectors that supported the `relationships.relation_comparisons` capability supported
relationships in predicates. This change allows you to use relationships in boolean expressions even if the data
connector lacks support. When the capability is available, relationship predicates are resolved directly within the
native data connector for more efficient processing. If not, the predicates are handled at the API layer.

#### 2024-06-30

##### Require GraphQL Config

A build error is now raised if a [GraphQLConfig](reference/metadata-reference/graphql-config.mdx) is not provided in the
project metadata.

---

## Metadata structure


### v2_CompatibilityConfig {#v2_compatibilityconfig-v2_compatibilityconfig}

The compatibility configuration of the Hasura metadata.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `CompatibilityConfig` | true |  |
| `date` | [CompatibilityDate](#v2_compatibilityconfig-compatibilitydate) | true | Any backwards incompatible changes made to Hasura DDN after this date won't impact the metadata. |



#### CompatibilityDate {#v2_compatibilityconfig-compatibilitydate}

Any backwards incompatible changes made to Hasura DDN after this date won't impact the metadata


**One of the following values:**

| Value | Description |
|-----|-----|
| `2024-06-30` / `2024-09-03` / `2024-09-18` / `2024-09-26` / `2024-10-07` / `2024-10-16` / `2024-10-31` / `2024-11-13` / `2024-11-15` / `2024-11-18` / `2024-11-26` / `2024-12-05` / `2024-12-10` / `2024-12-18` / `2025-01-07` / `2025-01-25` / `2025-02-04` / `2025-02-08` / `2025-02-20` | Known compatibility dates |
| string | Any date |



--- File: ../ddn-docs/docs/reference/metadata-reference/engine-plugins.mdx ---
# Engine Plugins

---
sidebar_position: 16
sidebar_label: Engine Plugins
description: "LifecyclePluginHook is a metadata object that defines the configuration for engine lifecyle plugins."
keywords:
  - plugin config
  - engine plugins
  - pre-parse plugin
  - HTTP hooks
  - Lifecycle hooks
  - lifecycle plugin hooks
toc_max_heading_level: 4
---

# Engine Plugins

## Introduction

Engine plugins are a way to extend the functionality of the Hasura GraphQL Engine. Engine plugins are executed at
various stages of the request lifecycle. Engine plugins can be used to modify the request, response, or to perform
custom operations.

### How LifeCyclePluginHooks work

Lifecycle plugin hooks are not generated by default when a Hasura project is created. They need to be manually added to
the metadata. To see a guide, check out [this page](/plugins/allowlist/index.mdx) for an example.

When you create a LifeCyclePluginHook, you define the configuration for the plugin in your metadata by passing
information like the URL of the hosted plugin and its name. The plugin is then executed at the specified stage of the
request lifecycle.

After creating a new LifeCyclePluginHook in your metadata, you'll need to
[create a new build](/reference/cli/commands/ddn_supergraph_build_local.mdx) using the CLI.

### Examples

```yaml title="An example of a LifecyclePluginHook:"
kind: LifecyclePluginHook
version: v1
definition:
  name: cloudflare allowlist
  url:
    valueFromEnv: ALLOW_LIST_URL
  pre: parse
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            valueFromEnv: M_AUTH_KEY
      session: {}
      rawRequest:
        query: {}
        variables: {}
```

| Field                       | Description                                                               | Reference                                                                                                                                                    |
| --------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `name`                      | The name of the lifecycle plugin hook.                                    | [LifecyclePluginHookPreParse](#lifecyclepluginhook-lifecyclepreparsepluginhook)                                                                              |
| `url`                       | The URL to access the plugin.                                             | [LifecyclePluginHookPreParse](#lifecyclepluginhook-lifecyclepreparsepluginhook)                                                                              |
| `pre`                       | The stage of the request lifecycle.                                       | [LifecyclePluginHookPreParse](#lifecyclepluginhook-lifecyclepreparsepluginhook)                                                                              |
| `config`                    | Configuration for the plugin.                                             | [PreParse](#lifecyclepluginhook-lifecyclepreparsepluginhookconfig) or [PreResponse](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfig)               |
| `config.request`            | The shape of the request object.                                          | [PreParse](#lifecyclepluginhook-lifecyclepreparsepluginhookconfigrequest) or [PreResponse](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest) |
| `config.request.headers`    | The headers that should be included for each request.                     | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig)                                                                    |
| `config.request.rawRequest` | The configuration of the raw request, including any queries or variables. | [RawRequestConfig](#lifecyclepluginhook-rawrequestconfig)                                                                                                    |

## Metadata structure


### LifecyclePluginHook {#lifecyclepluginhook-lifecyclepluginhook}

Definition of a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `kind` | `LifecyclePluginHook` | true |  |
| `version` | `v1` | true |  |
| `definition` | [LifecyclePluginHookV1](#lifecyclepluginhook-lifecyclepluginhookv1) | true | Definition of a lifecycle plugin hook - version 1. |

 **Example:**

```yaml
kind: LifecyclePluginHook
version: v1
definition:
  pre: parse
  name: test
  url:
    value: http://localhost:8080
  config:
    request:
      headers:
        additional:
          hasura-m-auth:
            value: zZkhKqFjqXR4g5MZCsJUZCnhCcoPyZ
      session: {}
      rawRequest:
        query: {}
        variables: {}
```


#### LifecyclePluginHookV1 {#lifecyclepluginhook-lifecyclepluginhookv1}

Definition of a lifecycle plugin hook - version 1.


**One of the following values:**

| Value | Description |
|-----|-----|
| [LifecyclePreParsePluginHook](#lifecyclepluginhook-lifecyclepreparsepluginhook) | Definition of a lifecycle plugin hook for the pre-parse stage. |
| [LifecyclePreResponsePluginHook](#lifecyclepluginhook-lifecyclepreresponsepluginhook) | Definition of a lifecycle plugin hook for the pre-response stage. |
| [LifecyclePreRoutePluginHook](#lifecyclepluginhook-lifecyclepreroutepluginhook) | Definition of a lifecycle plugin hook for the pre-route stage. |



#### LifecyclePreRoutePluginHook {#lifecyclepluginhook-lifecyclepreroutepluginhook}

Definition of a lifecycle plugin hook for the pre-route stage.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `pre` | `route` | true |  |
| `name` | string | true | The name of the lifecycle plugin hook. |
| `url` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | true | The URL to access the lifecycle plugin hook. |
| `config` | [LifecyclePreRoutePluginHookConfig](#lifecyclepluginhook-lifecyclepreroutepluginhookconfig) | true | Configuration for the lifecycle plugin hook. |



#### LifecyclePreRoutePluginHookConfig {#lifecyclepluginhook-lifecyclepreroutepluginhookconfig}

Configuration for a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `matchPath` | string | true | Regex to match the request path |
| `matchMethods` | [[RequestMethod](#lifecyclepluginhook-requestmethod)] | true | Possible HTTP methods for the request |
| `request` | [LifecyclePreRoutePluginHookConfigRequest](#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequest) | true | Configuration for the request to the lifecycle plugin hook. |
| `response` | [LifecyclePreRoutePluginHookConfigResponse](#lifecyclepluginhook-lifecyclepreroutepluginhookconfigresponse) / null | false | Configuration for the response to the lifecycle plugin hook. |



#### LifecyclePreRoutePluginHookConfigResponse {#lifecyclepluginhook-lifecyclepreroutepluginhookconfigresponse}

Configuration for a lifecycle plugin hook response.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers in the response from the engine. |



#### LifecyclePreRoutePluginHookConfigRequest {#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequest}

Configuration for a lifecycle plugin hook request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers in the pre-route plugin hook HTTP requests. |
| `method` | [LifecyclePreRoutePluginHookConfigRequestMethods](#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequestmethods) | true | Configuration for the HTTP method for the pre-route plugin hook HTTP requests. |
| `rawRequest` | [PreRouteRequestConfig](#lifecyclepluginhook-prerouterequestconfig) | true | Configuration for the raw request body for the pre-route plugin hook HTTP requests. |



#### PreRouteRequestConfig {#lifecyclepluginhook-prerouterequestconfig}

Configuration for the raw request body for the pre-route plugin hook HTTP requests.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `path` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the request path of the incoming request |
| `method` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the request method of the incoming request |
| `query` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the query params of the incoming request |
| `body` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for adding/excluding the body of the incoming request |



#### LifecyclePreRoutePluginHookConfigRequestMethods {#lifecyclepluginhook-lifecyclepreroutepluginhookconfigrequestmethods}

Configuration for the method for the pre-route plugin hook HTTP requests.


**Value:** `GET` / `POST`


#### RequestMethod {#lifecyclepluginhook-requestmethod}

Possible HTTP Request Methods for the incoming requests handled by the pre-route plugin hook.


**Value:** `GET` / `POST` / `PUT` / `DELETE` / `PATCH`


#### LifecyclePreResponsePluginHook {#lifecyclepluginhook-lifecyclepreresponsepluginhook}

Definition of a lifecycle plugin hook for the pre-response stage.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `pre` | `response` | true |  |
| `name` | string | true | The name of the lifecycle plugin hook. |
| `url` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | true | The URL to access the lifecycle plugin hook. |
| `config` | [LifecyclePreResponsePluginHookConfig](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfig) | true | Configuration for the lifecycle plugin hook. |



#### LifecyclePreResponsePluginHookConfig {#lifecyclepluginhook-lifecyclepreresponsepluginhookconfig}

Configuration for a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `request` | [LifecyclePreResponsePluginHookConfigRequest](#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest) | true | Configuration for the request to the lifecycle plugin hook. |



#### LifecyclePreResponsePluginHookConfigRequest {#lifecyclepluginhook-lifecyclepreresponsepluginhookconfigrequest}

Configuration for a lifecycle plugin hook request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers. |
| `session` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the session (includes roles and session variables). |
| `rawRequest` | [RawRequestConfig](#lifecyclepluginhook-rawrequestconfig) | true | Configuration for the raw request. |
| `rawResponse` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the response. |



#### LifecyclePreParsePluginHook {#lifecyclepluginhook-lifecyclepreparsepluginhook}

Definition of a lifecycle plugin hook for the pre-parse stage.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `pre` | `parse` | true |  |
| `name` | string | true | The name of the lifecycle plugin hook. |
| `url` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | true | The URL to access the lifecycle plugin hook. |
| `config` | [LifecyclePreParsePluginHookConfig](#lifecyclepluginhook-lifecyclepreparsepluginhookconfig) | true | Configuration for the lifecycle plugin hook. |



#### LifecyclePreParsePluginHookConfig {#lifecyclepluginhook-lifecyclepreparsepluginhookconfig}

Configuration for a lifecycle plugin hook.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `request` | [LifecyclePreParsePluginHookConfigRequest](#lifecyclepluginhook-lifecyclepreparsepluginhookconfigrequest) | true | Configuration for the request to the lifecycle plugin hook. |



#### LifecyclePreParsePluginHookConfigRequest {#lifecyclepluginhook-lifecyclepreparsepluginhookconfigrequest}

Configuration for a lifecycle plugin hook request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `headers` | [LifecyclePluginHookHeadersConfig](#lifecyclepluginhook-lifecyclepluginhookheadersconfig) / null | false | Configuration for the headers. |
| `session` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the session (includes roles and session variables). |
| `rawRequest` | [RawRequestConfig](#lifecyclepluginhook-rawrequestconfig) | true | Configuration for the raw request. |



#### RawRequestConfig {#lifecyclepluginhook-rawrequestconfig}

Configuration for the raw request.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `query` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the query. |
| `variables` | [LeafConfig](#lifecyclepluginhook-leafconfig) / null | false | Configuration for the variables. |



#### LeafConfig {#lifecyclepluginhook-leafconfig}

Leaf Configuration.

| Key | Value | Required | Description |
|-----|-----|-----|-----|



#### LifecyclePluginHookHeadersConfig {#lifecyclepluginhook-lifecyclepluginhookheadersconfig}

Configuration for a lifecycle plugin hook headers.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `additional` | [HttpHeaders](#lifecyclepluginhook-httpheaders) / null | false | Additional headers to be sent with the request. |
| `forward` | [string] | false | Headers to be forwarded from the incoming request. |



#### HttpHeaders {#lifecyclepluginhook-httpheaders}

Key value map of HTTP headers to be sent with an HTTP request. The key is the header name and the value is a potential reference to an environment variable.

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `<customKey>` | [EnvironmentValue](#lifecyclepluginhook-environmentvalue) | false |  |



#### EnvironmentValue {#lifecyclepluginhook-environmentvalue}

Either a literal string or a reference to a Hasura secret


**Must have exactly one of the following fields:**

| Key | Value | Required | Description |
|-----|-----|-----|-----|
| `value` | string | false |  |
| `valueFromEnv` | string | false |  |



--- File: ../ddn-docs/docs/reference/cli/index.mdx ---
# Overview

---
sidebar_position: 1
sidebar_label: Overview
description:
  "Discover the power of the Hasura CLI, your new tool for managing your Hasura projects directly from your terminal."
keywords:
  - hasura cli
  - graphql api management
  - command line interface
  - database management
  - project management
  - hasura commands
  - data layer control
  - devops
  - software development
  - data source tunneling
hide_table_of_contents: true
seoFrontMatterUpdated: true
---

# Hasura CLI

## Introduction

Hasura introduces a new CLI, that gives you complete control over your data layer. You can create projects, apply
builds, and author metadata â€” all without leaving your favorite terminal.

## Find out more

- [Install the CLI](/reference/cli/installation.mdx)
- [Learn about the commands](/reference/cli/commands/index.mdx)



--- File: ../ddn-docs/docs/reference/cli/installation.mdx ---
# Installation

---
sidebar_position: 2
sidebar_label: Installation
description:
  "Explore the step-by-step guide for installing the Hasura Command Line Interface for various systems. Learn how to
  download, prepare, and move necessary binaries or build from source."
keywords:
  - hasura cli installation
  - setting up hasura
  - hasura installation how-to
  - hasura cli download
  - hasura binaries
  - hasura api
  - graphql application development
  - hasura setup guide
  - installing hasura on mac
  - hasura for linux
seoFrontMatterUpdated: true
toc_max_heading_level: 3
---

import InstallTheCli from "@site/docs/_install-the-cli.mdx";

# Installation {#install-instructions}

## Install the CLI

<InstallTheCli />

## Verify the installation

Running `ddn version` should print the CLI version, For example:

```
DDN CLI Version: v2.0.1
```

## Set up auto-completion (optional)

The DDN CLI supports auto-completion for [zsh](#zsh), [Bash](#bash), [PowerShell](#powershell) and [fish](#fish).

### zsh

**Temporary set up**

```sh title="To load completions in the current session:"
source <(ddn completion zsh)
```

**Permanent set up**

```sh title="Ensure shell completion is enabled by adding the following to your ~/.zshrc file:"
echo "autoload -U compinit; compinit" >> ~/.zshrc
```

#### macOS

```sass
ddn completion zsh > $(brew --prefix)/share/zsh/site-functions/_ddn
```

#### Linux

```sh title="Execute the following:"
ddn completion zsh > "${fpath[1]}/_ddn"
```

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.

### Bash

Install the [bash-completion package](https://github.com/scop/bash-completion) using your OS's package manager if it's
not already installed.

**Temporary set up**

```sh title="To load completions in the current session:"
source <(ddn completion bash)
```

**Permanent set up**

#### macOS

```sh title="Execute the following:"
ddn completion bash > $(brew --prefix)/etc/bash_completion.d/ddn
```

#### Linux

```sh title="Execute the following:"
ddn completion bash > /etc/bash_completion.d/ddn
```

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.

### PowerShell

#### Step 1. Generate the auto-completion script

Run the following command to generate the script for enabling auto-completion:

```sh
ddn completion powershell | Out-String | Invoke-Expression
```

This temporarily enables auto-completion for the current session.

#### Step 2. Locate your profile

```sh title="Locate your PowerShell profile by running the following command:"
$PROFILE
```

Your PowerShell profile is a script that runs each time PowerShell starts.

Common paths for the PowerShell profile are:

- Windows PowerShell: `C:\Users\<YourUsername>\Documents\WindowsPowerShell\Microsoft.PowerShell_profile.ps1`
- PowerShell Core: `C:\Users\<YourUsername>\Documents\PowerShell\Microsoft.PowerShell_profile.ps1`

#### Step 3. Edit your PowerShell profile

```sh title="Open the profile in a text editor:"
notepad $PROFILE
```

```plaintext title="Add the following line to the end of the file:"
ddn completion powershell | Out-String | Invoke-Expression
```

Save the file and close the editor.

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.

### fish

**Temporary set up**

```sh title="Execute the following:"
ddn completion fish | source
```

**Permanent set up**

```sh title="Execute the following:"
ddn completion fish > ~/.config/fish/completions/ddn.fish
```

:::note

Restart your shell so the changes take effect

:::

Try using the DDN CLI with auto-completion by typing part of a command and pressing `TAB` to verify.



--- File: ../ddn-docs/docs/reference/cli/faq.mdx ---
# ../ddn-docs/docs/reference/cli/faq.mdx

---
sidebar_position: 4
description: "Frequently asked questions about the DDN CLI."
keywords:
  - hasura cli installation
  - setting up hasura
  - hasura installation how-to
  - hasura cli download
  - hasura binaries
  - hasura api
  - graphql application development
  - hasura setup guide
  - installing hasura on mac
  - hasura for linux
  - faq
  - frequently asked question
seoFrontMatterUpdated: true
---

import Thumbnail from "@site/src/components/Thumbnail";

# Troubleshooting FAQ

## How do I see debug logs from CLI commands?

Add `--log-level=debug` to the command to see debug logs from the CLI.

## How do I see the output in JSON or YAML format instead of the table format that is shown by default?

Use `--out=json` or `--out=yaml` to see the output in the corresponding format instead of JSON.

## How do I output the version of the DDN CLI?

Run `ddn version` to see the CLI version.

## Why does my engine container crash upon running `ddn run docker-start` during local development?

The engine Docker container can crash due to various reasons:

1. The mandatory environment variables required to configure the engine were not provided.
2. The metadata files required by the engine are invalid or non-existent.
3. The engine version is not up to date to handle new metadata objects.

#### 1. The mandatory environment variables required to configure the engine were not provided

```text title="Error output"
error: the following required arguments were not provided:
--metadata-path <PATH>
--authn-config-path <PATH>

Usage: engine --metadata-path <PATH> --authn-config-path < PATH>

For more information, try '--help'
```

In this case, verify that the env vars `AUTHN_CONFIG_PATH`, `INTROSPECTION_METADATA_FILE` and `METADATA_PATH` are passed
to the engine container.

The default docker compose file generated by the CLI passes these values to the engine container:

```
AUTHN_CONFIG_PATH: /md/auth_config.json
ENABLE_CORS: "true"
INTROSPECTION_METADATA_FILE: /md/metadata.json
METADATA_PATH: /md/open_dd.json
OTLP_ENDPOINT: http://local.hasura.dev:4317
```

`AUTHN_CONFIG_PATH`, `INTROSPECTION_METADATA_FILE` and the `METADATA_PATH` are the paths to the files created as a
result of `ddn supergraph build local`. These paths should be configured based on where it is mounted in the engine
container.

#### 2. The metadata files required by the engine are invalid or non-existent {#engine-env-var-config}

```text title="Error output"
Error while starting up the engine: failed to build engine state - No such file or directory (os error 2)
```

These are the files created as a result of `ddn supergraph build local`. If these files do not exist, then it can be
recreated by running the command.

Make sure the files are mounted to the engine container and the paths to it configured using the environment variables
mentioned in [this section](#engine-env-var-config).

By default the CLI generates these files inside `engine/build` of the project directory and this directory is copied
when building the engine image by the following Dockerfile specified in the `dockerfile` field of the default docker
compose file.

```
FROM ghcr.io/hasura/v3-engine
COPY ./build /md/
```

#### 3. The engine version is not up to date to handle new metadata objects

New features introduced in DDN may have new metadata objects or fields associated with it. If your metadata uses any of
these new objects but the engine version you are running is an older version that doesn't support these objects, then
the engine will fail to startup. This can be resolved by using the latest engine version as mentioned in
[this section](#pull-latest-engine).

## `ddn run docker-start` is using a different ddn binary than the one in the current shell. {#docker-start-different-binary}

The `ddn run` command is just a convenient way to run arbitrary commands defined in the `scripts` section of
`.hasura/context.yaml` of the project directory.

The `docker-start` is just the name of the script defined in this file. You can see the definition of the script by
going to `.hasura/context.yaml`.

As these scripts run in a new shell, the DDN CLI must be present in the path for it to be invoked correctly when the
script is run.

## How do I check that all the required dependencies for the CLI are installed?

The DDN CLI provides a `ddn doctor` command that checks that all the required dependencies are installed.

```text title="ddn doctor Output"
ddn doctor
3:23PM INF Evaluating the dependencies of DDN CLI...
DDN CLI location: "/usr/local/bin/ddn"
DDN CLI version: "v2.15.0"
+------------------------------+--------+-----+
| Criteria                     | Result | Fix |
+------------------------------+--------+-----+
| Docker available             | YES    |     |
+------------------------------+--------+-----+
| Docker Engine running        | YES    |     |
+------------------------------+--------+-----+
| Docker Registry reachable    | YES    |     |
+------------------------------+--------+-----+
| Docker Compose available     | YES    |     |
+------------------------------+--------+-----+
| Docker Compose version valid | YES    |     |
+------------------------------+--------+-----+
| Control Plane reachable      | YES    |     |
+------------------------------+--------+-----+
| DDN CLI Authenticated        | YES    |     |
+------------------------------+--------+-----+
```

## How do I specify the latest engine version for local development? {#pull-latest-engine}

The `ddn run docker-start` command uses the `--pull` flag in the `docker-start` script when bringing up docker
containers, and so will use the latest engine version. If you're running a`docker compose` command directly, then add
the `--pull` flag to get the latest engine version.

## Why are traces are not showing up for local development?

The local engine container and all connector containers send traces to the OpenTelemetry Collector container which is
defined by default in the docker compose file initialized by the DDN CLI.

Traces are then exported to a cloud endpoint by the OpenTelemetry container as defined in the
`otel-collector-config.yaml` file initialized by the CLI. For this, the OpenTelemetry Collector requires a PAT token.

If you are not able to see traces during local development:

1. Make sure you are logged in to the DDN CLI. You can check this with `ddn auth print-access-token`. If not, log in by
   running `ddn auth login`.
2. Make sure that your OpenTelemetry Collector container is running.
3. If you are running the docker compose command directly, then make sure that the environment variable `HASURA_DDN_PAT`
   is passed to the OpenTelemetry Collector container. The value for this env var would be the output of the
   `ddn auth print-access-token` command. If you are running `ddn run docker-start`, then this is already handled for
   you.
4. If you are seeing the issue when running `ddn run docker-start`, then it could be due to the `docker-start` script
   using a [different binary to the one defined in the current shell](#docker-start-different-binary).

## I'm unable to use the latest features in my project.

If you are not able to see newly introduced features, then it could be because your metadata is outdated. New features
like subscriptions may need corresponding changes to your metadata. If your metadata was initialized by an older version
of the CLI, then these features may not be available and you [may need to upgrade your
metadata](/project/configuration/overview.mdx



--- File: ../ddn-docs/docs/reference/cli/commands/ddn.mdx ---
# ddn

---
sidebar_label: ddn
sidebar_position: 1
description: DDN Command Line Interface using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
---

# DDN CLI: ddn

DDN Command Line Interface.

## Synopsis

```

       

DDDDDDD\   DDDDDDD\   NN\   NN\ 
DD  __DD\  DD  __DD\  NNN\  NN |
DD |  DD | DD |  DD | NNNN\ NN |
DD |  DD | DD |  DD | NN NN\NN |
DD |  DD | DD |  DD | NN \NNNN |
DD |  DD | DD |  DD | NN |\NNN |
DDDDDDD  | DDDDDDD  | NN | \NN |
\_______/  \_______/  \__|  \__|



```

```bash
ddn [flags]
```

## Available operations

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth
- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory
- [ddn command](/reference/cli/commands/ddn_command) - Perform Command related operations
- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations
- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations
- [ddn console](/reference/cli/commands/ddn_console) - Open the DDN console
- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations
- [ddn doctor](/reference/cli/commands/ddn_doctor) - Check if the dependencies of DDN CLI are present
- [ddn model](/reference/cli/commands/ddn_model) - Perform Model related operations
- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI
- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project
- [ddn relationship](/reference/cli/commands/ddn_relationship) - Perform Relationship related operations
- [ddn run](/reference/cli/commands/ddn_run) - Run specific script from project's context config
- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph related operations
- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph related operations

## Options

```sass
-h, --help               help for ddn
    --log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
    --no-prompt          Do not prompt for required but missing flags
    --out string         Output format. Can be table, json or yaml. (default "table")
    --timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```




--- File: ../ddn-docs/docs/reference/cli/commands/index.mdx ---
# ddn

---
sidebar_label: ddn
sidebar_position: 1
description: DDN Command Line Interface using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn
---

# DDN CLI: Commands

## Overview

The DDN CLI utilizes a typical command / flag syntax enabling you to quickly manage your supergraph. A typical command
structure will follow this pattern:

```bash
ddn <command> <subcommand> <argument> --<flag> "<flag_value>"
```

:::info Note

A complete list of all CLI commands, including details and examples, can be found in the side navigation to the left
under the Commands heading.

:::

## Environment variables

You can configure additional behavior for the CLI using environment variables.

### `HASURA_DDN_PROJECT_DIRECTORY`

Set the `HASURA_DDN_PROJECT_DIRECTORY` environment variable to the path of your project directory, which contains the `hasura.yaml` file. This is helpful when scripting automations and ensures that any scripts or commands run from different locations will automatically reference the correct project configuration.

## Getting help

You can use the `--help` flag to get information on any command.

For example, running `ddn --help` will return information on all available operations and flags:

```

DDDDDDD\   DDDDDDD\   NN\   NN\
DD  __DD\  DD  __DD\  NNN\  NN |
DD |  DD | DD |  DD | NNNN\ NN |
DD |  DD | DD |  DD | NN NN\NN |
DD |  DD | DD |  DD | NN \NNNN |
DD |  DD | DD |  DD | NN |\NNN |
DDDDDDD  | DDDDDDD  | NN | \NN |
\_______/  \_______/  \__|  \__|

Usage:
  ddn [flags]
  ddn [command]

DDN operations
  project        Manage Hasura DDN Project

Metadata operations
  command        Perform Command related operations
  connector      Perform Connector related operations
  connector-link Perform DataConnectorLink related operations
  model          Perform Model related operations
  relationship   Perform Relationship related operations
  subgraph       Perform Subgraph related operations
  supergraph     Perform Supergraph related operations

Authentication operations
  auth           Manage Hasura DDN CLI Auth

Other operations
  codemod        Perform transformations on your Hasura project directory
  completion     Generate the autocompletion script for the specified shell
  console        Open the DDN console
  context        Perform context operations
  help           Help about any command
  plugins        Manage plugins for the CLI
  run            Run specific script from project's context config
  update-cli     Update this CLI to the latest version or to a specific version
  version        Prints the CLI version

Flags:
  -h, --help               help for ddn
      --log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
      --no-prompt          Do not prompt for required but missing flags
      --out string         Output format. Can be table, json or yaml. (default "table")
      --timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
  -v, --version            Prints the CLI version

Use "ddn [command] --help" for more information about a command.

```



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_auth.mdx ---
# ddn auth

---
sidebar_label: ddn auth
sidebar_position: 2
description: Manage Hasura DDN CLI Auth using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn auth
---

# DDN CLI: ddn auth

Manage Hasura DDN CLI Auth.

## Synopsis

Manage Hasura DDN CLI Auth

## Available operations

- [ddn auth generate-promptql-secret-key](/reference/cli/commands/ddn_auth_generate-promptql-secret-key) - Generates the project's PromptQL secret key and saves to global config
- [ddn auth login](/reference/cli/commands/ddn_auth_login) - Login to DDN
- [ddn auth logout](/reference/cli/commands/ddn_auth_logout) - Logout from DDN
- [ddn auth print-access-token](/reference/cli/commands/ddn_auth_print-access-token) - Prints the access token to STDOUT
- [ddn auth print-promptql-secret-key](/reference/cli/commands/ddn_auth_print-promptql-secret-key) - Prints the project's PromptQL secret key to STDOUT

## Options

```sass
-h, --help   help for auth
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_auth_generate-promptql-secret-key.mdx ---
# ddn auth generate-promptql-secret-key

---
sidebar_label: ddn auth generate-promptql-secret-key
sidebar_position: 3
description: Generates the project's PromptQL secret key and saves to global config using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn auth generate-promptql-secret-key
---

# DDN CLI: ddn auth generate-promptql-secret-key

Generates the project's PromptQL secret key and saves to global config.

## Synopsis

Generates the project's PromptQL secret key and saves to global config

```bash
ddn auth generate-promptql-secret-key [flags]
```

## Examples

```bash
# Generates a PromptQL secret key for the project set in the context and saves to global config
 ddn auth generate-promptql-secret-key
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for generate-promptql-secret-key
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_auth_login.mdx ---
# ddn auth login

---
sidebar_label: ddn auth login
sidebar_position: 4
description: Login to DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn auth login
---

# DDN CLI: ddn auth login

Login to DDN.

## Synopsis

Login to DDN

```bash
ddn auth login [flags]
```

## Examples

```bash
# Login with browser
 ddn auth login

# Login with Personal Access Token
 ddn auth login --access-token <your-access-token>
```

## Options

```sass
    --access-token string   The Personal Access Token or Service Account Token [env: HASURA_DDN_ACCESS_TOKEN]
-h, --help                  help for login
    --pat string            Personal Access token [env: HASURA_DDN_PAT] (DEPRECATED: use --access-token instead)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_auth_logout.mdx ---
# ddn auth logout

---
sidebar_label: ddn auth logout
sidebar_position: 5
description: Logout from DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn auth logout
---

# DDN CLI: ddn auth logout

Logout from DDN.

## Synopsis

Logout from DDN

```bash
ddn auth logout [flags]
```

## Examples

```bash
# Logout from DDN CLI
 ddn auth logout
```

## Options

```sass
-h, --help   help for logout
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_auth_print-access-token.mdx ---
# ddn auth print-access-token

---
sidebar_label: ddn auth print-access-token
sidebar_position: 6
description: Prints the access token to STDOUT using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn auth print-access-token
---

# DDN CLI: ddn auth print-access-token

Prints the access token to STDOUT.

## Synopsis

Prints the access token to STDOUT

```bash
ddn auth print-access-token [flags]
```

**Alias:** pp, print-pat

## Examples

```bash
# Print the access token as a string to STDOUT
 ddn auth print-access-token

# Print access token as a JSON to STDOUT
 ddn auth print-access-token --out json
```

## Options

```sass
-h, --help   help for print-access-token
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_auth_print-promptql-secret-key.mdx ---
# ddn auth print-promptql-secret-key

---
sidebar_label: ddn auth print-promptql-secret-key
sidebar_position: 7
description: Prints the project's PromptQL secret key to STDOUT using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn auth print-promptql-secret-key
---

# DDN CLI: ddn auth print-promptql-secret-key

Prints the project's PromptQL secret key to STDOUT.

## Synopsis

Prints the project's PromptQL secret key to STDOUT

```bash
ddn auth print-promptql-secret-key [flags]
```

**Alias:** pp

## Examples

```bash
# Print the PromptQL secret key of the project set in the context
 ddn auth print-promptql-secret-key

# Print PromptQL secret key as JSON
 ddn auth print-promptql-secret-key --out json
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for print-promptql-secret-key
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn auth](/reference/cli/commands/ddn_auth) - Manage Hasura DDN CLI Auth



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod.mdx ---
# ddn codemod

---
sidebar_label: ddn codemod
sidebar_position: 8
description: Perform transformations on your Hasura project directory using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod
---

# DDN CLI: ddn codemod

Perform transformations on your Hasura project directory.

## Synopsis

Perform transformations on your Hasura project directory

## Available operations

- [ddn codemod configure-header-forwarding](/reference/cli/commands/ddn_codemod_configure-header-forwarding) - Configure headers to be forwarded to your connector
- [ddn codemod fix-traces-env-var](/reference/cli/commands/ddn_codemod_fix-traces-env-var) - Fix env var used for configuring traces for connectors
- [ddn codemod rename-graphql-prefixes](/reference/cli/commands/ddn_codemod_rename-graphql-prefixes) - Rename GraphQL root field and type name prefixes in metadata
- [ddn codemod upgrade-auth-config-to-v3](/reference/cli/commands/ddn_codemod_upgrade-auth-config-to-v3) - Upgrade AuthConfig version from "v1"/"v2" to "v3"
- [ddn codemod upgrade-context-v2-to-v3](/reference/cli/commands/ddn_codemod_upgrade-context-v2-to-v3) - Upgrade project's context config from v2 to v3
- [ddn codemod upgrade-graphqlconfig-aggregate](/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-aggregate) - Upgrade GraphqlConfig to support aggregates
- [ddn codemod upgrade-graphqlconfig-subscriptions](/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-subscriptions) - Upgrade GraphqlConfig to support subscriptions
- [ddn codemod upgrade-model-v1-to-v2](/reference/cli/commands/ddn_codemod_upgrade-model-v1-to-v2) - Upgrade model from "version" "v1" to "v2" in metadata
- [ddn codemod upgrade-object-boolean-expression-types](/reference/cli/commands/ddn_codemod_upgrade-object-boolean-expression-types) - Upgrade object boolean expression types metadata
- [ddn codemod upgrade-project-config-v2-to-v3](/reference/cli/commands/ddn_codemod_upgrade-project-config-v2-to-v3) - Upgrade project directory from version v2 to v3
- [ddn codemod upgrade-supergraph-config-v1-to-v2](/reference/cli/commands/ddn_codemod_upgrade-supergraph-config-v1-to-v2) - Upgrade all Supergraph config files at the root of the project directory from v1 to v2

## Options

```sass
-h, --help   help for codemod
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_configure-header-forwarding.mdx ---
# ddn codemod configure-header-forwarding

---
sidebar_label: ddn codemod configure-header-forwarding
sidebar_position: 9
description: Configure headers to be forwarded to your connector using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod configure-header-forwarding
---

# DDN CLI: ddn codemod configure-header-forwarding

Configure headers to be forwarded to your connector.

## Synopsis

Configure headers to be forwarded to your connector

```bash
ddn codemod configure-header-forwarding connector-name [flags]
```

## Examples

```bash
# Configure header forwarding for the DataConnectorLink corresponding to Connector `mypostgres`
 ddn codemod configure-header-forwarding mypostgres

# Configure header forwarding for the DataConnectorLink corresponding to Connector config file ./connector.yaml
 ddn codemod configure-header-forwarding --connector ./connector.yaml

# Configure header forwarding for the DataConnectorLink `mypostgres`
 ddn codemod configure-header-forwarding --connector-link mypostgres
```

## Options

```sass
    --connector string        Path to Connector config file
    --connector-link string   Name of the DataConnectorLink to configure
-h, --help                    help for configure-header-forwarding
    --subgraph string         Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_fix-traces-env-var.mdx ---
# ddn codemod fix-traces-env-var

---
sidebar_label: ddn codemod fix-traces-env-var
sidebar_position: 11
description: Fix env var used for configuring traces for connectors using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod fix-traces-env-var
---

# DDN CLI: ddn codemod fix-traces-env-var

Fix env var used for configuring traces for connectors.

## Synopsis

Fix env var used for configuring traces for connectors

```bash
ddn codemod fix-traces-env-var --dir <project-dir> [flags]
```

## Examples

```bash
# Fix env var used for configuring traces for connector
 ddn codemod fix-traces-env-var --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for fix-traces-env-var
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_rename-graphql-prefixes.mdx ---
# ddn codemod rename-graphql-prefixes

---
sidebar_label: ddn codemod rename-graphql-prefixes
sidebar_position: 12
description: Rename GraphQL root field and type name prefixes in metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod rename-graphql-prefixes
---

# DDN CLI: ddn codemod rename-graphql-prefixes

Rename GraphQL root field and type name prefixes in metadata.

## Synopsis

Rename GraphQL root field and type name prefixes in metadata. The from prefix will be stripped if provided, and the new prefix will be added. If the new prefix is already present, it will not be reapplied.

By default, subgraph.yaml is updated with the new prefixes.

```bash
ddn codemod rename-graphql-prefixes [flags]
```

## Examples

```bash
# Add root field and type name prefixes to the subgraph set in the context
 ddn codemod rename-graphql-prefixes --graphql-root-field 'app_' --graphql-type-name 'App_'

# Change the root field prefix for the specified subgraph without modifying subgraph.yaml
 ddn codemod rename-graphql-prefixes --subgraph app/subgraph.yaml --graphql-root-field 'foo_' --from-graphql-root-field 'app_' --no-update-subgraph-config
```

## Options

```sass
    --ci                               Disables the use of context
-c, --context string                   Name of the context to use. (default <current_context>)
    --from-graphql-root-field string   The previous GraphQL root field prefix
    --from-graphql-type-name string    The previous GraphQL type name prefix
    --graphql-root-field string        The new GraphQL root field prefix
    --graphql-type-name string         The new GraphQL type name prefix
-h, --help                             help for rename-graphql-prefixes
    --no-update-subgraph-config        Do not update the subgraph config with the new prefixes
    --subgraph string                  Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-auth-config-to-v3.mdx ---
# ddn codemod upgrade-auth-config-to-v3

---
sidebar_label: ddn codemod upgrade-auth-config-to-v3
sidebar_position: 13
description: Upgrade AuthConfig version from "v1"/"v2" to "v3" using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-auth-config-to-v3
---

# DDN CLI: ddn codemod upgrade-auth-config-to-v3

Upgrade AuthConfig version from "v1"/"v2" to "v3".

## Synopsis

Upgrade AuthConfig version from "v1"/"v2" to "v3"

```bash
ddn codemod upgrade-auth-config-to-v3 [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context. Finds `AuthConfig` present in any subgraphs within the supergraph and modifies it to version v3
 ddn codemod upgrade-auth-config-to-v3

# Run on a specific supergraph config file. Finds `AuthConfig` present in any subgraphs within the supergraph and modifies it to version v3
 ddn codemod upgrade-auth-config-to-v3 --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph config file. Finds `AuthConfig` present in the subgraph and modifies it to version v3
 ddn codemod upgrade-auth-config-to-v3 --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-auth-config-to-v3
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-context-v2-to-v3.mdx ---
# ddn codemod upgrade-context-v2-to-v3

---
sidebar_label: ddn codemod upgrade-context-v2-to-v3
sidebar_position: 14
description: Upgrade project's context config from v2 to v3 using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-context-v2-to-v3
---

# DDN CLI: ddn codemod upgrade-context-v2-to-v3

Upgrade project's context config from v2 to v3.

## Synopsis

Upgrade project's context config from v2 to v3

```bash
ddn codemod upgrade-context-v2-to-v3 --dir <project-dir> [flags]
```

## Examples

```bash
# Upgrade context present in the current Hasura directory from v2 to v3
 ddn codemod upgrade-context-v2-to-v3 --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for upgrade-context-v2-to-v3
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-aggregate.mdx ---
# ddn codemod upgrade-graphqlconfig-aggregate

---
sidebar_label: ddn codemod upgrade-graphqlconfig-aggregate
sidebar_position: 15
description: Upgrade GraphqlConfig to support aggregates using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-graphqlconfig-aggregate
---

# DDN CLI: ddn codemod upgrade-graphqlconfig-aggregate

Upgrade GraphqlConfig to support aggregates.

## Synopsis

Upgrade GraphqlConfig to support aggregates

```bash
ddn codemod upgrade-graphqlconfig-aggregate [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-graphqlconfig-aggregate

# Run on a specific supergraph
 ddn codemod upgrade-graphqlconfig-aggregate --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-graphqlconfig-aggregate --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-graphqlconfig-aggregate
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-graphqlconfig-subscriptions.mdx ---
# ddn codemod upgrade-graphqlconfig-subscriptions

---
sidebar_label: ddn codemod upgrade-graphqlconfig-subscriptions
sidebar_position: 16
description: Upgrade GraphqlConfig to support subscriptions using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-graphqlconfig-subscriptions
---

# DDN CLI: ddn codemod upgrade-graphqlconfig-subscriptions

Upgrade GraphqlConfig to support subscriptions.

## Synopsis

Upgrade GraphqlConfig to support subscriptions

```bash
ddn codemod upgrade-graphqlconfig-subscriptions [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-graphqlconfig-subscriptions

# Run on a specific supergraph
 ddn codemod upgrade-graphqlconfig-subscriptions --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-graphqlconfig-subscriptions --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-graphqlconfig-subscriptions
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-model-v1-to-v2.mdx ---
# ddn codemod upgrade-model-v1-to-v2

---
sidebar_label: ddn codemod upgrade-model-v1-to-v2
sidebar_position: 17
description: Upgrade model from "version" "v1" to "v2" in metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-model-v1-to-v2
---

# DDN CLI: ddn codemod upgrade-model-v1-to-v2

Upgrade model from "version" "v1" to "v2" in metadata.

## Synopsis

Upgrade model from "version" "v1" to "v2" in metadata

```bash
ddn codemod upgrade-model-v1-to-v2 [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-model-v1-to-v2

# Run on a specific supergraph
 ddn codemod upgrade-model-v1-to-v2 --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-model-v1-to-v2 --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-model-v1-to-v2
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-object-boolean-expression-types.mdx ---
# ddn codemod upgrade-object-boolean-expression-types

---
sidebar_label: ddn codemod upgrade-object-boolean-expression-types
sidebar_position: 18
description: Upgrade object boolean expression types metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-object-boolean-expression-types
---

# DDN CLI: ddn codemod upgrade-object-boolean-expression-types

Upgrade object boolean expression types metadata.

## Synopsis

Upgrade object boolean expression types metadata

```bash
ddn codemod upgrade-object-boolean-expression-types [flags]
```

## Examples

```bash
# Run on the supergraph defined in the context
 ddn codemod upgrade-object-boolean-expression-types

# Run on a specific supergraph
 ddn codemod upgrade-object-boolean-expression-types --supergraph ./supergraph.cloud.yaml

# Run on a specific subgraph
 ddn codemod upgrade-object-boolean-expression-types --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for upgrade-object-boolean-expression-types
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-project-config-v2-to-v3.mdx ---
# ddn codemod upgrade-project-config-v2-to-v3

---
sidebar_label: ddn codemod upgrade-project-config-v2-to-v3
sidebar_position: 19
description: Upgrade project directory from version v2 to v3 using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-project-config-v2-to-v3
---

# DDN CLI: ddn codemod upgrade-project-config-v2-to-v3

Upgrade project directory from version v2 to v3.

## Synopsis

Upgrade project directory from version v2 to v3

```bash
ddn codemod upgrade-project-config-v2-to-v3 --dir <project-dir> [flags]
```

## Examples

```bash
# Upgrade project in the current directory from v2 to v3
 ddn codemod upgrade-project-config-v2-to-v3 --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for upgrade-project-config-v2-to-v3
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_codemod_upgrade-supergraph-config-v1-to-v2.mdx ---
# ddn codemod upgrade-supergraph-config-v1-to-v2

---
sidebar_label: ddn codemod upgrade-supergraph-config-v1-to-v2
sidebar_position: 20
description: Upgrade all Supergraph config files at the root of the project directory from v1 to v2 using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn codemod upgrade-supergraph-config-v1-to-v2
---

# DDN CLI: ddn codemod upgrade-supergraph-config-v1-to-v2

Upgrade all Supergraph config files at the root of the project directory from v1 to v2.

## Synopsis

Upgrade all Supergraph config files at the root of the project directory from v1 to v2

```bash
ddn codemod upgrade-supergraph-config-v1-to-v2 --dir <project-dir> [flags]
```

## Examples

```bash
# Upgrade all Supergraph config files in the current directory from v1 to v2
 ddn codemod upgrade-supergraph-config-v1-to-v2 --dir .
```

## Options

```sass
    --dir string   The Hasura project directory (required)
-h, --help         help for upgrade-supergraph-config-v1-to-v2
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn codemod](/reference/cli/commands/ddn_codemod) - Perform transformations on your Hasura project directory



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_command.mdx ---
# ddn command

---
sidebar_label: ddn command
sidebar_position: 21
description: Perform Command related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn command
---

# DDN CLI: ddn command

Perform Command related operations.

## Synopsis

Perform Command related operations

**Alias:** commands

## Available operations

- [ddn command add](/reference/cli/commands/ddn_command_add) - Add new Commands to the local metadata
- [ddn command list](/reference/cli/commands/ddn_command_list) - Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands
- [ddn command remove](/reference/cli/commands/ddn_command_remove) - Removes Commands (and related metadata) in the local metadata
- [ddn command show](/reference/cli/commands/ddn_command_show) - Show diff between the command and its corresponding ndc function/procedure
- [ddn command update](/reference/cli/commands/ddn_command_update) - Update Commands in the local metadata

## Options

```sass
-h, --help   help for command
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_command_add.mdx ---
# ddn command add

---
sidebar_label: ddn command add
sidebar_position: 22
description: Add new Commands to the local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn command add
---

# DDN CLI: ddn command add

Add new Commands to the local metadata.

## Synopsis

Add new Commands to the local metadata

```bash
ddn command add <connector-link-name> <procedure/function-name> [flags]
```

## Examples

```bash
# Add all Commands for DataConnectorLink "myfns" in the subgraph set in the context
 ddn command add myfns "*"

# Add a new Command "Login" from the procedure "Login" in the DataConnectorLink "myfns" in "app" Subgraph
 ddn command add myfns Login --subgraph ./app/subgraph.yaml

# Add all the Commands from the procedures/functions in the DataConnectorLink "myfns" in "app" Subgraph
 ddn command add myfns "*" --subgraph ./app/subgraph.yaml

# Add Commands filtered by glob pattern from the procedures/functions in the DataConnectorLink "myfns" in "app" Subgraph
 ddn command add myfns "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for add
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_command_list.mdx ---
# ddn command list

---
sidebar_label: ddn command list
sidebar_position: 23
description: Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn command list
---

# DDN CLI: ddn command list

Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands.

## Synopsis

Lists details about the functions/procedures of DataConnectorLink, and their corresponding Commands

```bash
ddn command list <connector-link-name> [flags]
```

## Examples

```bash
# List details about the functions/procedures of DataConnectorLink `mydb`, and their corresponding Commands
 ddn command list mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for list
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_command_remove.mdx ---
# ddn command remove

---
sidebar_label: ddn command remove
sidebar_position: 24
description: Removes Commands (and related metadata) in the local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn command remove
---

# DDN CLI: ddn command remove

Removes Commands (and related metadata) in the local metadata.

## Synopsis

Removes Commands (and related metadata) in the local metadata

```bash
ddn command remove <command-name> [flags]
```

## Examples

```bash
# Remove all Commands using the subgraph set in current context
 ddn command remove "*"

# Remove the Command "Album" in the "app" Subgraph
 ddn command remove Album --subgraph ./app/subgraph.yaml

# Remove all the Commands in the Subgraph "app"
 ddn command remove "*" --subgraph ./app/subgraph.yaml

# Remove Command filtered by glob pattern in the Subgraph "app"
 ddn command remove "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for remove
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_command_show.mdx ---
# ddn command show

---
sidebar_label: ddn command show
sidebar_position: 25
description: Show diff between the command and its corresponding ndc function/procedure using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn command show
---

# DDN CLI: ddn command show

Show diff between the command and its corresponding ndc function/procedure.

## Synopsis

Show diff between the command and its corresponding ndc function/procedure

```bash
ddn command show <command-name> [flags]
```

## Examples

```bash
# Show diff between the 'InsertUsers' command and its corresponding ndc function/procedure
 ddn command show InsertUsers

# Show diff between commands and their corresponding ndc functions/procedures for all commands matching glob pattern 'Delete*'
 ddn command show "Delete*"
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_command_update.mdx ---
# ddn command update

---
sidebar_label: ddn command update
sidebar_position: 26
description: Update Commands in the local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn command update
---

# DDN CLI: ddn command update

Update Commands in the local metadata.

## Synopsis

Update Commands in the local metadata

```bash
ddn command update <command-name> [flags]
```

## Examples

```bash
# Update all Commands using the subgraph set in current context
 ddn command update "*"

# Update the Command "Login" in the "app" Subgraph
 ddn command update Login --subgraph ./app/subgraph.yaml

# Update all the Commands in "app" Subgraph
 ddn command update "*" --subgraph ./app/subgraph.yaml

# Update Commands filtered by glob pattern in the Subgraph "app"
 ddn command update "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for update
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn command](/reference/cli/commands/ddn_command) - Perform Command related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector.mdx ---
# ddn connector

---
sidebar_label: ddn connector
sidebar_position: 27
description: Perform Connector related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector
---

# DDN CLI: ddn connector

Perform Connector related operations.

## Synopsis

Perform Connector related operations

**Alias:** connectors

## Available operations

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild related operations
- [ddn connector env](/reference/cli/commands/ddn_connector_env) - Manage environment variables for Connectors
- [ddn connector init](/reference/cli/commands/ddn_connector_init) - Add a new Connector
- [ddn connector introspect](/reference/cli/commands/ddn_connector_introspect) - Introspect the Connector data source to update the Connector configuration
- [ddn connector plugin](/reference/cli/commands/ddn_connector_plugin) - Run a subcommand from a Connector plugin
- [ddn connector remove](/reference/cli/commands/ddn_connector_remove) - Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects
- [ddn connector setenv](/reference/cli/commands/ddn_connector_setenv) - Run specified command with environment variables set
- [ddn connector show-resources](/reference/cli/commands/ddn_connector_show-resources) - Show resources of a Connector

## Options

```sass
-h, --help   help for connector
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_build.mdx ---
# ddn connector build

---
sidebar_label: ddn connector build
sidebar_position: 28
description: Perform ConnectorBuild related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector build
---

# DDN CLI: ddn connector build

Perform ConnectorBuild related operations.

## Synopsis

Perform ConnectorBuild related operations

## Available operations

- [ddn connector build create](/reference/cli/commands/ddn_connector_build_create) - Create a ConnectorBuild on Hasura DDN
- [ddn connector build delete](/reference/cli/commands/ddn_connector_build_delete) - Delete a ConnectorBuild from a Project
- [ddn connector build get](/reference/cli/commands/ddn_connector_build_get) - List ConnectorBuilds or get details of a specific one from Hasura DDN
- [ddn connector build logs](/reference/cli/commands/ddn_connector_build_logs) - Get logs of a ConnectorBuild from Hasura DDN

## Options

```sass
-h, --help   help for build
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_build_create.mdx ---
# ddn connector build create

---
sidebar_label: ddn connector build create
sidebar_position: 29
description: Create a ConnectorBuild on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector build create
---

# DDN CLI: ddn connector build create

Create a ConnectorBuild on Hasura DDN.

## Synopsis

Create a ConnectorBuild on Hasura DDN

```bash
ddn connector build create --connector <path-to-connector-config-file> [flags]
```

## Options

```sass
    --ci                             Disables the use of context
    --connector string               Path to Connector config file
-c, --context string                 Name of the context to use. (default <current_context>)
-e, --env stringArray                Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray           Path to .env file. Can be repeated to provide multiple env files
-h, --help                           help for create
-p, --project string                 DDN Project name
    --target-connector-link string   DataConnectorLink to update with the schema from the ConnectorBuild
    --target-env-file string         Path to the env file in which the Build URLs should be updated in
    --target-subgraph string         Path to Subgraph config file containing target DataConnectorLink
    --update-connector-link-schema   Update DataConnectorLink schema with the NDC schema of the built connector. (default: false)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_build_delete.mdx ---
# ddn connector build delete

---
sidebar_label: ddn connector build delete
sidebar_position: 30
description: Delete a ConnectorBuild from a Project using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector build delete
---

# DDN CLI: ddn connector build delete

Delete a ConnectorBuild from a Project.

## Synopsis

Delete a ConnectorBuild from a Project

```bash
ddn connector build delete <connector-build-id> [flags]
```

## Examples

```bash
# Delete a ConnectorBuild
 ddn connector build delete 1d4f4831-54a2-4ded-b680-07d00510a522
```

## Options

```sass
-h, --help   help for delete
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_build_get.mdx ---
# ddn connector build get

---
sidebar_label: ddn connector build get
sidebar_position: 31
description: List ConnectorBuilds or get details of a specific one from Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector build get
---

# DDN CLI: ddn connector build get

List ConnectorBuilds or get details of a specific one from Hasura DDN.

## Synopsis

List ConnectorBuilds or get details of a specific one from Hasura DDN

```bash
ddn connector build get [connector-build-id] [flags]
```

## Examples

```bash
# Get details of a ConnectorBuild
 ddn connector build get 1d4f4831-54a2-4ded-b680-07d00510a522

# Get details of all ConnectorBuilds for Connector "mydb" for a Project and Subgraph
 ddn connector build get --connector-name mydb --project myproject --subgraph-name myapp

# Get details of all ConnectorBuilds for Connector defined in a Connector config file
 ddn connector build get --connector ./myapp/connector/connector.cloud.yaml

# Get details of all ConnectorBuilds for a Project
 ddn connector build get --project myproject

# Get details of all ConnectorBuilds for a Subgraph in a Project
 ddn connector build get --project myproject --subgraph-name myapp

# Get NDC Schema of a ConnectorBuild
 ddn connector build get 1d4f4831-54a2-4ded-b680-07d00510a522 --schema
```

## Options

```sass
    --ci                      Disables the use of context
    --connector string        Path to Connector config file
    --connector-name string   Connector name
-c, --context string          Name of the context to use. (default <current_context>)
-h, --help                    help for get
-p, --project string          DDN Project name
    --schema                  Get NDC schema of ConnectorBuild
    --subgraph-name string    Subgraph name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_build_logs.mdx ---
# ddn connector build logs

---
sidebar_label: ddn connector build logs
sidebar_position: 32
description: Get logs of a ConnectorBuild from Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector build logs
---

# DDN CLI: ddn connector build logs

Get logs of a ConnectorBuild from Hasura DDN.

## Synopsis

Get logs of a ConnectorBuild from Hasura DDN

```bash
ddn connector build logs <connector-build-id> [flags]
```

## Examples

```bash
# Get running deploy logs
 ddn connector build logs <connector-build-id>

# Get running deploy logs and keep following
 ddn connector build logs <connector-build-id> --follow

# Get running deploy logs and keep following since a specified time duration
 ddn connector build logs <connector-build-id> --follow --since 10m

# Get build logs of a ConnectorBuild
 ddn connector build logs <connector-build-id> --build
```

## Options

```sass
    --build          Specifies whether to show the build logs. (default: false)
    --follow         Specifies whether to continuously follow the deployment logs. (default: false)
-h, --help           help for logs
    --since string   Specifies the starting time for log retrieval. Can be in ISO format (e.g. 2024-03-26T11:05:15Z) or a duration (e.g. 5m for 5 minutes ago). (By default, prints the entire available logs).
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector build](/reference/cli/commands/ddn_connector_build) - Perform ConnectorBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_env.mdx ---
# ddn connector env

---
sidebar_label: ddn connector env
sidebar_position: 33
description: Manage environment variables for Connectors using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector env
---

# DDN CLI: ddn connector env

Manage environment variables for Connectors.

## Synopsis

Manage environment variables for Connectors

## Available operations

- [ddn connector env add](/reference/cli/commands/ddn_connector_env_add) - Add environment variables and its corresponding mapping to a Connector

## Options

```sass
-h, --help   help for env
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_env_add.mdx ---
# ddn connector env add

---
sidebar_label: ddn connector env add
sidebar_position: 34
description: Add environment variables and its corresponding mapping to a Connector using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector env add
---

# DDN CLI: ddn connector env add

Add environment variables and its corresponding mapping to a Connector.

## Synopsis

Add environment variables and its corresponding mapping to a Connector

```bash
ddn connector env add [flags]
```

## Examples

```bash
# Add new environment variable to Connector mydb
 ddn connector env add mydb --env NEW_VAR=value

# Add new environment variables to a specific env file
 ddn connector env add mydb --subgraph ./subgraph/subgraph.yaml --env VAR1=VAL1 --env VAR2=VAL2 --target-env-file .env.local

# Add new environment variable to Connector located at ./foo/my_db/connector.yaml
 ddn connector env add --connector ./foo/my_db/connector.yaml --env NEW_VAR=value
```

## Options

```sass
    --ci                            Disables the use of context
    --connector string              Path to Connector config file
-c, --context string                Name of the context to use. (default <current_context>)
-e, --env stringArray               Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
-h, --help                          help for add
    --target-env-file stringArray   Path to the specific environment file to update (optional)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector env](/reference/cli/commands/ddn_connector_env) - Manage environment variables for Connectors



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_init.mdx ---
# ddn connector init

---
sidebar_label: ddn connector init
sidebar_position: 35
description: Add a new Connector using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector init
---

# DDN CLI: ddn connector init

Add a new Connector.

## Synopsis

Add a new Connector

```bash
ddn connector init [connector-name] --hub-connector <connector-type> [flags]
```

## Examples

```bash
# Initialize a Connector interactively in a step by step manner
 ddn connector init -i

# Initialize a Postgres Connector "mydb" in the Subgraph "app"
 ddn connector init mydb --subgraph ./app/subgraph.yaml --hub-connector hasura/postgres

# Initialize a Postgres Connector "mydb" inside the directory ./connector
 ddn connector init mydb --dir ./connector --hub-connector hasura/postgres

# Initialize a NodeJS Connector "mylambda" in the Subgraph "app" on port 8765
 ddn connector init mylambda --subgraph ./app/subgraph.yaml  --hub-connector hasura/nodejs --configure-port 8765 
```

## Options

```sass
    --add-env stringArray            Environment variable to set in the Connector. Can be repeated to provide multiple env vars
    --add-to-compose-file string     The compose file to include the generated connector compose file
    --ci                             Disables the use of context
    --configure-port string          Initialize the connector with an already configured port
-c, --context string                 Name of the context to use. (default <current_context>)
    --dir string                     Directory to initialize the Connector
-h, --help                           help for init
    --hub-connector string           Name and version of Connector in Hasura Connector Hub. ref: https://hasura.io/connectors
-i, --interactive                    Interactive mode
    --no-link                        Do not create a ConnectorLink
    --subgraph string                Subgraph to initialize the Connector in
    --target-cloud-env-file string   Path to the cloud environment file
    --target-local-env-file string   Path to the local environment file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_introspect.mdx ---
# ddn connector introspect

---
sidebar_label: ddn connector introspect
sidebar_position: 36
description: Introspect the Connector data source to update the Connector configuration using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector introspect
---

# DDN CLI: ddn connector introspect

Introspect the Connector data source to update the Connector configuration.

## Synopsis

Introspect the Connector data source to update the Connector configuration

```bash
ddn connector introspect <connector-name> --subgraph <path-to-subgraph-config-file> [flags]
```

## Examples

```bash
# Introspect Connector my_db from Subgraph located at ./foo/subgraph.yaml and update DataConnectorLink
 ddn connector introspect my_db --subgraph ./foo/subgraph.yaml

# Introspect Connector located at ./foo/my_db/connector.yaml
 ddn connector introspect --connector ./foo/my_db/connector.yaml

# Introspect Connector my_db but do not update DataConnectorLink
 ddn connector introspect my_db --subgraph ./foo/subgraph.yaml --no-update-link
```

## Options

```sass
    --add-all-resources      Add all Models, Commands and Relationships from the updated DataConnectorLink to the local metadata
    --ci                     Disables the use of context
    --connector string       Path to Connector config file
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for introspect
    --no-update-link         Ignore updating DataConnectorLink in the metadata
    --subgraph string        Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_plugin.mdx ---
# ddn connector plugin

---
sidebar_label: ddn connector plugin
sidebar_position: 37
description: Run a subcommand from a Connector plugin using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector plugin
---

# DDN CLI: ddn connector plugin

Run a subcommand from a Connector plugin.

## Synopsis

Run a subcommand from a Connector plugin

```bash
ddn connector plugin [flags] [-- <args>]
```

## Examples

```bash
# Run the "update" command of a connector
 ddn connector plugin --connector app/connector/mypostgres/connector.yaml -- update
```

## Options

```sass
    --ci                     Disables the use of context
    --connector string       Path to Connector config file
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for plugin
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_remove.mdx ---
# ddn connector remove

---
sidebar_label: ddn connector remove
sidebar_position: 38
description: Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector remove
---

# DDN CLI: ddn connector remove

Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects.

## Synopsis

Remove a Connector, its corresponding DataConnectorLink and all its associated metadata objects

```bash
ddn connector remove <connector-name> [flags]
```

**Alias:** rm

## Examples

```bash
# Remove Connector my_db and associated DataConnectorLink from the default subgraph
 ddn connector remove my_db

# Remove Connector my_db from Subgraph located at ./foo/subgraph.yaml and associated DataConnectorLink
 ddn connector remove my_db --subgraph ./foo/subgraph.yaml

# Remove Connector located at ./foo/my_db/connector.yaml with DataConnectorLink given by app/subgraph.yaml and name my_db
 ddn connector remove --connector ./foo/my_db/connector.yaml --subgraph app/subgraph.yaml --connector-link my_db
```

## Options

```sass
    --ci                                Disables the use of context
    --connector string                  Path to Connector config file
    --connector-link string             DataConnectorLink corresponding to the Connector
-c, --context string                    Name of the context to use. (default <current_context>)
-h, --help                              help for remove
    --remove-from-compose-file string   The compose file to include the generated connector compose file
    --subgraph string                   SubgraphConfig file to remove the Connector from
    --target-env-file stringArray       Paths to the .env files
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_setenv.mdx ---
# ddn connector setenv

---
sidebar_label: ddn connector setenv
sidebar_position: 39
description: Run specified command with environment variables set using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector setenv
---

# DDN CLI: ddn connector setenv

Run specified command with environment variables set.

## Synopsis

Run specified command with environment variables set

```bash
ddn connector setenv --connector <path-to-connector-config-file> -- <command> [flags]
```

## Examples

```bash
# Set environment variables for the Connector data source
 ddn connector setenv --connector ./foo/my_db.connector.local.yaml -- npm run start
```

## Options

```sass
    --ci                     Disables the use of context
    --connector string       Path to Connector config file
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for setenv
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector_show-resources.mdx ---
# ddn connector show-resources

---
sidebar_label: ddn connector show-resources
sidebar_position: 40
description: Show resources of a Connector using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector show-resources
---

# DDN CLI: ddn connector show-resources

Show resources of a Connector.

## Synopsis

Show resources of a Connector

```bash
ddn connector show-resources <connector-name> [flags]
```

## Examples

```bash
# Show resources for Connector `mydb`
 ddn connector show-resources mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show-resources
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector](/reference/cli/commands/ddn_connector) - Perform Connector related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector-link.mdx ---
# ddn connector-link

---
sidebar_label: ddn connector-link
sidebar_position: 42
description: Perform DataConnectorLink related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector-link
---

# DDN CLI: ddn connector-link

Perform DataConnectorLink related operations.

## Synopsis

Perform DataConnectorLink related operations

**Alias:** connector-links

## Available operations

- [ddn connector-link add](/reference/cli/commands/ddn_connector-link_add) - Add a new DataConnectorLink to a Subgraph
- [ddn connector-link add-resources](/reference/cli/commands/ddn_connector-link_add-resources) - Add all models, commands and relationships from a DataConnectorLink's schema
- [ddn connector-link remove](/reference/cli/commands/ddn_connector-link_remove) - Remove a DataConnectorLink from a Subgraph
- [ddn connector-link show](/reference/cli/commands/ddn_connector-link_show) - Show DataConnectorLink details
- [ddn connector-link update](/reference/cli/commands/ddn_connector-link_update) - Fetch NDC details from the Connector and update the DataConnectorLink

## Options

```sass
-h, --help   help for connector-link
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector-link_add.mdx ---
# ddn connector-link add

---
sidebar_label: ddn connector-link add
sidebar_position: 43
description: Add a new DataConnectorLink to a Subgraph using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector-link add
---

# DDN CLI: ddn connector-link add

Add a new DataConnectorLink to a Subgraph.

## Synopsis

Add a new DataConnectorLink to a Subgraph

```bash
ddn connector-link add <connector-link-name> [flags]
```

## Examples

```bash
# Add a DataConnectorLink to the Subgraph "app"
 ddn connector-link add mydb --subgraph ./app/subgraph.yaml

# Add a DataConnectorLink to the Subgraph "app" and configure its connector URL as the Connector's local Docker service URL
 ddn connector-link add mydb --subgraph app/subgraph.yaml --configure-host http://local.hasura.dev:<port>
```

## Options

```sass
    --ci                                 Disables the use of context
    --configure-connector-token string   Token used to authenticate requests to the Connector
    --configure-host string              Read and Write URL of the Connector
-c, --context string                     Name of the context to use. (default <current_context>)
-h, --help                               help for add
    --subgraph string                    Path to Subgraph config file
    --target-env-file string             Subgraph env file to write the connector URLs to
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector-link_add-resources.mdx ---
# ddn connector-link add-resources

---
sidebar_label: ddn connector-link add-resources
sidebar_position: 44
description: Add all models, commands and relationships from a DataConnectorLink's schema using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector-link add-resources
---

# DDN CLI: ddn connector-link add-resources

Add all models, commands and relationships from a DataConnectorLink's schema.

## Synopsis

Add all models, commands and relationships from a DataConnectorLink's schema

```bash
ddn connector-link add-resources <connector-link-name> [flags]
```

## Examples

```bash
# Add all models, commands and relationships from the schema of DataConnectorLink 'mydb' for Subgraph config 'app'
 ddn connector-link add-resources mydb --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for add-resources
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector-link_remove.mdx ---
# ddn connector-link remove

---
sidebar_label: ddn connector-link remove
sidebar_position: 45
description: Remove a DataConnectorLink from a Subgraph using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector-link remove
---

# DDN CLI: ddn connector-link remove

Remove a DataConnectorLink from a Subgraph.

## Synopsis

Remove a DataConnectorLink from a Subgraph

```bash
ddn connector-link remove <connector-link-name> [flags]
```

**Alias:** rm

## Examples

```bash
# Remove a DataConnectorLink from the Subgraph "app"
 ddn connector-link remove mydb --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                            Disables the use of context
-c, --context string                Name of the context to use. (default <current_context>)
-h, --help                          help for remove
    --subgraph string               Path to Subgraph config file
    --target-env-file stringArray   Env file to remove ConnectorLink environment variables from
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector-link_show.mdx ---
# ddn connector-link show

---
sidebar_label: ddn connector-link show
sidebar_position: 46
description: Show DataConnectorLink details using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector-link show
---

# DDN CLI: ddn connector-link show

Show DataConnectorLink details.

## Synopsis

Show DataConnectorLink details

```bash
ddn connector-link show <connector-link-name> [flags]
```

## Examples

```bash
# Show DataConnectorLink details for `mydb`
 ddn connector-link show mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_connector-link_update.mdx ---
# ddn connector-link update

---
sidebar_label: ddn connector-link update
sidebar_position: 47
description: Fetch NDC details from the Connector and update the DataConnectorLink using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn connector-link update
---

# DDN CLI: ddn connector-link update

Fetch NDC details from the Connector and update the DataConnectorLink.

## Synopsis

Fetch NDC details from the Connector and update the DataConnectorLink

```bash
ddn connector-link update <connector-link-name> [flags]
```

## Examples

```bash
# Update the schema of a DataConnectorLink 'mydb' for Subgraph config 'app'
 ddn connector-link update mydb --subgraph ./app/subgraph.yaml

# Update the schema of a DataConnectorLink 'mydb' and add all Models, Commands and Relationships to the metadata for Subgraph config 'app'
 ddn connector-link update mydb --add-all-resources --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --add-all-resources      Add all Models, Commands and Relationships from the updated DataConnectorLink to the local metadata
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for update
    --subgraph string        Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn connector-link](/reference/cli/commands/ddn_connector-link) - Perform DataConnectorLink related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_console.mdx ---
# ddn console

---
sidebar_label: ddn console
sidebar_position: 48
description: Open the DDN console using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn console
---

# DDN CLI: ddn console

Open the DDN console.

## Synopsis

Open the DDN console

```bash
ddn console [flags]
```

## Examples

```bash
# Open the console for the DDN project set in the context
 ddn console

# Open the local dev console
 ddn console --local

# Open the local dev console with a specific engine url
 ddn console --url http://localhost:8080

# Open the console for a specific DDN project
 ddn console --project my-project-123

# Open the console for a specific SupergraphBuild
 ddn console --project my-project-123 --build-version build-version-123
```

## Options

```sass
    --build-version string   SupergraphBuild version
-h, --help                   help for console
    --local                  Open the local dev console
-p, --project string         DDN Project name
    --url string             Local engine url
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context.mdx ---
# ddn context

---
sidebar_label: ddn context
sidebar_position: 49
description: Perform context operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context
---

# DDN CLI: ddn context

Perform context operations.

## Synopsis

Manage default value of keys to be used in DDN CLI commands

## Available operations

- [ddn context create-context](/reference/cli/commands/ddn_context_create-context) - Create a new context
- [ddn context get](/reference/cli/commands/ddn_context_get) - Get the value of a key in the context
- [ddn context get-context](/reference/cli/commands/ddn_context_get-context) - List contexts or get details of a specific context
- [ddn context get-current-context](/reference/cli/commands/ddn_context_get-current-context) - Get name and contents of current context
- [ddn context set](/reference/cli/commands/ddn_context_set) - Set the value of a key in the context
- [ddn context set-current-context](/reference/cli/commands/ddn_context_set-current-context) - Set the current context
- [ddn context unset](/reference/cli/commands/ddn_context_unset) - Unset the value of a key in the context

## Options

```sass
-h, --help   help for context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_create-context.mdx ---
# ddn context create-context

---
sidebar_label: ddn context create-context
sidebar_position: 50
description: Create a new context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context create-context
---

# DDN CLI: ddn context create-context

Create a new context.

## Synopsis

Create a new context

```bash
ddn context create-context <name> [flags]
```

## Examples

```bash
# Create a new context called staging
 ddn context create-context staging
```

## Options

```sass
-h, --help   help for create-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_get.mdx ---
# ddn context get

---
sidebar_label: ddn context get
sidebar_position: 51
description: Get the value of a key in the context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context get
---

# DDN CLI: ddn context get

Get the value of a key in the context.

## Synopsis

Get the value of a key in the context

```bash
ddn context get <key> (Allowed keys: project, supergraph, subgraph, localEnvFile, cloudEnvFile, selfHostedDataPlane, noBuildConnectors) [flags]
```

## Examples

```bash
# Get the Project name set in the context
 ddn context get project

# Get the Supergraph config file path set in the context
 ddn context get supergraph
```

## Options

```sass
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_get-context.mdx ---
# ddn context get-context

---
sidebar_label: ddn context get-context
sidebar_position: 52
description: List contexts or get details of a specific context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context get-context
---

# DDN CLI: ddn context get-context

List contexts or get details of a specific context.

## Synopsis

List contexts or get details of a specific context

```bash
ddn context get-context [name] [flags]
```

## Examples

```bash
# Get list of contexts
 ddn context get-context

# Get details of context 'default'
 ddn context get-context default
```

## Options

```sass
-h, --help   help for get-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_get-current-context.mdx ---
# ddn context get-current-context

---
sidebar_label: ddn context get-current-context
sidebar_position: 53
description: Get name and contents of current context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context get-current-context
---

# DDN CLI: ddn context get-current-context

Get name and contents of current context.

## Synopsis

Get name and contents of current context

```bash
ddn context get-current-context [flags]
```

## Examples

```bash
# Get name and contents of current context
 ddn context get-current-context
```

## Options

```sass
-h, --help   help for get-current-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_set.mdx ---
# ddn context set

---
sidebar_label: ddn context set
sidebar_position: 54
description: Set the value of a key in the context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context set
---

# DDN CLI: ddn context set

Set the value of a key in the context.

## Synopsis

Set default value of keys to be used in DDN CLI commands

```bash
ddn context set <key> <value> (Allowed keys: project, supergraph, subgraph, localEnvFile, cloudEnvFile, selfHostedDataPlane, noBuildConnectors) [flags]
```

## Examples

```bash
# Set the Project name in the context
 ddn context set project foo-bar-1234

# Set the local Supergraph config file path in the context
 ddn context set supergraph ./supergraph.local.yaml

# Set the Subgraph config file path in the context
 ddn context set subgraph ./app/subgraph.yaml
```

## Options

```sass
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for set
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_set-current-context.mdx ---
# ddn context set-current-context

---
sidebar_label: ddn context set-current-context
sidebar_position: 55
description: Set the current context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context set-current-context
---

# DDN CLI: ddn context set-current-context

Set the current context.

## Synopsis

Set the current context

```bash
ddn context set-current-context <name> [flags]
```

## Examples

```bash
# Set current context to 'default'
 ddn context set-current-context default
```

## Options

```sass
-h, --help   help for set-current-context
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_context_unset.mdx ---
# ddn context unset

---
sidebar_label: ddn context unset
sidebar_position: 56
description: Unset the value of a key in the context using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn context unset
---

# DDN CLI: ddn context unset

Unset the value of a key in the context.

## Synopsis

Unset the value of a key in the context

```bash
ddn context unset <key> (Allowed keys: project, supergraph, subgraph, localEnvFile, cloudEnvFile, selfHostedDataPlane, noBuildConnectors) [flags]
```

## Examples

```bash
# Unset the Project name set in the context
 ddn context unset project

# Unset the Supergraph config file path set in the context
 ddn context unset supergraph
```

## Options

```sass
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for unset
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn context](/reference/cli/commands/ddn_context) - Perform context operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_doctor.mdx ---
# ddn doctor

---
sidebar_label: ddn doctor
sidebar_position: 58
description: Check if the dependencies of DDN CLI are present using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn doctor
---

# DDN CLI: ddn doctor

Check if the dependencies of DDN CLI are present.

## Synopsis

Check if the dependencies (Docker and Docker Compose) of DDN CLI are installed, are of the required version and if they are running.

```bash
ddn doctor [flags]
```

## Options

```sass
-h, --help   help for doctor
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_model.mdx ---
# ddn model

---
sidebar_label: ddn model
sidebar_position: 62
description: Perform Model related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn model
---

# DDN CLI: ddn model

Perform Model related operations.

## Synopsis

Perform Model related operations

**Alias:** models

## Available operations

- [ddn model add](/reference/cli/commands/ddn_model_add) - Add new Models to the local metadata
- [ddn model list](/reference/cli/commands/ddn_model_list) - Lists details about the collections of DataConnectorLink, and their corresponding Models
- [ddn model remove](/reference/cli/commands/ddn_model_remove) - Removes Models (and related metadata) in the local metadata
- [ddn model show](/reference/cli/commands/ddn_model_show) - Show diff between the model and its corresponding ndc collection
- [ddn model update](/reference/cli/commands/ddn_model_update) - Update Models in the local metadata

## Options

```sass
-h, --help   help for model
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_model_add.mdx ---
# ddn model add

---
sidebar_label: ddn model add
sidebar_position: 63
description: Add new Models to the local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn model add
---

# DDN CLI: ddn model add

Add new Models to the local metadata.

## Synopsis

Add new Models to the local metadata

```bash
ddn model add <connector-link-name> <collection-name> [flags]
```

## Examples

```bash
# Add all Models for DataConnectorLink "mydb" in the subgraph set in the context
 ddn model add mydb "*"

# Add a new Model "Album" from the collection "Album" in the DataConnectorLink "mydb" in Subgraph "app"
 ddn model add mydb Album --subgraph ./app/subgraph.yaml

# Add all the Models from the collections in the DataConnectorLink "mydb" in Subgraph "app"
 ddn model add mydb "*" --subgraph ./app/subgraph.yaml

# Add Models filtered by glob pattern from the collections in the DataConnectorLink "mydb" in Subgraph "app"
 ddn model add mydb "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for add
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_model_list.mdx ---
# ddn model list

---
sidebar_label: ddn model list
sidebar_position: 64
description: Lists details about the collections of DataConnectorLink, and their corresponding Models using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn model list
---

# DDN CLI: ddn model list

Lists details about the collections of DataConnectorLink, and their corresponding Models.

## Synopsis

Lists details about the collections of DataConnectorLink, and their corresponding Models

```bash
ddn model list <connector-link-name> [flags]
```

## Examples

```bash
# List details about the collections of DataConnectorLink `mydb`, and their corresponding Models
 ddn model list mydb
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for list
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_model_remove.mdx ---
# ddn model remove

---
sidebar_label: ddn model remove
sidebar_position: 65
description: Removes Models (and related metadata) in the local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn model remove
---

# DDN CLI: ddn model remove

Removes Models (and related metadata) in the local metadata.

## Synopsis

Removes Models (and related metadata) in the local metadata

```bash
ddn model remove <model-name> [flags]
```

## Examples

```bash
# Remove all Models using the subgraph set in current context
 ddn model remove "*"

# Remove the Model "Album" in the "app" Subgraph
 ddn model remove Album --subgraph ./app/subgraph.yaml

# Remove all the Models in the Subgraph "app"
 ddn model remove "*" --subgraph ./app/subgraph.yaml

# Remove Models filtered by glob pattern in the Subgraph "app"
 ddn model remove "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for remove
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_model_show.mdx ---
# ddn model show

---
sidebar_label: ddn model show
sidebar_position: 66
description: Show diff between the model and its corresponding ndc collection using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn model show
---

# DDN CLI: ddn model show

Show diff between the model and its corresponding ndc collection.

## Synopsis

Show diff between the model and its corresponding ndc collection

```bash
ddn model show <model-name> [flags]
```

## Examples

```bash
# Show diff between the 'Carts' model and its corresponding ndc collection
 ddn model show Carts

# Show diff between models and their corresponding ndc collections for all models matching glob pattern 'User*'
 ddn model show "User*"
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for show
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_model_update.mdx ---
# ddn model update

---
sidebar_label: ddn model update
sidebar_position: 67
description: Update Models in the local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn model update
---

# DDN CLI: ddn model update

Update Models in the local metadata.

## Synopsis

Update Models in the local metadata

```bash
ddn model update <model-name> [flags]
```

## Examples

```bash
# Update all Models using the subgraph set in current context
 ddn model update "*"

# Update the Model "Album" in the "app" Subgraph
 ddn model update Album --subgraph ./app/subgraph.yaml

# Update all the Models in the Subgraph "app"
 ddn model update "*" --subgraph ./app/subgraph.yaml

# Update Models filtered by glob pattern in the Subgraph "app"
 ddn model update "user*" --subgraph ./app/subgraph.yaml
```

## Options

```sass
    --ci                Disables the use of context
-c, --context string    Name of the context to use. (default <current_context>)
-h, --help              help for update
    --pattern string    Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string   Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn model](/reference/cli/commands/ddn_model) - Perform Model related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_plugins.mdx ---
# ddn plugins

---
sidebar_label: ddn plugins
sidebar_position: 68
description: Manage plugins for the CLI using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn plugins
---

# DDN CLI: ddn plugins

Manage plugins for the CLI.

## Synopsis

The functionality of the CLI can be extended by using plugins. For a list of all available plugins, run ``ddn plugins list``, or visit this repository: https://github.com/hasura/cli-plugins-index.

If you're interested in contributing, please open a PR against this repo.

**Alias:** plugin

## Available operations

- [ddn plugins install](/reference/cli/commands/ddn_plugins_install) - Install a plugin from the index
- [ddn plugins list](/reference/cli/commands/ddn_plugins_list) - List all available plugins with index, versions and installation status
- [ddn plugins uninstall](/reference/cli/commands/ddn_plugins_uninstall) - Uninstall a plugin
- [ddn plugins upgrade](/reference/cli/commands/ddn_plugins_upgrade) - Upgrade a plugin to a newer version

## Options

```sass
-h, --help   help for plugins
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_plugins_install.mdx ---
# ddn plugins install

---
sidebar_label: ddn plugins install
sidebar_position: 69
description: Install a plugin from the index using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn plugins install
---

# DDN CLI: ddn plugins install

Install a plugin from the index.

## Synopsis

To install plugins that extend the functionality of the DDN CLI, you can use the install command. This command will install the plugin from the index and add it to your configuration file.

```bash
ddn plugins install <name> [flags]
```

**Alias:** add

## Examples

```bash
# Install a plugin named "ndc-postgres"
 ddn plugins install ndc-postgres
```

## Options

```sass
-h, --help             help for install
    --version string   Version to be installed
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_plugins_list.mdx ---
# ddn plugins list

---
sidebar_label: ddn plugins list
sidebar_position: 70
description: List all available plugins with index, versions and installation status using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn plugins list
---

# DDN CLI: ddn plugins list

List all available plugins with index, versions and installation status.

## Synopsis

Run the plugins list command to see a list of all the available plugins which extend the functionality of the CLI, their versions and installation status.

```bash
ddn plugins list [flags]
```

**Alias:** ls

## Examples

```bash
# List all plugins
 ddn plugins list

# List all plugins without updating the plugin index local cache
 ddn plugins list --dont-update-index
```

## Options

```sass
    --dont-update-index   Don't update the plugin index local cache, only show the list
-h, --help                help for list
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_plugins_uninstall.mdx ---
# ddn plugins uninstall

---
sidebar_label: ddn plugins uninstall
sidebar_position: 71
description: Uninstall a plugin using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn plugins uninstall
---

# DDN CLI: ddn plugins uninstall

Uninstall a plugin.

## Synopsis

To uninstall a plugin, run the uninstall command with the name of the plugin as an argument. If unsure of the plugin's name, you can run the `ddn plugins list` command to see a list of all the available plugins.

```bash
ddn plugins uninstall <plugin-name> [flags]
```

**Alias:** remove

## Examples

```bash
# Uninstall a plugin named "ndc-postgres"
 ddn plugins uninstall ndc-postgres
```

## Options

```sass
-h, --help   help for uninstall
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_plugins_upgrade.mdx ---
# ddn plugins upgrade

---
sidebar_label: ddn plugins upgrade
sidebar_position: 72
description: Upgrade a plugin to a newer version using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn plugins upgrade
---

# DDN CLI: ddn plugins upgrade

Upgrade a plugin to a newer version.

## Synopsis

To upgrade a plugin, run the upgrade command with the name of the plugin as an argument. If unsure of the plugin's name, you can run the `ddn plugins list` command to see a list of all the available plugins.

```bash
ddn plugins upgrade <plugin-name> [flags]
```

**Alias:** update

## Examples

```bash
# Upgrade a plugin "ndc-postgres" to a newer version
 ddn plugins upgrade ndc-postgres
```

## Options

```sass
-h, --help             help for upgrade
    --version string   Version to be upgraded
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn plugins](/reference/cli/commands/ddn_plugins) - Manage plugins for the CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project.mdx ---
# ddn project

---
sidebar_label: ddn project
sidebar_position: 73
description: Manage Hasura DDN Project using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project
---

# DDN CLI: ddn project

Manage Hasura DDN Project.

## Synopsis

Manage Hasura DDN Project

## Available operations

- [ddn project create](/reference/cli/commands/ddn_project_create) - Create a new Project on Hasura DDN
- [ddn project delete](/reference/cli/commands/ddn_project_delete) - Delete a Project on Hasura DDN
- [ddn project get](/reference/cli/commands/ddn_project_get) - List Hasura DDN Projects or get details of a specific one
- [ddn project init](/reference/cli/commands/ddn_project_init) - Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary
- [ddn project set-api-access-mode](/reference/cli/commands/ddn_project_set-api-access-mode) - Set the API access mode for a Project on Hasura DDN
- [ddn project set-self-hosted-engine-url](/reference/cli/commands/ddn_project_set-self-hosted-engine-url) - Set the engine's URL for a project in a self hosted data plane.
- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project

## Options

```sass
-h, --help   help for project
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_create.mdx ---
# ddn project create

---
sidebar_label: ddn project create
sidebar_position: 74
description: Create a new Project on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project create
---

# DDN CLI: ddn project create

Create a new Project on Hasura DDN.

## Synopsis

Create a new Project on Hasura DDN

```bash
ddn project create [project-name] [flags]
```

## Examples

```bash
# Create a Project on Hasura DDN with auto-generated name
 ddn project create

# Create a Project with name "test-project" on Hasura DDN
 ddn project create test-project
```

## Options

```sass
    --data-plane-id uuid   The DDN instance where the Project should be hosted
-h, --help                 help for create
    --plan string          DDN Project plan
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_delete.mdx ---
# ddn project delete

---
sidebar_label: ddn project delete
sidebar_position: 75
description: Delete a Project on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project delete
---

# DDN CLI: ddn project delete

Delete a Project on Hasura DDN.

## Synopsis

Delete a Project on Hasura DDN

```bash
ddn project delete <project-name> [flags]
```

## Examples

```bash
# Delete a Project "pet-lion-2649" on Hasura DDN
 ddn project delete pet-lion-2649
```

## Options

```sass
-f, --force   Delete project without confirmation
-h, --help    help for delete
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_get.mdx ---
# ddn project get

---
sidebar_label: ddn project get
sidebar_position: 76
description: List Hasura DDN Projects or get details of a specific one using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project get
---

# DDN CLI: ddn project get

List Hasura DDN Projects or get details of a specific one.

## Synopsis

List Hasura DDN Projects or get details of a specific one

```bash
ddn project get [project-name] [flags]
```

## Examples

```bash
# List all your Projects on Hasura DDN.
 ddn project get

# Get details of a Project "pet-lion-2649" on Hasura DDN.
 ddn project get pet-lion-2649
```

## Options

```sass
-h, --help   help for get
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_init.mdx ---
# ddn project init

---
sidebar_label: ddn project init
sidebar_position: 77
description: Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project init
---

# DDN CLI: ddn project init

Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary.

## Synopsis

Configure the local directory to use a Hasura DDN project, creating a DDN project and subgraphs as necessary

```bash
ddn project init [project-name] [flags]
```

## Examples

```bash
# Initialize a new Hasura DDN project (with an auto-generated name) with subgraphs based on your local directory
 ddn project init

# Configure the local directory to use an existing Hasura DDN project creating subgraphs on the DDN project as necessary
 ddn project init --with-project myproject
```

## Options

```sass
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
    --data-plane-id uuid     The DDN instance where the Project should be hosted
    --env-file-name string   Env file to be created and added to context as cloudEnvFile (default ".env.cloud")
    --from-env-file string   Env file to initialize the cloudEnvFile from
-h, --help                   help for init
    --plan string            DDN Project plan
    --supergraph string      Path to Supergraph config file
    --with-project string    Use an existing project instead of creating a new one
    --with-promptql          Initialize with PromptQL support (alpha)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_set-api-access-mode.mdx ---
# ddn project set-api-access-mode

---
sidebar_label: ddn project set-api-access-mode
sidebar_position: 78
description: Set the API access mode for a Project on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project set-api-access-mode
---

# DDN CLI: ddn project set-api-access-mode

Set the API access mode for a Project on Hasura DDN.

## Synopsis

Set the API access mode for a Project on Hasura DDN

```bash
ddn project set-api-access-mode <public|private> [flags]
```

## Examples

```bash
# Set the API access mode for the DDN project set in the context as private
 ddn project set-api-access-mode private

# Set the API access mode for the DDN project set in the context as public
 ddn project set-api-access-mode public

# Set the API access mode for the DDN project 'my-project-123' as private
 ddn project set-api-access-mode private --project my-project-123

# Set the API access mode for the DDN project 'my-project-123' as public
 ddn project set-api-access-mode public --project my-project-123
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for set-api-access-mode
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_set-self-hosted-engine-url.mdx ---
# ddn project set-self-hosted-engine-url

---
sidebar_label: ddn project set-self-hosted-engine-url
sidebar_position: 79
description: Set the engine's URL for a project in a self hosted data plane. using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project set-self-hosted-engine-url
---

# DDN CLI: ddn project set-self-hosted-engine-url

Set the engine's URL for a project in a self hosted data plane..

## Synopsis

Set the engine's URL for a project in a self hosted data plane.

```bash
ddn project set-self-hosted-engine-url <url> [flags]
```

## Examples

```bash
# Set project URL to "example.com:3000" for project "pet-lion-2649"
 ddn project set-self-hosted-engine-url example.com:3000 --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for set-self-hosted-engine-url
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_version.mdx ---
# ddn version

---
sidebar_label: ddn version
sidebar_position: 79
description: Prints the CLI version using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn version
---

# DDN CLI: ddn version

Prints the CLI version.

## Synopsis

Use this command to print the current version of the CLI. This command can also be used to check if a new version of the
CLI is available.

```bash
ddn version [flags]
```

## Options

```sass
-h, --help   help for version
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_subgraph.mdx ---
# ddn project subgraph

---
sidebar_label: ddn project subgraph
sidebar_position: 80
description: Manage Subgraphs in a Hasura DDN Project using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project subgraph
---

# DDN CLI: ddn project subgraph

Manage Subgraphs in a Hasura DDN Project.

## Synopsis

Manage Subgraphs in a Hasura DDN Project

## Available operations

- [ddn project subgraph create](/reference/cli/commands/ddn_project_subgraph_create) - Create a new Subgraph in a Hasura DDN Project
- [ddn project subgraph delete](/reference/cli/commands/ddn_project_subgraph_delete) - Delete a Subgraph from a Project
- [ddn project subgraph get](/reference/cli/commands/ddn_project_subgraph_get) - List Subgraphs for a Hasura DDN Project or get details of a specific one

## Options

```sass
-h, --help   help for subgraph
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project](/reference/cli/commands/ddn_project) - Manage Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_subgraph_create.mdx ---
# ddn project subgraph create

---
sidebar_label: ddn project subgraph create
sidebar_position: 81
description: Create a new Subgraph in a Hasura DDN Project using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project subgraph create
---

# DDN CLI: ddn project subgraph create

Create a new Subgraph in a Hasura DDN Project.

## Synopsis

Create a new Subgraph in a Hasura DDN Project

```bash
ddn project subgraph create <subgraph-name> [flags]
```

## Examples

```bash
# Create a new Subgraph "app" in a Project
 ddn project subgraph create app --project pet-lion-2649

# Create a new Subgraph "app" in a Project with a description
 ddn project subgraph create app --project pet-lion-2649 --description "application management APIs"
```

## Options

```sass
    --ci                   Disables the use of context
-c, --context string       Name of the context to use. (default <current_context>)
-d, --description string   (Optional) description of the subgraph
-h, --help                 help for create
-p, --project string       DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_subgraph_delete.mdx ---
# ddn project subgraph delete

---
sidebar_label: ddn project subgraph delete
sidebar_position: 82
description: Delete a Subgraph from a Project using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project subgraph delete
---

# DDN CLI: ddn project subgraph delete

Delete a Subgraph from a Project.

## Synopsis

Delete a Subgraph from a Project

```bash
ddn project subgraph delete <subgraph-name> [flags]
```

## Examples

```bash
# Delete a Subgraph 'app' from a Project
 ddn project subgraph delete app --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for delete
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_project_subgraph_get.mdx ---
# ddn project subgraph get

---
sidebar_label: ddn project subgraph get
sidebar_position: 83
description: List Subgraphs for a Hasura DDN Project or get details of a specific one using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn project subgraph get
---

# DDN CLI: ddn project subgraph get

List Subgraphs for a Hasura DDN Project or get details of a specific one.

## Synopsis

List Subgraphs for a Hasura DDN Project or get details of a specific one

```bash
ddn project subgraph get [subgraph-name] [flags]
```

## Examples

```bash
# List all Subgraphs for a Project
 ddn project subgraph get --project pet-lion-2649

# View details of a Subgraph "app" in a Project
 ddn project subgraph get app --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn project subgraph](/reference/cli/commands/ddn_project_subgraph) - Manage Subgraphs in a Hasura DDN Project



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_relationship.mdx ---
# ddn relationship

---
sidebar_label: ddn relationship
sidebar_position: 84
description: Perform Relationship related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn relationship
---

# DDN CLI: ddn relationship

Perform Relationship related operations.

## Synopsis

Perform Relationship related operations

**Alias:** relationships

## Available operations

- [ddn relationship add](/reference/cli/commands/ddn_relationship_add) - Adds Relationships from foreign keys on collection-name or targeting collection-name
- [ddn relationship list](/reference/cli/commands/ddn_relationship_list) - Lists Relationships for a given DataConnectorLink

## Options

```sass
-h, --help   help for relationship
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_relationship_add.mdx ---
# ddn relationship add

---
sidebar_label: ddn relationship add
sidebar_position: 85
description: Adds Relationships from foreign keys on collection-name or targeting collection-name using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn relationship add
---

# DDN CLI: ddn relationship add

Adds Relationships from foreign keys on collection-name or targeting collection-name.

## Synopsis

Adds Relationships from foreign keys on collection-name or targeting collection-name

```bash
ddn relationship add <connector-link-name> <collection-name> [flags]
```

## Examples

```bash
# Add all Relationships for DataConnectorLink "mydb" in the subgraph set in the context
 ddn relationship add mydb "*"

# Add Relationships for the collection "Album" in the DataConnectorLink "mydb" in the Subgraph "app"
 ddn relationship add mydb Album --subgraph ./app/subgraph.yaml

# Add Relationships for collections that match the glob pattern "sales_*"
 ddn relationship add mydb "sales_*"

# Add Relationships for the collection "Album" defined by the foreign key "artists_album_id_fkey" on the collection "Artist"
 ddn relationship add mydb Album --fk-collection Artist --fk-name artists_album_id_fkey
```

## Options

```sass
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
    --fk-collection string   Only consider foreign keys defined on this collection
    --fk-name string         Only consider foreign keys with this name
-h, --help                   help for add
    --pattern string         Pattern to detect targets. Can be 'glob' or 'literal'. (default "glob")
    --subgraph string        Path to Subgraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn relationship](/reference/cli/commands/ddn_relationship) - Perform Relationship related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_relationship_list.mdx ---
# ddn relationship list

---
sidebar_label: ddn relationship list
sidebar_position: 86
description: Lists Relationships for a given DataConnectorLink using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn relationship list
---

# DDN CLI: ddn relationship list

Lists Relationships for a given DataConnectorLink.

## Synopsis

Lists Relationships for a given DataConnectorLink

```bash
ddn relationship list [flags]
```

## Examples

```bash
# List all Relationships for the supergraph set in the context
 ddn relationship list

# List all Relationships for a specific supergraph
 ddn relationship list --supergraph ./supergraph.cloud.yaml

# List all Relationships for a specific subgraph
 ddn relationship list --subgraph app/subgraph.yaml
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
-h, --help                help for list
    --subgraph string     Path to Subgraph config file
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn relationship](/reference/cli/commands/ddn_relationship) - Perform Relationship related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_run.mdx ---
# ddn run

---
sidebar_label: ddn run
sidebar_position: 87
description: Run specific script from project's context config using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn run
---

# DDN CLI: ddn run

Run specific script from project's context config.

## Synopsis

Run custom scripts defined in project's context config (typically at `<project-root>/.hasura/context.yaml`)

```bash
ddn run <script-name> [flags] [-- <args>]
```

## Examples

```bash
# Run script `docker-start` defined in project's context config
 ddn run docker-start

# Run script `docker-start` defined in project's context config in detached mode
 ddn run docker-start -- -d
```

## Options

```sass
-h, --help   help for run
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph.mdx ---
# ddn subgraph

---
sidebar_label: ddn subgraph
sidebar_position: 88
description: Perform Subgraph related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph
---

# DDN CLI: ddn subgraph

Perform Subgraph related operations.

## Synopsis

Perform Subgraph related operations

**Alias:** subgraphs

## Available operations

- [ddn subgraph add](/reference/cli/commands/ddn_subgraph_add) - Add a Subgraph config file to Supergraph config file
- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild related operations
- [ddn subgraph init](/reference/cli/commands/ddn_subgraph_init) - Initialize a new Subgraph in local metadata

## Options

```sass
-h, --help   help for subgraph
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph_add.mdx ---
# ddn subgraph add

---
sidebar_label: ddn subgraph add
sidebar_position: 89
description: Add a Subgraph config file to Supergraph config file using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph add
---

# DDN CLI: ddn subgraph add

Add a Subgraph config file to Supergraph config file.

## Synopsis

Add a Subgraph config file to Supergraph config file

```bash
ddn subgraph add --subgraph <path-to-subgraph-config-file> --target-supergraph <path-to-supergraph-config-file> [flags]
```

## Examples

```bash
# Add a Subgraph config file "./app/subgraph.yaml" to the Supergraph config file "./supergraph.yaml"
 ddn subgraph add --subgraph ./app/subgraph.yaml --target-supergraph ./supergraph.yaml

# Add a Subgraph config file "./app/subgraph.yaml" to multiple Supergraph config files"
 ddn subgraph add --subgraph ./app/subgraph.yaml --target-supergraph ./supergraph.stg.yaml --target-supergraph ./supergraph.prod.yaml
```

## Options

```sass
-h, --help                            help for add
    --subgraph string                 Path to Subgraph config file (required)
    --target-supergraph stringArray   Supergraph config file to add the Subgraph. Can be repeated to provide multiple Supergraph config files (required)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph_build.mdx ---
# ddn subgraph build

---
sidebar_label: ddn subgraph build
sidebar_position: 90
description: Perform SubgraphBuild related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph build
---

# DDN CLI: ddn subgraph build

Perform SubgraphBuild related operations.

## Synopsis

Perform SubgraphBuild related operations

## Available operations

- [ddn subgraph build apply](/reference/cli/commands/ddn_subgraph_build_apply) - Apply a Subgraph build on Hasura DDN
- [ddn subgraph build create](/reference/cli/commands/ddn_subgraph_build_create) - Create a SubgraphBuild on Hasura DDN
- [ddn subgraph build get](/reference/cli/commands/ddn_subgraph_build_get) - List SubgraphBuilds or get details of a specific one

## Options

```sass
-h, --help   help for build
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph_build_apply.mdx ---
# ddn subgraph build apply

---
sidebar_label: ddn subgraph build apply
sidebar_position: 91
description: Apply a Subgraph build on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph build apply
---

# DDN CLI: ddn subgraph build apply

Apply a Subgraph build on Hasura DDN.

## Synopsis

Apply a Subgraph build on Hasura DDN

```bash
ddn subgraph build apply <subgraph-build-version> [flags]
```

## Examples

```bash
# Apply a Subgraph build to Project "pet-lion-2649"
 ddn subgraph build apply <subgraph-build-version> --project pet-lion-2649
```

## Options

```sass
    --ci                       Disables the use of context
-c, --context string           Name of the context to use. (default <current_context>)
-h, --help                     help for apply
-p, --project string           DDN Project name
    --self-hosted-data-plane   Is the data plane self hosted?
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph_build_create.mdx ---
# ddn subgraph build create

---
sidebar_label: ddn subgraph build create
sidebar_position: 92
description: Create a SubgraphBuild on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph build create
---

# DDN CLI: ddn subgraph build create

Create a SubgraphBuild on Hasura DDN.

## Synopsis

Create a SubgraphBuild on Hasura DDN

```bash
ddn subgraph build create [flags]
```

## Examples

```bash
# Build the Subgraph from a config file for Project with some environment variables
 ddn subgraph build create --subgraph ./app/subgraph.yaml --project pet-lion-2649 --env key1=val1
```

## Options

```sass
    --ci                             Disables the use of context
-c, --context string                 Name of the context to use. (default <current_context>)
-d, --description string             (Optional) description of the build
-e, --env stringArray                Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray           Path to .env file. Can be repeated to provide multiple env files
-h, --help                           help for create
    --no-build-connectors            Do not recursively build all connectors in the subgraph and use their URLs for subgraph build. (default: false)
-p, --project string                 DDN Project name
    --self-hosted-data-plane         Is the data plane self hosted?
    --subgraph string                Path to Subgraph config file
    --target-env-file string         Env file to write the connector build URLs to.
    --update-connector-link-schema   Update DataConnectorLink schema with the NDC schema of the connectors built recursively. (default: false)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph_build_get.mdx ---
# ddn subgraph build get

---
sidebar_label: ddn subgraph build get
sidebar_position: 93
description: List SubgraphBuilds or get details of a specific one using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph build get
---

# DDN CLI: ddn subgraph build get

List SubgraphBuilds or get details of a specific one.

## Synopsis

List SubgraphBuilds or get details of a specific one

```bash
ddn subgraph build get [subgraph-build-version] [flags]
```

## Examples

```bash
# View details of a SubgraphBuild in the Project 'pet-lion-2649'
 ddn subgraph build get <subgraph-build-version> --project pet-lion-2649

# List all SubgraphBuilds of a Project 'pet-lion-2649'
 ddn subgraph build get --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph build](/reference/cli/commands/ddn_subgraph_build) - Perform SubgraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_subgraph_init.mdx ---
# ddn subgraph init

---
sidebar_label: ddn subgraph init
sidebar_position: 94
description: Initialize a new Subgraph in local metadata using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn subgraph init
---

# DDN CLI: ddn subgraph init

Initialize a new Subgraph in local metadata.

## Synopsis

Initialize a new Subgraph in local metadata

```bash
ddn subgraph init <subgraph-name> --dir <dir-name> [flags]
```

## Examples

```bash
# Initialize a Subgraph "app"
 ddn subgraph init app

# Initialize a Subgraph "app" in the directory "./app" and add it to Supergraph config files "./supergraph.yaml" and "./supergraph.cloud.yaml"
 ddn subgraph init app --dir ./app --target-supergraph ./supergraph.yaml --target-supergraph ./supergraph.cloud.yaml
```

## Options

```sass
    --dir string                          Directory to initialize the Subgraph (Defaults to subgraph name)
    --graphql-root-field-prefix string    Prefix to use while generating GraphQL root fields
    --graphql-type-name-prefix string     Prefix to use while generating GraphQL type names
-h, --help                                help for init
    --subgraph-naming-convention string   Naming convention for the subgraph. Can be 'graphql' or 'none'. (default "graphql")
    --target-supergraph stringArray       Supergraph config file to add the Subgraph. Can be repeated to provide multiple Supergraph config files
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn subgraph](/reference/cli/commands/ddn_subgraph) - Perform Subgraph related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph.mdx ---
# ddn supergraph

---
sidebar_label: ddn supergraph
sidebar_position: 95
description: Perform Supergraph related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph
---

# DDN CLI: ddn supergraph

Perform Supergraph related operations.

## Synopsis

Perform Supergraph related operations

**Alias:** supergraphs

## Available operations

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations
- [ddn supergraph init](/reference/cli/commands/ddn_supergraph_init) - Initialize a new Supergraph project directory
- [ddn supergraph prune](/reference/cli/commands/ddn_supergraph_prune) - Removes unused metadata of the Supergraph

## Options

```sass
-h, --help   help for supergraph
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_update-cli.mdx ---
# ddn update-cli

---
sidebar_label: ddn update-cli
sidebar_position: 95
description: Update this CLI to the latest version or to a specific version using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn update-cli
---

# DDN CLI: ddn update-cli

Update this CLI to the latest version or to a specific version.

## Synopsis

You can use this command to update the CLI to the latest version or a specific version.

```bash
ddn update-cli [flags]
```

## Examples

```bash
# Update CLI to latest version:
 ddn update-cli

# Update CLI to a specific version (say v1.0.0):
 ddn update-cli --version v1.0.0

# To disable the auto-update check on the CLI, set
# "show_update_notification": false
# in ~/.ddn/config.yaml
```

## Options

```sass
-h, --help             help for update-cli
    --version string   A specific version to install
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build.mdx ---
# ddn supergraph build

---
sidebar_label: ddn supergraph build
sidebar_position: 96
description: Perform SupergraphBuild related operations using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build
---

# DDN CLI: ddn supergraph build

Perform SupergraphBuild related operations.

## Synopsis

Perform SupergraphBuild related operations

## Available operations

- [ddn supergraph build apply](/reference/cli/commands/ddn_supergraph_build_apply) - Apply a SupergraphBuild to its Project on Hasura DDN
- [ddn supergraph build create](/reference/cli/commands/ddn_supergraph_build_create) - Create a SupergraphBuild on Hasura DDN
- [ddn supergraph build delete](/reference/cli/commands/ddn_supergraph_build_delete) - Delete a SupergraphBuild from a Project
- [ddn supergraph build diff](/reference/cli/commands/ddn_supergraph_build_diff) - See changes made to the GraphQL schema from one build version to another.
- [ddn supergraph build get](/reference/cli/commands/ddn_supergraph_build_get) - List SupergraphBuilds or get details of a specific one
- [ddn supergraph build local](/reference/cli/commands/ddn_supergraph_build_local) - Build the Supergraph and generate assets to run the local Engine
- [ddn supergraph build set-self-hosted-engine-url](/reference/cli/commands/ddn_supergraph_build_set-self-hosted-engine-url) - Set the engine's URL for a build for a project in a self hosted data plane.

## Options

```sass
-h, --help   help for build
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_apply.mdx ---
# ddn supergraph build apply

---
sidebar_label: ddn supergraph build apply
sidebar_position: 97
description: Apply a SupergraphBuild to its Project on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build apply
---

# DDN CLI: ddn supergraph build apply

Apply a SupergraphBuild to its Project on Hasura DDN.

## Synopsis

Apply a SupergraphBuild to its Project on Hasura DDN

```bash
ddn supergraph build apply <supergraph-build-version> [flags]
```

## Examples

```bash
# Apply a SupergraphBuild to a Project "pet-lion-2649"
 ddn supergraph build apply <supergraph-build-version> --project pet-lion-2649
```

## Options

```sass
    --ci                       Disables the use of context
-c, --context string           Name of the context to use. (default <current_context>)
-h, --help                     help for apply
    --no-diff                  Do not do a GraphQL schema diff against the applied build
-p, --project string           DDN Project name
    --self-hosted-data-plane   Is the data plane self hosted?
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_completion.mdx ---
# ddn completion

---
sidebar_label: ddn completion
sidebar_position: 98
description: Generate autocompletion scripts for the DDN CLI.
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn completion
---

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the specified shell.

## Synopsis

Generate the autocompletion script for DDN for the specified shell.  
See each sub-command's help for details on how to use the generated script.

```bash
ddn completion [command] [flags]
```

## Available commands

- [bash](/reference/cli/commands/ddn_completion_bash): Generate the autocompletion script for bash
- [fish](/reference/cli/commands/ddn_completion_fish): Generate the autocompletion script for fish
- [powershell](/reference/cli/commands/ddn_completion_powershell): Generate the autocompletion script for PowerShell
- [zsh](/reference/cli/commands/ddn_completion_zsh): Generate the autocompletion script for zsh

## Options

```sass
-h, --help   help for completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn](/reference/cli/commands/ddn) - DDN Command Line Interface



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_create.mdx ---
# ddn supergraph build create

---
sidebar_label: ddn supergraph build create
sidebar_position: 98
description: Create a SupergraphBuild on Hasura DDN using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build create
---

# DDN CLI: ddn supergraph build create

Create a SupergraphBuild on Hasura DDN.

## Synopsis

Create a SupergraphBuild on Hasura DDN

```bash
ddn supergraph build create [flags]
```

## Examples

```bash
# Build the Connectors and the Supergraph
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --env-file .env.cloud

# Build the Supergraph without building the Connectors
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --no-build-connectors

# Build the Supergraph and update the link schema and target env file
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --update-connector-link-schema --target-env-file .env.cloud

# Build a composed Supergraph using Subgraphs with build versions and using the applied Supergraph Build as the base Supergraph (Advanced plan only)
 ddn supergraph build create --subgraph-version globals:c15b0b4031 --subgraph-version my_subgraph:c15b0b4031 --base-supergraph-on-applied

# Build a composed Supergraph using Subgraphs with build versions and base Supergraph version (Advanced plan only)
 ddn supergraph build create --subgraph-version globals:c15b0b4031 --subgraph-version my_subgraph:c15b0b4031 --base-supergraph-version c15b0b4871

# Build the Supergraph and also apply the build
 ddn supergraph build create --supergraph supergraph.yaml --project pet-lion-2649 --apply
```

## Options

```sass
    --apply                            Apply the build created after it is completed
    --base-supergraph-on-applied       Use the applied Supergraph as the base supergraph
    --base-supergraph-version string   Base Supergraph version for the compose build
    --ci                               Disables the use of context
-c, --context string                   Name of the context to use. (default <current_context>)
-d, --description string               (Optional) description of the build
-e, --env stringArray                  Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray             Path to .env file. Can be repeated to provide multiple env files
-h, --help                             help for create
    --no-build-connectors              Do not recursively build all connectors in all subgraphs and use their URLs for supergraph build. (default: false)
    --no-diff                          Do not do a GraphQL schema diff against the applied build
    --output-dir string                Path to the directory to output the build artifacts
-p, --project string                   DDN Project name
    --self-hosted-data-plane           Is the data plane self hosted?
    --subgraph-version stringArray     Subgraph(s) with build version to compose
    --supergraph string                Path to Supergraph config file
    --target-env-file string           Env file to write the connector build URLs to.
    --update-connector-link-schema     Update DataConnectorLink schema with the NDC schema of the connectors built recursively. (default: false)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_completion_bash.mdx ---
# ddn completion bash

---
sidebar_label: ddn completion bash
sidebar_position: 99
description: Generate autocompletion scripts for the DDN CLI for the bash shell.
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn completion bash
---

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the bash shell.

## Synopsis

Generate the autocompletion script for DDN for the bash shell.

To load completions in your current shell session:

```sass
        source <(ddn completion bash)
```

To load completions for every new session, execute once:

#### Linux:

```sass
        ddn completion bash > /etc/bash_completion.d/ddn
```

#### macOS:

```sass
        ddn completion bash > $(brew --prefix)/etc/bash_completion.d/ddn
```

You will need to start a new shell for this setup to take effect.

## Options

```sass
-h, --help   help for bash completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_delete.mdx ---
# ddn supergraph build delete

---
sidebar_label: ddn supergraph build delete
sidebar_position: 99
description: Delete a SupergraphBuild from a Project using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build delete
---

# DDN CLI: ddn supergraph build delete

Delete a SupergraphBuild from a Project.

## Synopsis

Delete a SupergraphBuild from a Project

```bash
ddn supergraph build delete <supergraph-build-version> [flags]
```

## Examples

```bash
# Delete a SupergraphBuild from a Project "pet-lion-2649"
 ddn supergraph build delete <supergraph-build-version> --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for delete
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_completion_fish.mdx ---
# ddn completion fish

---
sidebar_label: ddn completion fish
sidebar_position: 100
description: Generate autocompletion scripts for the DDN CLI for the fish shell.
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn completion fish
---

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the fish shell.

## Synopsis

Generate the autocompletion script for DDN for the fish shell.

To load completions in your current shell session:

```sass
        ddn completion fish | source
```

To load completions for every new session, execute once:

```sass
        ddn completion fish > ~/.config/fish/completions/ddn.fish
```

You will need to start a new shell for this setup to take effect.

## Options

```sass
-h, --help   help for fish completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_diff.mdx ---
# ddn supergraph build diff

---
sidebar_label: ddn supergraph build diff
sidebar_position: 100
description: See changes made to the GraphQL schema from one build version to another. using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build diff
---

# DDN CLI: ddn supergraph build diff

See changes made to the GraphQL schema from one build version to another..

## Synopsis

See changes made to the GraphQL schema from one build version to another.

```bash
ddn supergraph build diff <build-version-1> <build-version-2> [flags]
```

## Examples

```bash
# Compare changes made to the GraphQL schema from build version "qfrr5e5jyw" to "g6v6nh73h0" for Project "pet-lion-2649"
 ddn supergraph build diff qfrr5e5jyw g6v6nh73h0 --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for diff
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_completion_powershell.mdx ---
# ddn completion PowerShell

---
sidebar_label: ddn completion PowerShell
sidebar_position: 101
description: Generate autocompletion scripts for the DDN CLI for the PowerShell shell.
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn completion powershell
---

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the PowerShell shell.

## Synopsis

Generate the autocompletion script for DDN for the PowerShell shell.

To load completions in your current shell session:

```sass
         ddn completion powershell | Out-String | Invoke-Expression
```

To load completions for every new session, add the output of the above command to your powershell profile.

## Options

```sass
-h, --help   help for powershell completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_get.mdx ---
# ddn supergraph build get

---
sidebar_label: ddn supergraph build get
sidebar_position: 101
description: List SupergraphBuilds or get details of a specific one using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build get
---

# DDN CLI: ddn supergraph build get

List SupergraphBuilds or get details of a specific one.

## Synopsis

List SupergraphBuilds or get details of a specific one

```bash
ddn supergraph build get [supergraph-build-version] [flags]
```

## Examples

```bash
# View details of a SupergraphBuild in the Project "pet-lion-2649"
 ddn supergraph build get <supergraph-build-version> --project pet-lion-2649

# List all SupergraphBuilds of a Project "pet-lion-2649"
 ddn supergraph build get --project pet-lion-2649
```

## Options

```sass
    --ci               Disables the use of context
-c, --context string   Name of the context to use. (default <current_context>)
-h, --help             help for get
-p, --project string   DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_completion_zsh.mdx ---
# ddn completion zsh

---
sidebar_label: ddn completion zsh
sidebar_position: 102
description: Generate autocompletion scripts for the DDN CLI for the zsh shell.
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn completion zsh
---

# DDN CLI: ddn completion

Generate autocompletion scripts for the DDN CLI for the zsh shell.

## Synopsis

Generate the autocompletion script for DDN for the zsh shell.

If shell completion is not already enabled in your environment you will need to enable it. You can execute the following
once:

```sass
        echo "autoload -U compinit; compinit" >> ~/.zshrc
```

To load completions in your current shell session:

```sass
        source <(ddn completion zsh)
```

To load completions for every new session, execute once:

#### Linux:

```sass
         ddn completion zsh > "${fpath[1]}/_ddn"
```

#### macOS:

```sass
        ddn completion zsh > $(brew --prefix)/share/zsh/site-functions/_ddn
```

You will need to start a new shell for this setup to take effect.

## Options

```sass
-h, --help   help for zsh completion
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn completion](/reference/cli/commands/ddn_completion) - Generate autocompletion scripts for the DDN CLI



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_local.mdx ---
# ddn supergraph build local

---
sidebar_label: ddn supergraph build local
sidebar_position: 102
description: Build the Supergraph and generate assets to run the local Engine using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build local
---

# DDN CLI: ddn supergraph build local

Build the Supergraph and generate assets to run the local Engine.

## Synopsis

Build the Supergraph and generate assets to run the local Engine using the DDN hosted metadata build service.

Any relationships to subgraphs defined in other repositories will not be validated and will be ignored.

```bash
ddn supergraph build local [flags]
```

## Examples

```bash
# Build the Supergraph using the supergraph config and local env file from context and output it to default engine directory i.e. <project-root>/engine/build
 ddn supergraph build local

# Build the Supergraph using a specific supergraph config file and env file and output it to a specific directory
 ddn supergraph build local --output-dir <path-to-engine-directory> --supergraph supergraph.yaml --env-file .env
```

## Options

```sass
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
-e, --env stringArray        Environment variable, e.g. key=val. Can be repeated to provide multiple env vars
    --env-file stringArray   Path to .env file. Can be repeated to provide multiple env files
-h, --help                   help for local
    --output-dir string      Path to the engine directory to output the build artifacts. (defaults to `<project-root>/engine/build)
    --supergraph string      Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_build_set-self-hosted-engine-url.mdx ---
# ddn supergraph build set-self-hosted-engine-url

---
sidebar_label: ddn supergraph build set-self-hosted-engine-url
sidebar_position: 103
description: Set the engine's URL for a build for a project in a self hosted data plane. using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph build set-self-hosted-engine-url
---

# DDN CLI: ddn supergraph build set-self-hosted-engine-url

Set the engine's URL for a build for a project in a self hosted data plane..

## Synopsis

Set the engine's URL for a build for a project in a self hosted data plane.

```bash
ddn supergraph build set-self-hosted-engine-url <url> --build-version <build-version> [flags]
```

## Examples

```bash
# Set build URL to "example.com:3000" for project "pet-lion-2649"
 ddn supergraph build set-self-hosted-engine-url example.com:3000 --build-version <build-version> --project pet-lion-2649
```

## Options

```sass
    --build-version string   SupergraphBuild version (required)
    --ci                     Disables the use of context
-c, --context string         Name of the context to use. (default <current_context>)
-h, --help                   help for set-self-hosted-engine-url
-p, --project string         DDN Project name
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph build](/reference/cli/commands/ddn_supergraph_build) - Perform SupergraphBuild related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_init.mdx ---
# ddn supergraph init

---
sidebar_label: ddn supergraph init
sidebar_position: 104
description: Initialize a new Supergraph project directory using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph init
---

# DDN CLI: ddn supergraph init

Initialize a new Supergraph project directory.

## Synopsis

Initialize a new Supergraph project directory

```bash
ddn supergraph init <path-to-project-dir> [flags]
```

## Examples

```bash
# Initialize a new Supergraph project directory with a default subgraph 'app'
 ddn supergraph init <path-to-project-dir>

# Initialize a new Supergraph project directory with a subgraph 'mysg'
 ddn supergraph init <path-to-project-dir> --create-subgraph mysg
```

## Options

```sass
    --create-project                      Create a project while initializing the supergraph
    --create-subgraph string              The default subgraph to add (default "app")
    --globals-subgraph string             Name of the globals subgraph (default "globals")
-h, --help                                help for init
    --no-globals-subgraph                 Do not add a globals subgraph
    --no-subgraph                         Do not add a default subgraph
    --project-data-plane-id uuid          The DDN instance where the Project should be hosted, only used with --create-project flag or --with-promptql flag
    --project-name string                 Create a new project with this name, only used with --create-project flag or --with-promptql flag
    --project-plan string                 DDN Project plan, only used with --create-project flag or --with-promptql flag
    --subgraph-naming-convention string   Naming convention for the subgraph. Can be 'graphql' or 'none'.
    --with-project string                 Use an existing project instead of creating a new one, only used with --create-project flag or --with-promptql flag
    --with-promptql                       Initialize with PromptQL support, this also initializes a project with PromptQL enabled. (alpha)
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph related operations



--- File: ../ddn-docs/docs/reference/cli/commands/ddn_supergraph_prune.mdx ---
# ddn supergraph prune

---
sidebar_label: ddn supergraph prune
sidebar_position: 105
description: Removes unused metadata of the Supergraph using the DDN CLI
keywords:
  - hasura
  - DDN
  - docs
  - CLI
  - ddn supergraph prune
---

# DDN CLI: ddn supergraph prune

Removes unused metadata of the Supergraph.

## Synopsis

Removes unused metadata of the Supergraph

```bash
ddn supergraph prune [flags]
```

## Options

```sass
    --ci                  Disables the use of context
-c, --context string      Name of the context to use. (default <current_context>)
    --dry-run             Perform a dry run only. The changes won't be applied
-h, --help                help for prune
    --supergraph string   Path to Supergraph config file
```

## Options inherited from parent operations

```sass
--log-level string   Log level. Can be DEBUG, WARN, INFO, ERROR, or FATAL. (default "INFO")
--no-prompt          Do not prompt for required but missing flags
--out string         Output format. Can be table, json or yaml. (default "table")
--timeout int        Request timeout in seconds [env: HASURA_DDN_TIMEOUT] (default 100)
```

## Parent operation

- [ddn supergraph](/reference/cli/commands/ddn_supergraph) - Perform Supergraph related operations



--- File: ../ddn-docs/docs/reference/connectors/index.mdx ---
# Connectors

---
title: Connectors
sidebar_position: 1
description: "Learn about Hasura's powerful plugins architecture."
sidebar_label: Connector Configuration
keywords:
  - hasura plugins
  - allowlist plugin
  - plugins architecture
  - engine plugins
seoFrontMatterUpdated: false
---

# Connectors

## Introduction

The connector reference section primarily focuses on two areas:

- Connector configuration
- Extending the GraphQL API using native operations

If you're getting started with a connector and want to connect it to your data source as quickly as possible,
[learn how to connect a data source](/data-sources/connect-to-a-source.mdx) to Hasura DDN.

## Connector docs

- [ClickHouse](/reference/connectors/clickhouse/index.mdx)
- [MongoDB](/reference/connectors/mongodb/index.mdx)
- [MySQL](/reference/connectors/mysql/index.mdx)
- [Oracle](/reference/connectors/oracle/index.mdx)
- [PostgreSQL](/reference/connectors/postgresql/index.mdx)
- [SQL Server](/reference/connectors/sqlserver/index.mdx)
- [Trino](/reference/connectors/trino/index.mdx)



--- File: ../ddn-docs/docs/reference/connectors/clickhouse/index.mdx ---
# ClickHouse

---
title: ClickHouse
sidebar_position: 1
description:
  "Learn how to configure the ClickHouse connector and utilize native operations to extend your API's capability."
sidebar_label: ClickHouse
keywords:
  - clickhouse
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# ClickHouse

## Introduction

The ClickHouse data connector makes available any database resource that is listed in its configuration.

The Native Data Connector for ClickHouse supports all kinds of queries. In the related sections, we'll try to give an
overview of the features of the ClickHouse connector and how to configure it in a Hasura project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with ClickHouse and Hasura DDN as quickly as possible, check out our
[ClickHouse tutorial](/how-to-build-with-ddn/with-clickhouse.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a ClickHouse instance.

:::

## ClickHouse docs

- [Connector configuration](/reference/connectors/clickhouse/configuration.mdx)
- [Native operations](/reference/connectors/clickhouse/native-operations/index.mdx)



--- File: ../ddn-docs/docs/reference/connectors/clickhouse/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura ClickHouse connector, including connection URI details,
  and native queries."
keywords:
  - clickhouse
  - configuration
---

# Configuration

## Introduction

When you integrate the ClickHouse connector into a subgraph using the CLI, it first creates a new subdirectory in that
subgraph containing a templated configuration file. After you input the connection string and execute the update
command, the CLI will introspect your ClickHouse instance and automatically finalize the configuration for you.

## Manually editing the connector configuration

It is possible to manually modify the configuration, and valid changes will be preserved when the configuration is
updated.

The configuration file is accompanied by a json schema file which will help validate any manually written changes.

### Table alias

The keys in the tables object in the configuration file can be changed to modify the alias a table will be exposed
under.

This alias must remain unique.

### Table Return Type

Tables can return the same type as another table. This is useful for views that return rows from another table.

This will allow both tables to share an object type, which in turn allows both tables to share relationships and object
type permissions.



--- File: ../ddn-docs/docs/reference/connectors/clickhouse/native-operations/native-queries.mdx ---
# Native Queries

---
sidebar_position: 1
sidebar_label: Native Queries
description:
  "Native Queries allow you to run custom aggregation pipelines on your ClickHouse instance. This allows you to run
  queries that are not supported by Hasura's GraphQL engine. This page explains how to configure various types of Native
  Queries in Hasura."
keywords:
  - Native Queries
---

# Native Queries

## Introduction

This connector supports Native Queries: writing raw SQL queries to treat as collections (virtual tables)

This is an alternative to writing views on the database, which is usually preferable, but may not be plausible. This can
also be useful to iterate on views before creating them on the database.

:::warning Remote relationships

Parameterized Native Queries cannot be queried through remote relationships. Use
[ClickHouse Parameterized Views](https://clickhouse.com/docs/en/sql-reference/statements/create/view#parameterized-view)
instead.

:::

## Writing Native Queries

You can create SQL queries for your connector by writing them in `.sql` files and storing them within your configuration
directory, usually organized in a specific subdirectory for better structure.

**Requirements**:

- Each `.sql` file should contain only a single SQL statement to maintain clarity and modularity.
- When specifying arguments in your SQL queries, use the
  [ClickHouse parameter syntax](https://clickhouse.com/docs/en/interfaces/cli#cli-queries-with-parameters-syntax) to
  dynamically insert values.

**Example**:

Here's an example of a `.sql` file named `ArtistByName.sql` that retrieves artist data by name from the database:

```sql
-- queries/ArtistByName.sql
SELECT *
FROM "default"."Artist"
WHERE "Artist"."Name" = {ArtistName: String}
```

### Configuring queries in JSON

After creating your SQL query file, you need to define it in your `configuration.json` file. This involves specifying
the file path and the expected return type of the query.

Here is how you might add the `ArtistByName.sql` query to your configuration.json:

```json
{
  "tables": {},
  "queries": {
    "Name": {
      "exposed_as": "collection",
      "file": "queries/ArtistByName.sql",
      "return_type": {
        "kind": "definition",
        "columns": {
          "ArtistId": "Int32",
          "Name": "String"
        }
      }
    }
  }
}
```

### Determining return types

To accurately define the return type in your JSON configuration, you can use the
[ClickHouse `toTypeName`](https://clickhouse.com/docs/en/sql-reference/functions/other-functions#totypenamex) function
to inspect the types of columns returned by your query.

This SQL snippet can help you determine the column types by executing your query and applying the `toTypeName` function:

```sql
SELECT * APPLY toTypeName
FROM (
 -- your SQL here
) q LIMIT 1;
```

### Using table references

If your query's return type matches an existing table schema, and you prefer to reference the table directly in your
schema, you can do so by specifying it in your `configuration.json`.

This configuration links the `ArtistByName` query directly to the `Artist` table's schema:

```json
{
  "tables": {
    "Artist": {
      "name": "Artist",
      "schema": "default",
      "comment": "",
      "primary_key": {
        "name": "ArtistId",
        "columns": ["ArtistId"]
      },
      "return_type": {
        "kind": "definition",
        "columns": {
          "ArtistId": "Int32",
          "Name": "Nullable(String)"
        }
      }
    }
  },
  "queries": {
    "Name": {
      "exposed_as": "collection",
      "file": "queries/ArtistByName.sql",
      "return_type": {
        "kind": "table_reference",
        "table_name": "Artist"
      }
    }
  }
}
```



--- File: ../ddn-docs/docs/reference/connectors/clickhouse/native-operations/index.mdx ---
# Native Operations

---
sidebar_position: 2
sidebar_label: Native Operations
description:
  "Native Queries allow you to run custom aggregation pipelines on your ClickHouse instance. This allows you to run
  queries that are not supported by Hasura's GraphQL engine. This page explains how to configure various types of Native
  Queries in Hasura."
keywords:
  - native operations
---

# Native Operations

## Introduction

Native Operations allow you to run custom SQL queries on your ClickHouse database. This allows you to run queries that
are not supported by Hasura DDN's GraphQL engine. This unlocks the full power of your database, allowing you to run
complex queries directly from your Hasura GraphQL API.

## Get started

- [Native queries](/reference/connectors/clickhouse/native-operations/native-queries.mdx)



--- File: ../ddn-docs/docs/reference/connectors/elasticsearch/index.mdx ---
# Elasticsearch

---
title: Elasticsearch
sidebar_position: 1
description: "Learn how to configure the Elasticsearch connector and get an instant API for your Elasticsearch data."
sidebar_label: Elasticsearch
keywords:
  - elasticsearch
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# Elasticsearch

## Introduction

The Elasticsearch connector enables you to connect Elasticsearch data sources to your Hasura DDN API. Here we'll provide
an overview of the features offered by the connector and guide you through the configuration process within a Hasura DDN
project.

| Feature                                 | Supported |
| --------------------------------------- | --------- |
| Native Queries                          | âŒ        |
| Native Mutations                        | âŒ        |
| Filter / Search via term                | âœ…        |
| Filter / Search via terms               | âœ…        |
| Filter / Search via match               | âœ…        |
| Filter / Search via match_bool_prefix   | âœ…        |
| Filter / Search via match_phrase        | âœ…        |
| Filter / Search via prefix              | âœ…        |
| Filter / Search via range               | âœ…        |
| Filter / Search via regexp              | âœ…        |
| Filter / Search via wildcard            | âœ…        |
| Filter / Search via terms_set           | âŒ        |
| Filter / Search via intervals           | âŒ        |
| Filter / Search via query_string        | âŒ        |
| Filter / Search via simple_query_string | âŒ        |
| Filter / Search via fuzzy               | âŒ        |
| Simple Aggregation                      | âœ…        |
| Sort                                    | âœ…        |
| Paginate via offset                     | âœ…        |
| Paginate via search_after               | âœ…        |
| Distinct                                | âŒ        |
| Enums                                   | âŒ        |
| Default Values                          | âœ…        |
| User-defined Functions                  | âŒ        |
| Index Aliases                           | âœ…        |
| Field Aliases                           | âŒ        |
| Multi Fields                            | âŒ        |
| Runtime Fields                          | âŒ        |
| Field Analyzers                         | âŒ        |

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with Elasticsearch and Hasura DDN as quickly as possible, check out our
[Elasticsearch tutorial](/how-to-build-with-ddn/with-elasticsearch.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to an Elasticsearch instance.

:::

## Elasticsearch docs

- [Connector configuration](/reference/connectors/elasticsearch/configuration.mdx)



--- File: ../ddn-docs/docs/reference/connectors/elasticsearch/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description: "Reference documentation for the setup process for the Hasura Elasticsearch connector"
keywords:
  - elasticsearch
  - configuration
---

# Configuration

## Introduction

This document is the reference documentation for the Elasticsearch data connector configuration. Configuration is
currently a text file called `configuration.json` in the root folder of the connector. It is generated at the time of
introspection of the database for the first time and is used to set up the connector. Subsequent introspections (using the
`update` command) will overwrite the file. Below is an example configuration file along with explanations of its
components:

```json
{
  "indices": {
    "customers": {
      "mappings": {
        "properties": {
          "customer_id": {
            "type": "keyword"
          },
          "email": {
            "type": "keyword"
          },
          "location": {
            "type": "geo_point"
          },
          "name": {
            "fields": {
              "raw": {
                "ignore_above": 256,
                "type": "keyword"
              }
            },
            "type": "text"
          }
        }
      }
    }
  },
  "queries": {}
}
```

#### `indices`

The `indices` section reflects the indices that are available in the Elasticsearch instance. You should not edit this
section. If you wish to remove certain indices from your GraphQL API, do not import it into your Hausra DDN metadata.

#### `queries`

This section is reserved for future use and is currently not used. You should not edit this section.



--- File: ../ddn-docs/docs/reference/connectors/mongodb/configuration.mdx ---
# Configuration

---
sidebar_position: 1
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura MongoDB connector, including connection URI details, and
  native queries."
keywords:
  - mongodb
  - configuration
---

# Configuration

## Introduction

This document is the reference documentation for the MongoDB data connector configuration. Configuration is currently
broken down into three folders and a root configuration file.

1. [`configuration.json`](#root-configuration-file)
2. [`schema`](#schema)
3. [`native_queries`](#native-queries)
4. [`native_mutations`](#native-mutations)

### Root Configuration File

The root configuration file serves as a central point for configuring various aspects of the system. Below is an example
configuration file along with explanations of its components:

**Example**

```json
{
  "introspectionOptions": {
    "sampleSize": 100,
    "noValidatorSchema": false,
    "allSchemaNullable": true
  },
  "serializationOptions": {
    "extendedJsonMode": "relaxed"
  }
}
```

#### `introspectionOptions`

The `introspectionOptions` section allows customization of options related to introspection, which is the process of
examining or analyzing the system's structure and capabilities.

- `sampleSize`: Specifies the size of the sample used for introspection. In the provided example, the sample size is set
  to 100. Adjust this value as needed based on the requirements and characteristics of your system.
  - **Default**: `100`
- `noValidatorSchema`: Determines whether to include a validator schema during introspection. When set to false, a
  validator schema will be included. In the example, it's set to false, indicating that a validator schema will be
  included if one exists.
  - **Default**: `false`
- `allSchemaNullable`: Indicates whether all schema elements are nullable. When set to true, all schema elements are
  considered nullable. This setting can impact how the system handles data validation and type checking. In the example,
  it's set to true, implying that all schema elements are nullable.
  - **Default**: `true`

The `.configuration_metadata` file is a blank file with no content. It's used by the system as a marker to check if the
`configuration.json` file has been modified. Essentially, its presence alone serves as an indicator that the
configuration file has been updated since the last check. This simple mechanism helps the system keep track of changes
to the configuration without storing any actual timestamps or data.

#### `serializationOptions`

Unlike other data, fields with the configured type `extendedJSON` are converted to JSON using a format defined by
MongoDB called Extended JSON. This format has two formats: `canonical` and `relaxed`. The setting `extendedJsonMode`
determines which format the connector will use. See [Extended JSON](./extended-json.mdx).

### Schema

The `schema` directory contains JSON files of all collections that were introspected in your database.

**Example**

```json
{
  "name": "users",
  "collections": {
    "users": {
      "type": "users"
    }
  },
  "objectTypes": {
    "users": {
      "fields": {
        "_id": {
          "type": {
            "scalar": "objectId"
          }
        },
        "email": {
          "type": {
            "scalar": "string"
          }
        },
        "name": {
          "type": {
            "scalar": "string"
          }
        },
        "password": {
          "type": {
            "scalar": "string"
          }
        }
      }
    }
  }
}
```

#### Name

The `name` property specifies the name which gets exposed to Hasura metadata/tooling:

```json
{
  "name": "users"
}
```

#### Collections

The `collections` property contains definitions of the collection name in your database and the object type that
describes the type of each document in that collection:

```json
{
  "collections": {
    "users": {
      "type": "users"
    }
  }
}
```

#### Object types

The `objectTypes` property defines the document object type. Object types contain `fields` where each field specifies
a `type` and an optional `description`. See [Type Expressions](./type-expressions.mdx#object-types).

### Native queries

Native queries are named MongoDB [aggregation pipelines][MongoDB: pipeline] that you specify in the data connector
configuration. They are similar to a database view, but more expressive as they admit parameters similar to a function
and do not require any DDL permissions on the database. See the configuration reference on
[Native Queries](#updating-with-introspection).

More information in the [Native Queries](/reference/connectors/mongodb/native-operations/native-queries) documentation.

### Native mutations {#native-mutations}

Native mutations are named MongoDB [runCommand][MongoDB: runCommand] operations that you specify in the data connector
configuration. These are commonly used for mutations, but we do support the full [runCommand][MongoDB: runCommand] API.
See the configuration reference on [Native Mutations](#updating-with-introspection).

More information in the [Native Mutations](/reference/connectors/mongodb/native-operations/native-mutations.mdx)
documentation.

## Configuration workflows

The data connector provides a plugin to the hasura CLI to assist you in authoring configuration.

We provide the `mongodb-cli-plugin`, which is a small executable, of which the builds can be accessed
[here](https://github.com/hasura/ndc-mongodb/releases/).

:::warning Current status

The intended way to use this plugin is through the main `ddn` CLI.

But at the time of this writing, this part of the developer experience is undergoing active development, so the exact
command invocations are likely to change.

:::

### Updating with introspection {#updating-with-introspection}

Whenever the schema of your database changes you will need to update your data connector configuration accordingly to
reflect those changes.

Running `mongodb-cli-plugin update` in a directory will do the following:

Connect to the database with the specified `connection-uri`, create a `schema` directory, if one does not exist, and
then either create or update a file for each collection that is introspected in your database.

Currently, the `mongodb-cli-plugin` tries to use any validation schemas if they exist, and falls back to sampling
documents in your collection. The current sample size is 10. This will be configurable in future releases.

### Manually editing

There are occasions when the automatic introspection falls short of your needs. For instance, it may not detect a
particular entity type, or it may pick names according to conventions you disagree with.

:::note

This only applies to the files under the `schema` directory

:::

If you find yourself in this situation you are still able to bring your configuration into an acceptable state by
editing it manually. In this case you'd be well advised to keep your configuration files under version control, as
re-running the `update` command will overwrite your manually-crafted changes.

[Native Queries](/reference/connectors/mongodb/native-operations/native-queries) and
[Native Mutations](/reference/connectors/mongodb/native-operations/native-mutations.mdx) will always need manual
authorship currently. We plan on improving this authorship flow in the future.

[MongoDB: pipeline]: https://www.mongodb.com/docs/manual/core/aggregation-pipeline
[MongoDB: runCommand]: https://www.mongodb.com/docs/manual/reference/method/db.runCommand



--- File: ../ddn-docs/docs/reference/connectors/mongodb/index.mdx ---
# MongoDB

---
title: MongoDB
sidebar_position: 1
description:
  "Learn how to configure the MongoDB connector and utilize native operations to extend your API's capability."
sidebar_label: MongoDB
keywords:
  - mongodb
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# MongoDB

## Introduction

The MongoDB Native Data Connector for Hasura DDN expands our connectivity options, enabling integration with MongoDB
noSQL databases. Here we'll provide an overview of the features offered by the MongoDB connector and guide you through
the configuration process within a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with MongoDB and Hasura DDN as quickly as possible, check out our
[MongoDB tutorial](/how-to-build-with-ddn/with-mongodb.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a MongoDB instance.

:::

## MongoDB docs

- [Connector configuration](/reference/connectors/mongodb/configuration.mdx)
- [Connector types](./connector-types.mdx)
  - [Extended JSON](./extended-json.mdx)
  - [Type Expressions](./type-expressions.mdx)
- [Native operations](./native-operations/index.mdx)



--- File: ../ddn-docs/docs/reference/connectors/mongodb/connector-types.mdx ---
# Connector Types

---
sidebar_position: 2
sidebar_label: Connector Types
description: "Types used by the MongoDB connector, and how those types map to GraphQL"
keywords:
  - mongodb
  - configuration
---

# Connector Types

Although MongoDB is schema-less it does use a set of types for storing data. Specifically MongoDB stores data in
[BSON][] format. BSON is similar to JSON except that it is a binary instead of a text format, and BSON includes a number
of types that are not distinguished in JSON. For example BSON includes several numeric types while JSON only has one
type for numbers.

[BSON]: https://www.mongodb.com/docs/manual/reference/bson-types/

Choices of types determine what operations are available for filtering and sorting data. Types also determine how data
from your database will be represented in JSON since all data coming into and out of your database through the GraphQL
API is converted from BSON to JSON or vice versa.

This page describes the types that MongoDB supports, and provides information about type-related configuration in the
MongoDB connector.

## Scalar types

The MongoDB connector supports all of the scalar types defined by [BSON][] (except `dbPointer`), plus a first-class UUID
type (technically in BSON UUIDs are stored using the `binData` scalar type), and a special type called `extendedJSON`
which represents all possible BSON values. These are the types available:

| Type                  | Details                                                                                         |
| --------------------- | ----------------------------------------------------------------------------------------------- |
| `extendedJSON`        | the connector's "any" type - this type can hold any possible value                              |
| `int`                 | 32-bit integer                                                                                  |
| `long`                | 64-bit integer                                                                                  |
| `double`              | 64-bit float                                                                                    |
| `decimal`             | 128-bit float                                                                                   |
| `string`              | UTF-8 encoded string                                                                            |
| `date`                | date with time                                                                                  |
| `timestamp`           | used internally by MongoDB - use `date` instead                                                 |
| `binData`             | binary data with a "subtype" that indicates how the data is interpreted                         |
| `uuid`                | UUIDs such as those generated by the mongosh `new UUID()` constructor (added in connector v1.7) |
| `objectId`            | the default document identifier used by MongoDB                                                 |
| `bool`                |                                                                                                 |
| `null`                |                                                                                                 |
| `regex`               | regular expression                                                                              |
| `javascript`          | JavaScript code                                                                                 |
| `javascriptWithScope` | JavaScript code with bindings for closure variables                                             |
| `minKey`              | defined to be less than any other value for comparisons, regardless of type                     |
| `maxKey`              | defined to be greater than any other value for comparisons, regardless of type                  |
| `undefined`           | deprecated by MongoDB                                                                           |
| `dbPointer`           | deprecated by MongoDB, not supported by connector                                               |
| `symbol`              | deprecated by MongoDB                                                                           |

:::tip Scalar type names in GraphQL

The table above gives names for types as they are used in connector configuration which is taken from names in BSON
documentation. On the other hand, GraphQL uses uppercase type names. In your GraphQL schema and in metadata
configuration in `.hml` files you will see these type names with the first letter uppercased.

:::

## Complex types

The complex types that the connector recognizes are:

| Type Constructor | Details                                                                       |
| ---------------- | ----------------------------------------------------------------------------- |
| array            | homogenous array                                                              |
| object           | record type with a fixed set of string keys, and a distinct type for each key |
| nullable         | values of a nullable type might be `null`                                     |

For details on how to write complex types see [Type Expressions](./type-expressions.mdx).

## Representations in JSON

GraphQL operates using JSON; to interact with BSON data in your database, the connector must convert inputs (such as
filter arguments or mutation inputs) from JSON to BSON, and must convert result data from BSON to JSON. These
conversions are determined by the types configured in your connector's schema.

:::warning Extended JSON

The `extendedJSON` type represents all possible BSON values. Fields of this type are converted to JSON differently from
fields with more specific types. See [Extended JSON][].

[Extended JSON]: /reference/connectors/mongodb/extended-json

:::

In general, conversions between JSON and BSON are symmetric: input values will look the same as response data. Here are
JSON conversion details for each scalar type:

| Type                  | Example JSON                                                          | Details                                                                                                                              |
| --------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| `extendedJSON`        | `{ "$numberInt": "3" }`                                               | see [Extended JSON][]                                                                                                                |
| `int`                 | `3`                                                                   | JSON number                                                                                                                          |
| `long`                | `"3"`                                                                 | converted to a string to avoid accidental loss of precision                                                                          |
| `double`              | `3.14`                                                                | JSON number                                                                                                                          |
| `decimal`             | `"3.14"`                                                              | converted to a string to avoid accidental loss of precision                                                                          |
| `string`              | `"Hello, world!"`                                                     | JSON string                                                                                                                          |
| `date`                | `"2016-03-04T00:00:00.000000000Z"`                                    | string in ISO-8601 format                                                                                                            |
| `timestamp`           | `{ "t":1565545664, "i":1 }`                                           | use `date` instead                                                                                                                   |
| `binData`             | `{ "base64": "EEEBEIEIERA=", subType: "00" }`                         | base64-encoded data and interpretation hint - see [BSON docs](https://www.mongodb.com/docs/manual/reference/bson-types/#binary-data) |
| `uuid`                | `"40a693d0-c00a-425d-af5c-535e37fdfe9c"`                              | string in UUID format                                                                                                                |
| `objectId`            | `"66135e86eed2c00176f6fbe6"`                                          | string with hex encoding - the same format that `ObjectId` stringifies to in mongosh                                                 |
| `bool`                | `true`                                                                |                                                                                                                                      |
| `null`                | `null`                                                                |                                                                                                                                      |
| `regex`               | `{ "pattern": "^H", "options": "i" }`                                 | JSON object with separate fields for pattern and options - inputs may be given as a plain string for a pattern without options       |
| `javascript`          | `"function(x) { return x + 1 }"`                                      | JSON string                                                                                                                          |
| `javascriptWithScope` | `{ "$code": "function(x) { return x + y }", "$scope": { "y": "1" } }` | JSON object with code as string, and mappings for captured variables                                                                 |
| `minKey`              | `{}`                                                                  | empty object                                                                                                                         |
| `maxKey`              | `{}`                                                                  | empty object                                                                                                                         |
| `undefined`           | `null`                                                                | JSON doesn't have an `undefined` so BSON undefined converts to `null` instead                                                        |
| `symbol`              | `"foo"`                                                               | JSON string                                                                                                                          |
| `dbPointer`           |                                                                       | not supported by connector                                                                                                           |



--- File: ../ddn-docs/docs/reference/connectors/mongodb/extended-json.mdx ---
# Extended JSON

---
sidebar_position: 3
sidebar_label: Extended JSON
description:
  "Types used by the MongoDB connector, and how those types map to GraphQL"
keywords:
  - mongodb
  - configuration
---

# Extended JSON

The MongoDB connector uses introspection to infer schemas for database collections. Those schemas specify a type for
each document field. (See [Types](./connector-types.mdx)). Because MongoDB itself is schema-less, there are cases where
document data does not fit neatly into a type description. For these cases the special scalar type, `extendedJSON`, is
provided as an escape hatch. `extendedJSON` is the connector's "any" type - a field or input of type `extendedJSON` can
have any value.

The situation that comes up most often is heterogeneous data: different documents in a collection contain different
types of data under the same field name. For example consider a collection that has documents that have a `createdAt`
field, and some of those documents use `date` values for that field while others use `string` values. Since Hasura does
not implement union types there is no way to configure the connector for a field with type "either `date` or `string`".
In this case the solution is to set the type of the field to `extendedJSON`. And that is what the introspection system
does automatically when it samples data in this situation.

Another case that can come up is a desire to store dynamic data. For example, you may have a field that stores objects
with arbitrary keys. Since Hasura does not implement a dictionary type that can't be expressed directly in connector
configuration. Once again the solution is to set the field type to `extendedJSON`.

## Conversion to JSON

GraphQL operates using JSON; to interact with BSON data in your database the connector must convert inputs (such as
filter arguments or mutation inputs) from JSON to BSON, and must convert result data from BSON to JSON. These
conversions are determined by the types configured in your connector's schema.

Values with the `extendedJSON` type convert differently than other types. (For conversions for other types see
[Representations in JSON](./connector-types.mdx#representations-in-json).) `extendedJSON` values are converted to JSON
according to the [Extended JSON][] specification defined by MongoDB. This is the same format you get in mongosh when
serializing values using `EJSON`. For example, `EJSON.stringify(db.posts.findOne())`. Unlike JSON representations for
other types, Extended JSON includes inline type tags to preserve type information about the original BSON values. For
example when the BSON value `3` is converted to JSON using Extended JSON the result looks like this:

```json
{ "$numberInt": "3" }
```

[Extended JSON]: https://www.mongodb.com/docs/manual/reference/mongodb-extended-json/

Extended JSON has two modes: canonical and relaxed. Canonical mode is fully lossless, while relaxed mode can be nicer to
work with. The connector can be configured to use the format that you prefer using the
`serializationOptions.extendedJSONMode` option. See [Root Configuration
File](./configuration.mdx#root-configuration-file).



--- File: ../ddn-docs/docs/reference/connectors/mongodb/type-expressions.mdx ---
# Type Expressions

---
sidebar_position: 4
sidebar_label: Type Expressions
description:
  "Instructions for configuring types of schema fields, and native operation inputs and outputs."
keywords:
  - mongodb
  - configuration
---

# Type Expressions in Connector Configuration

Connector [configuration](./configuration.mdx) includes configuration for database collection schemas, and native
operations. Each of these require **type expressions** to specify the types of collection fields, native operation
arguments, and native operation outputs.

```json title="For example here is a schema configuration:"
{
  "name": "users",
  "collections": {
    "users": {
      "type": "users"
    }
  },
  "objectTypes": {
    "users": {
      "fields": {
        "_id": {
          "type": { "scalar": "objectId" },
          "description": "primary key"
        },
        "name": {
          "type": { "scalar": "string" }
        },
        "metadata": {
          "type": "extendedJSON"
        }
      }
    }
  }
}
```

This example specifies the collection object type, an object type called `users`, and provides a definition for that
object type with types for its `_id` and `_name` fields.

Types are expressed as JSON objects with (in most cases) a single field to distinguish scalar types vs the different
complex types.

## Scalar types

Scalar types are given as an object with a `scalar` key:

```json
{ "scalar": "string" }
```

The exception is the scalar type `extendJSON` which is expressed as a string instead of an object:

```json
"extendedJSON"
```

See [Connector Types](./connector-types.mdx#scalar-types) for a list of available scalar types.

## Nullable types

To get a nullable type, wrap any other type in an object with a `nullable` property:

```json
{ "nullable": { "scalar": "string" } }
```

```json
{ "nullable": "extendedJSON" }
```

Values of nullable types may be values allowed by the underlying type, or they may be `null`.

## Array types

To get an array type, wrap any other type in an object with an `arrayOf` property:

```json
{ "arrayOf": { "scalar": "string" } }
```

Arrays are homogeneous: every element of the array is assigned the same type.

## Object types

Objects are record types with a fixed set of string keys, and a distinct type for each key.

A reference to an object type is given by an object with an `object` property whose value is the name of an object type.

```json
{ "object": "TypeName" }
```

All object types are named types - there is no option for inline, anonymous object types.

Definitions for object types are given in `objectTypes` maps in connector configuration files.

```json
  "objectTypes": {
    "users": {
      "fields": {
        "_id": {
          "type": { "scalar": "objectId" },
          "description": "primary key"
        },
        "name": {
          "type": { "scalar": "string" }
        },
        "metadata": {
          "type": "extendedJSON"
        }
      }
    }
  }
```

The above example defines an object type called `users`. Each field of the object type has a required `type` field that
uses the type expression syntax described on this page, and an optional `description` field which may contain markdown.

These `objectTypes` maps can appear in each schema, native query, and native mutation file. The definitions from every
configuration file are merged into one shared namespace. This has two implications:

- Every object type name must be unique across all configuration files for the same connector.
- Each configuration file may reference object types defined in any other configuration file. For example a native query
  may reference an object type defined in a schema configuration file.



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/index.mdx ---
# Native Operations

---
sidebar_position: 1
sidebar_label: Native Operations
description:
  "Native operations allow you to run custom aggregation pipelines on your MongoDB instance. This allows you to run
  queries that are not supported by Hasura's GraphQL engine. This page explains how to configure various types of Native
  Operations in Hasura."
keywords:
  - native operations
---

# Native Operations

## Introduction

Native Operations allow you to run custom SQL queries on your MongoDB database. This allows you to run queries that are
not supported by Hasura DDN's GraphQL engine. This unlocks the full power of your database, allowing you to run complex
queries and mutations directly from your Hasura GraphQL API.

## Get started

- [Syntax](/reference/connectors/mongodb/native-operations/syntax.mdx)
- [Native queries](/reference/connectors/mongodb/native-operations/native-queries.mdx)
- [Native mutations](/reference/connectors/mongodb/native-operations/native-mutations.mdx)
- [Security best practices](/reference/connectors/mongodb/native-operations/security-best-practices.mdx)
- [Supported aggregation pipeline features](/reference/connectors/mongodb/native-operations/supported-aggregation-pipeline-features.mdx)
- [Vector search](/reference/connectors/mongodb/native-operations/vector-search.mdx)



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/syntax.mdx ---
# Syntax

---
sidebar_position: 1
sidebar_label: Syntax
description: Learn how to write Native Operations for MongoDB with the proper syntax in Hasura.
keywords:
  - hasura
  - native operations
  - troubleshooting
  - aggregate pipeline
  - custom mutations
  - custom queries
  - native queries
  - native mutations
  - syntax
  - gotchas
seoFrontMatterUpdated: false
toc_max_heading_level: 4
---

# Native Operations MongoDB Syntax

## Native Queries

Pipeline to include in MongoDB queries. For details on how to write an aggregation pipeline see
https://www.mongodb.com/docs/manual/core/aggregation-pipeline/

The pipeline may include Extended JSON.

Keys and values in the pipeline may contain placeholders of the form `{{variableName}}` which will be substituted when
the native query is executed according to the given arguments.

Placeholders must be inside quotes so that the pipeline can be stored in JSON format. If the pipeline includes a string
whose only content is a placeholder, when the variable is substituted the string will be replaced by the type of the
variable. For example in this pipeline,

```json
[
  {
    "$documents": "{{ documents }}"
  }
]
```

If the type of the `documents` argument is an array then after variable substitution the pipeline will expand to:

```json
[
  {
    "$documents": [
      /* array of documents */
    ]
  }
]
```

### Examples

#### Collection Representation

```json
{
  "name": "ArtistWAlbumCount",
  "representation": "collection",
  "inputCollection": "Artist",
  "description": "Artists including their album count",
  "arguments": {},
  "resultDocumentType": "ArtistWAlbumCount",
  "objectTypes": {
    "ArtistWAlbumCount": {
      "fields": {
        "ArtistId": {
          "type": {
            "scalar": "int"
          }
        },
        "Name": {
          "type": {
            "scalar": "string"
          }
        },
        "AlbumCount": {
          "type": {
            "scalar": "int"
          }
        },
        "_id": {
          "type": {
            "scalar": "objectId"
          }
        }
      }
    }
  },
  "pipeline": [
    {
      "$lookup": {
        "from": "Album",
        "localField": "ArtistId",
        "foreignField": "ArtistId",
        "as": "Albums"
      }
    },
    { "$unwind": "$Albums" },
    {
      "$group": {
        "_id": "$ArtistId",
        "id": { "$first": "$_id" },
        "Name": { "$first": "$Name" },
        "AlbumCount": { "$count": {} }
      }
    },
    {
      "$project": {
        "_id": "$id",
        "ArtistId": "$_id",
        "Name": 1,
        "AlbumCount": 1
      }
    }
  ]
}
```

#### Function Representation {#example-using-function-representation}

```json
{
  "name": "hello",
  "representation": "function",
  "description": "Basic test of native queries",
  "arguments": {
    "name": { "type": { "scalar": "string" } }
  },
  "resultDocumentType": "Hello",
  "objectTypes": {
    "Hello": {
      "fields": {
        "__value": { "type": { "scalar": "string" } }
      }
    }
  },
  "pipeline": [
    {
      "$documents": [
        {
          "__value": "{{ name }}"
        }
      ]
    }
  ]
}
```

### Documentation

#### Name

| Property | Required? |
| -------- | --------- |
| `"name"` | Yes       |

Represents the name of the query that is exposed in your API.

#### Representation

| Property           | Required? |
| ------------------ | --------- |
| `"representation"` | Yes       |

:::tip

Using a representation of a function in MongoDB Native Queries follows the same requirements of [functions in the DDN
spec][DDN: functions]. Specifically:

> A function is a collection which returns a single row and a single column, named \_\_value

<br />
Refer to the [function example](#example-using-function-representation) above.{" "}

:::

There are 2 options:

1. `collection` is used here in terms of [DDN collections][DDN: collections].
2. `function` is used here in terms of [DDN functions][DDN: functions].

Native queries with the `function` representation have the restriction that the aggregation pipeline output must be a
singe document with a single property named `__value`. When the function is invoked the document will be unwrapped, and
the value in `__value` will be used as the return value of the function. This allows functions to produce values that
are not objects, as opposed to MongoDB aggregation pipelines which can only produce streams of objects.

#### Input Collection

| Property            | Required? |
| ------------------- | --------- |
| `"inputCollection"` | No        |

Determines how the pipeline is run. If `"inputCollection"` is present then the pipeline runs with that collection as the
starting point. It is the same as doing `db.artist.aggregate` in `mongosh`. If no `"inputCollection"` is present then
the pipeline is run with `db` as the starting point. Which is the same as doing `db.aggregate` in `mongosh`.

#### Description

| Property        | Required? |
| --------------- | --------- |
| `"description"` | No        |

Used for documentation purposes in the API.

#### Arguments

| Property      | Required? |
| ------------- | --------- |
| `"arguments"` | No        |

Used if you want to accept arguments in your API for your pipeline. Please refer to [Type Expressions](../type-expressions.mdx). Arguments are
denoted in your [pipeline](#pipeline) code using `"{{ argument }}"`.

**Example:**

```json
  "arguments": {
    "abs": {
      "type": {
        "scalar": "int"
      }
    },
    "binarySize": {
      "type": {
        "scalar": "string"
      }
    }
```

#### Result Document Type

| Property               | Required? |
| ---------------------- | --------- |
| `"resultDocumentType"` | Yes       |

Represents the result type of your pipeline. This can refer to an existing object type from your schema or can be object
types you write yourself following the structure of [Type Expressions](../type-expressions.mdx)

A pipeline returns a stream of documents. `resultDocumentType` is the type of each individual document.

#### Object Types

| Property        | Required? |
| --------------- | --------- |
| `"objectTypes"` | No        |

If you need to return a type that is not already in your schema then you can write your own. Please refer to
[Type Expressions](../type-expressions.mdx#object-types) on how to write your own object types.

Object types from all configuration files occupy a shared namespace so it is important that object type names be unique
within your configuration.

**Example:**

```json
  "objectTypes": {
    "ArtistWAlbumCount": {
      "fields": {
        "ArtistId": {
          "type": {
            "scalar": "int"
          }
        },
        "Name": {
          "type": {
            "scalar": "string"
          }
        },
        "AlbumCount": {
          "type": {
            "scalar": "int"
          }
        },
        "_id": {
          "type": {
            "scalar": "objectId"
          }
        }
      }
    }
  }
```

#### Pipeline

| Property     | Required? |
| ------------ | --------- |
| `"pipeline"` | Yes       |

The pipeline is almost identical to what you would write in javascript or mongosh using the [aggregate
pipeline][MongoDB: pipeline] API. The only difference is the double qouted keys to comply with JSON. For documentation
on aggregate pipelines please refer to [MongoDB][MongoDB: pipeline]. If you have arguments they need to be wrapped in
double quotes and double braces `"{{ argument }}"`. See example with arguments below.

**Example:** without arguments

```json
  "pipeline": [
    {
      "$lookup": {
        "from": "Album",
        "localField": "ArtistId",
        "foreignField": "ArtistId",
        "as": "Albums"
      }
    },
    {"$unwind": "$Albums"},
    {
      "$group": {
        "_id": "$ArtistId",
        "id": {"$first": "$_id"},
        "Name": {"$first": "$Name"},
        "AlbumCount": {"$count": {}}
      }
    },
    {
      "$project": {
        "_id": "$id",
        "ArtistId": "$_id",
        "Name": 1,
        "AlbumCount": 1
      }
    }
  ]
```

**Example:** with arguments

```json
"pipeline": [
  {
    "$documents": [
      {
        "__value": {
          "abs": {
            "$abs": "{{ abs }}"
          },
          "binarySize": {
            "$binarySize": "{{ binarySize }}"
          },
          "ceil": {
            "$ceil": "{{ ceil }}"
          },
          "floor": {
            "$floor": "{{ floor }}"
          },
          "divide": {
            "$divide": ["{{ dividend }}", "{{ divisor }}"]
          }
        }
      }
    ]
  }
]
```

[DDN: collections]: https://hasura.github.io/ndc-spec/specification/schema/collections.html
[DDN: functions]: https://hasura.github.io/ndc-spec/specification/schema/functions.html
[MongoDB: pipeline]: https://www.mongodb.com/docs/manual/core/aggregation-pipeline

## Native Mutations

Command to run via MongoDB's `runCommand` API. For details on how to write commands see
https://www.mongodb.com/docs/manual/reference/method/db.runCommand/

The command is read as Extended JSON. It may be in canonical or relaxed format, or a mixture of both. See
https://www.mongodb.com/docs/manual/reference/mongodb-extended-json/

Keys and values in the command may contain placeholders of the form `{{variableName}}` which will be substituted when
the native procedure is executed according to the given arguments.

Placeholders must be inside quotes so that the command can be stored in JSON format. If the command includes a string
whose only content is a placeholder, when the variable is substituted the string will be replaced by the type of the
variable. For example in this command,

```json
{
  "insert": "posts",
  "documents": "{{ documents }}"
}
```

If the type of the `documents` argument is an array then after variable substitution the command will expand to:

```json
{
  "insert": "posts",
  "documents": [
    /* array of documents */
  ]
}
```

### Examples

#### Insert

```json
  "name": "insertArtist",
  "description": "Example of a database insert using a native mutation",
  "resultType": {
    "object": "InsertArtist"
  },
  "arguments": {
    "artistId": {
      "type": {
        "scalar": "int"
      }
    },
    "name": {
      "type": {
        "scalar": "string"
      }
    }
  },
  "objectTypes": {
    "InsertArtist": {
      "fields": {
        "ok": {
          "type": {
            "scalar": "int"
          }
        },
        "n": {
          "type": {
            "scalar": "int"
          }
        }
      }
    }
  },
  "command": {
    "insert": "Artist",
    "documents": [
      {
        "ArtistId": "{{ artistId }}",
        "Name": "{{ name }}"
      }
    ]
  }
}
```

#### Update

```json
{
  "name": "updateArtist",
  "description": "Example of a database update using a native mutation",
  "resultType": {
    "object": "UpdateArtist"
  },
  "arguments": {
    "artistId": {
      "type": {
        "scalar": "int"
      }
    },
    "name": {
      "type": {
        "scalar": "string"
      }
    }
  },
  "objectTypes": {
    "UpdateArtist": {
      "fields": {
        "ok": {
          "type": {
            "scalar": "int"
          }
        },
        "n": {
          "type": {
            "scalar": "int"
          }
        }
      }
    }
  },
  "command": {
    "update": "Artist",
    "updates": [
      {
        "q": {
          "ArtistId": "{{ artistId }}"
        },
        "u": {
          "ArtistId": "{{ artistId }}",
          "Name": "{{ name }}"
        }
      }
    ]
  }
}
```

#### Delete

```json
{
  "name": "deleteArtist",
  "description": "Example of a database delete using a native mutation",
  "resultType": {
    "object": "DeleteArtist"
  },
  "arguments": {
    "artistId": {
      "type": {
        "scalar": "int"
      }
    }
  },
  "objectTypes": {
    "DeleteArtist": {
      "fields": {
        "ok": {
          "type": {
            "scalar": "int"
          }
        },
        "n": {
          "type": {
            "scalar": "int"
          }
        }
      }
    }
  },
  "command": {
    "delete": "Artist",
    "deletes": [
      {
        "q": {
          "ArtistId": "{{ artistId }}"
        }
      }
    ]
  }
}
```

### Documentation

#### Name

`"name"`: **Required**

Represents the name of the query that is exposed in your API.

#### Description

`"description"`: **Optional**

Used for documentation purposes in the API.

#### Result Type

`"resultType"`: **Required**

Represents the result type of your command. This can refer to an existing object type from your schema or can be an
object type you write in the native query file following the structure described in [Type Expressions](../type-expressions.mdx).

#### Arguments

`"arguments"`: **Optional**

Used if you want to accept arguments in your API for your pipeline. Please refer to [Type Expressions](../type-expressions.mdx). Arguments are
denoted in your [command](#command) code using `"{{ argument }}"`.

**Example:**

```json
"arguments": {
  "age": {
    "type": {
      "scalar": "int"
    }
  },
  "name": {
    "type": {
      "scalar": "string"
    }
  }
}
```

#### Object Types

`"objectTypes"`: **Optional**

If you need to return a type that is not already in your schema then you can write your own. Please refer to
[Type Expressions](../type-expressions.mdx#object-types) on how to write your own object types.

**Example:**

```json
"objectTypes": {
  "ArtistWAlbumCount": {
    "fields": {
      "ArtistId": {
        "type": {
          "scalar": "int"
        }
      },
      "Name": {
        "type": {
          "scalar": "string"
        }
      },
      "AlbumCount": {
        "type": {
          "scalar": "int"
        }
      },
      "_id": {
        "type": {
          "scalar": "objectId"
        }
      }
    }
  }
}
```

#### Command

`"command"`: **Required**

The command is almost identical to what you would write in javascript or mongosh using the
[runCommand][MongoDB: runCommand] API. The only difference is the double qouted keys to comply with JSON. For
documentation on runCommand please refer to [MongoDB][MongoDB: runCommand]. If you have arguments they need to be
wrapped in double quotes and double braces `"{{ argument }}"`. See example below.

**Example:**

```json
"command": {
  "insert": "Artist",
  "documents": [
    {
      "ArtistId": "{{ artistId }}",
      "Name": "{{ name }}"
    }
  ]
}

```

[MongoDB: runCommand]: https://www.mongodb.com/docs/manual/reference/method/db.runCommand



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/native-queries.mdx ---
# Native Queries

---
sidebar_position: 2
sidebar_label: Native Queries
description:
  "Native Queries allow you to run custom aggregation pipelines on your MongoDB instance. This allows you to run queries
  that are not supported by Hasura's GraphQL engine. This page explains how to configure various types of Native Queries
  in Hasura."
keywords:
  - Native Queries
---

# Native Queries

## Introduction

Native Queries allow you to run custom queries on your MongoDB database. This allows you to run queries that are not
supported by Hasura's GraphQL engine. These queries are defined as [aggregation pipelines][] which is the query format
that is used by MongoDB's aggregate command.

[aggregation pipelines]: https://www.mongodb.com/docs/manual/aggregation/#std-label-aggregation-pipeline-intro

## Managing native queries

Native Queries can be managed either via the DDN CLI, or by editing connector configuration files directly.

To connect a query to your data graph the native query configuration must declare the type of data that it produces, and
the types of any parameters. The major difference between using the CLI vs configuring by hand is how these type
declarations are managed.

### 1. CLI (BETA)

The CLI will type-check your aggregation pipeline, and will automatically produce type declarations for you. But this
functionality is experimental - there are aggregation pipeline features that the CLI does not recognize yet. The CLI
will not be able to produce configurations for pipelines that use those features. See [supported pipeline features][].

### 2. Directly in configuration files

When writing configuration directly you may use any aggregation pipeline features, and you have complete control over
how types are declared. But you must write all type declarations yourself.

:::tip Switching between CLI and configuration files

You can freely switch between configuring with the CLI or writing configuration files directly. For example you can
manage one native query with the CLI, and configure another directly; or you can edit CLI-generated configuration files.
The recommendation is to try using the CLI first, and if it doesn't work or does not produce the correct configuration
then write configuration directly.

:::

[supported pipeline features]: ./supported-aggregation-pipeline-features.mdx

## Manage native queries with the DDN CLI {#manage-native-queries-with-the-ddn-cli}

:::caution Connector version requirement

Managing native queries using the CLI requires version 1.5.0 or later of the MongoDB Connector. If you have an earlier
version please skip to the [Write native query configurations directly](#write-native-query-configurations-directly)
section.

This is a BETA feature - it is a work in progress, and will not work for all cases. It is safe to experiment with since
it is limited to managing native query configuration files, and does not lock you into anything.

:::

### Create a native query

Create a file with a `.json` extension that contains the aggregation pipeline for your query. For example this pipeline
outputs frequency counts for words appearing in movie titles in a given year:

```json title="title_word_frequency.json"
[
  {
    "$match": {
      "year": { "$eq": "{{ year }}" }
    }
  },
  {
    "$replaceWith": {
      "title_words": { "$split": ["$title", " "] }
    }
  },
  { "$unwind": { "path": "$title_words" } },
  {
    "$group": {
      "_id": "$title_words",
      "count": { "$count": {} }
    }
  }
]
```

In your supergraph directory run a command like this using the path to the pipeline file as an argument,

```sh
ddn connector plugin \
  --connector app/connector/my_connector/connector.yaml \
  -- native-query create title_word_frequency.json \
  --collection movies
```

The name of the native query is based on the input JSON file name, in this case `title_word_frequency`. You can override
this with the `--name` flag.

If your aggregation pipeline uses a collection as an input specify it with the `--collection` flag. Your connector must
already be configured with the schema for the collection. Type inference will be based on the configured schema.

If you are updating an existing native query add the `--force` flag to overwrite the existing configuration.

You should see output like this:

```text
Wrote native query configuration to your-project/connector/native_queries/title_word_frequency.json

input collection: movies
representation: collection

## parameters

year: int!

## result type

{
  _id: string!,
  count: int!
}
```

You may keep the pipeline file that you wrote if you think that you might want to edit it to update the native query
later, or you may delete it.

The final step is to update your metadata to track the new native query:

```sh
ddn connector-link update my_connector --add-all-resources
```

#### Native Query Parameters

Parameters / arguments for your native query are specified using the placeholder syntax, `"{{ parameter_name }}"` inside
a string. The example above includes a parameter called `year` using this syntax.

:::danger Avoid code injection

When placeholders are substituted, arguments are **not** sanitized. It is up to you as the author of a native query to
construct the query to avoid potential code injection. See [Security Best Practices](./security-best-practices.mdx).

:::

Despite the placeholder appearing in a string, if the only content of the string is the placeholder then the placeholder
will be replaced with an argument value at query time which may be any type, including complex types such as arrays or
objects. In the example above the type of the `year` parameter is `int!` so the string `"{{ year }}"` will be replaced
with a non-null int value.

The type checker attempts to infer types for parameters. If it is not able to do so you can add a type annotation with a
bar (`|`) following the parameter name. For example, `"{{ year | int! }}"`. Type annotations use a GraphQL-style type
expression syntax so for example a non-nullable list of nullable double values would be `[double]!`. The scalar types
used in these type expressions are BSON types - for a list see [Connector Types](../connector-types.mdx#scalar-types).

The same parameter may occur any number of times in a pipeline. If you use a type annotation you may apply it to any one
occurrence of the parameter, or to all of them.

### List, inspect, and delete native queries

List names of configured native queries with,

```sh
ddn connector plugin \
  --connector app/connector/my_connector/connector.yaml \
  -- native-query list
```

Show information about a native including its pipeline and type declarations with,

```sh
ddn connector plugin \
  --connector app/connector/my_connector/connector.yaml \
  -- native-query show <native-query-name>
```

Delete a native query with,

```sh
ddn connector plugin \
  --connector app/connector/my_connector/connector.yaml \
  -- native-query delete <native-query-name>
```

After deleting you will need to update your metadata configuration as well.

## Write native query configurations directly {#write-native-query-configurations-directly}

Native Queries can be defined by

1. Adding a `native_queries` directory if one doesn't already exist in your connector configuration directory
2. Adding a `.json` file following the syntax laid out in the following sections.

Native Queries can take arguments using the placeholder syntax, `"{{argument_name}}"`. Arguments must be specified along
with their type.

:::danger Avoid code injection

When placeholders are substituted, arguments are **not** sanitized. It is up to you as the author of a native query to
construct the query to avoid potential code injection. See [Security Best Practices](./security-best-practices.mdx).

:::

Here's an example of simple `"hello"` Native Query:

### Configuration

```json
{
  "name": "hello",
  "representation": "function",
  "description": "Basic test of native queries",
  "arguments": {
    "name": { "type": { "scalar": "string" } }
  },
  "resultDocumentType": "Hello",
  "objectTypes": {
    "Hello": {
      "fields": {
        "__value": { "type": { "scalar": "string" } }
      }
    }
  },
  "pipeline": [
    {
      "$documents": [
        {
          "__value": { "$literal": "{{ name }}" }
        }
      ]
    }
  ]
}
```

This will create a query called `"hello"` which takes a single argument called `"name"` of type `string`. The query will
return an object with the key `"__value"` and the value of the argument `"hello"`.

:::tip Valid Native Query syntax

Check out our page on writing valid Hasura DDN
[Native Operations syntax](/reference/connectors/mongodb/native-operations/syntax.mdx).

:::

To add the native query to your supergraph you need to update the metadata with a command like this one,

```sh
ddn connector-link update <connector-name> --add-all-resources
```

### Usage

With the example above, you can then use the query in your GraphQL API like this:

```graphql
query MyQuery {
  hello(name: "world") {
    __value
  }
}
```



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/native-mutations.mdx ---
# Native Mutations

---
sidebar_position: 3
sidebar_label: Native Mutations
description:
  "Native mutations allow you to mutate date in your MongoDB instance. This page explains how to configure various types
  of Native Mutations in Hasura."
keywords:
  - native mutationsc
---

# Native Mutations

## Introduction

Native Mutations allow you to run custom commands on your MongoDB database using the [runCommand][MongoDB: runCommand]
API that can be exposed via the Hasura GraphQL and modify the database state. As point-to-point mutations are not yet
supported, this is one way to mutate data using your GraphQL API.

## Setup

Native Mutations can be defined by

1. Adding a `native_mutations` directory if one doesn't already exist in your connector configuration directory
2. Adding a `.json` file following the syntax laid out in the following sections.

## Example

Here's an example of simple `"insert_artist"` Native Mutation:

### Configuration

```json
{
  "name": "insertArtist",
  "description": "Example of a database insert using a native mutation",
  "resultType": {
    "object": "InsertArtist"
  },
  "arguments": {
    "artistId": {
      "type": {
        "scalar": "int"
      }
    },
    "name": {
      "type": {
        "scalar": "string"
      }
    }
  },
  "objectTypes": {
    "InsertArtist": {
      "fields": {
        "ok": {
          "type": {
            "scalar": "int"
          }
        },
        "n": {
          "type": {
            "scalar": "int"
          }
        }
      }
    }
  },
  "command": {
    "insert": "Artist",
    "documents": [
      {
        "ArtistId": "{{ artistId }}",
        "Name": "{{ name }}"
      }
    ]
  }
}
```

This will create a mutation called `"insertUser"` which takes two arguments called `"artistId"` of type `int`, and
`"name"` of type `string`. The query will return an object with the keys `"ok"` and `"n"` and the values as `int`s.

Native Queries can take arguments using the placeholder syntax, `"{{argument_name}}"`. Arguments must be specified along
with their type.

:::danger Avoid code injection

When placeholders are substituted arguments are **not** sanitized. It is up to you as the author of a native query to
construct the query to avoid potential code injection. See [Security Best Practices](./security-best-practices.mdx).

:::

To add the native query to your supergraph you need to update the metadata with a command like this one,

```sh
ddn connector-link update <connector-name> --add-all-resources
```

:::tip Valid Native Mutation Syntax

Check out our page on writing valid Hasura DDN
[Native Operations syntax](/reference/connectors/mongodb/native-operations/syntax.mdx).

:::

### Usage

With the example above, you can then use the query in your GraphQL API like this:

```graphql
mutation MyMutation {
  insertArtist(artistId: 10000, name: "Pearl Jam") {
    ok
    n
  }
}
```

[MongoDB: runCommand]: https://www.mongodb.com/docs/manual/reference/method/db.runCommand



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/security-best-practices.mdx ---
# Security Best Practices

---
sidebar_position: 4
sidebar_label: Security Best Practices
description:
  "When writing native queries it is critical to understand the security implications of how arguments are interpreted."
keywords:
  - native queries
  - native mutations
  - security
seoFrontMatterUpdated: false
---

# Security Best Practices in MongoDB Native Operations

Native queries and native mutations allow you to construct any arbitrary query or database command respectively. That
power comes with the potential to introduce exploits in your API which you should take care to avoid. In particular
input parameters in native queries and native mutations are **not** sanitized so you must take care to escape uses of
inputs. These are inputs that appear in your query or command using the double-curly-brace placeholder syntax: `{{
parameter_name }}`.

## Do not substitute ASTs

Double-curly-brace parameter substitution technically makes it possible to accept pipeline expressions or stages (a.k.a
"ASTs") as arguments. **Do not do this**.

```json
[
  {
    "$match": {
      "user_id": { "$eq": "{{ user_id }}" }
    }
  },
  "{{ custom_query_steps }}",
  {
    "$project": {
      "title": true,
      "content": true
    }
  }
]
```

The use of `custom_query_steps` gives complete control to users to insert any code into the query. Even if there is no
sensitive data in a collection, there are aggregation stages, like `$out`, that can write to the database, and therefore
can **inject data**. There are also stages, like `$lookup`, that can **read data from other collections**.

:::info JSON delimiter injection is not possible

Native query pipelines are parsed as BSON (by first parsing as JSON and interpreting the resulting document as [Extended
JSON][]) **before** parameters are substituted. So there is no need to worry about arguments that terminate a JSON node to
insert an extra node. For example an input of the form, `'"}}, { "$out": ... }'`, would not cause a security problem.

[Extended JSON]: https://www.mongodb.com/docs/manual/reference/mongodb-extended-json/

:::

## Escape inputs

Native queries are written as [aggregation pipelines][] which are JSON documents. Certain strings and object keys are
interpreted specially - they are evaluated as expressions instead of as plain values. In general any string that begins
with a dollar sign (`$`) evaluates as a lookup for a field in the "current" document, or references a variable; and any
object key that begins with a dollar sign represents an expression operator.

[aggregation pipelines]: https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/

Consider this pipeline which is intended to produce results that pass through an input argument so that it appears in
query output:

```json
[
  {
    "$project": {
      "value_from_query_input": "{{ user_input }}"
    }
  }
]
```

If a user is allowed to provide a string value for `user_input` then they could provide a string like
`"$some_sensitive_field"`. That evaluates to whatever is in a field with the same name in each database document, which
could cause information to leak that you might not want users to be able to access.

The best way to avoid this potential problem is to **escape** any user inputs that appear in your query. MongoDB
provides an operator that does this: `{ "$literal": <input value> }`. Unfortunately that operator is not allowed in
all contexts, so the specific procedure for escaping inputs depends on where those inputs appear.

### Escape inputs in `$match` stages

This section applies to `$match` stages that do **not** use the `$expr` syntax to switch to an expression context. If
you have a `$match` stage that uses the `$expr` operator see the [Escape inputs in other stages][] section below.

[Escape inputs in other stages]: #escape-inputs-in-other-stages

Although MongoDB aggregation pipelines support a fully-featured expression language, those expressions are not allowed
by default in `$match`. Instead of expressions `$match` takes a [query predicate][] which only permits a limited set of
query operators: `{ "$eq": <value> }`, `{ "$or": [<predicates>] }`, etc. This means that the input-escaping operator, `{
"$literal": <input value> }`, is not permitted because that is an expression operator, not a query operator. On the
other hand this also means that arbitrary expressions cannot be injected into this context via user input.

[query predicate]: https://www.mongodb.com/docs/manual/reference/glossary/#std-term-query-predicate

Where you can potentially see a problem is when using an implicit equality check instead of explicitly using the `$eq`
operator.

This is an **implicit** equality comparison:

```json
[
  {
    "$match": {
      "user_id": "{{ user_id }}"
    }
  }
]
```

This is an **explicit** equality comparison:

```json
[
  {
    "$match": {
      "user_id": { "$eq": "{{ user_id }}" }
    }
  }
]
```

**Always use explicit comparisons.** This has the effect of escaping inputs because arguments to query operators are
interpreted as literal values.

The above examples filter to documents with an exact match for an input user ID. If a user is allowed to supply an
object value for `user_value`, and you are using an implicit comparison what can happen is that the user can switch the
query to a different comparison operator. For example if the value of `user_id` is `{ "$ne": "1234" }` then instead of
getting only documents with `user_id` `1234` the query would return data from documents with all other user IDs, which
might leak sensitive information.

If you instead use an explicit comparison like `{ "$eq": <comparison value> }` then `comparison value` will be
interpreted as a literal value.

:::info The connector verifies input types, with a caveat

In general the MongoDB connector checks that user inputs match the types declared for a query. So if the parameter
`user_id` has the type `string` or `long` the connector won't accept an object as input. But if the parameter type is
`extendedJSON` then the connector will accept any type of argument.

:::

### Escape inputs in other stages

In most cases where user inputs appear you can escape them using the expression operator, `$literal`. For example the
proper escaping for the example from the intro to this section looks like this:

```json
[
  {
    "$project": {
      "value_from_query_input": { "$literal": "{{ user_input }}" }
    }
  }
]
```

The `$literal` operator can be used in any context where an expression is used. But it is not allowed in query
predicates.

Use `$literal` in `$match` stages if that stage uses the `$expr` operator. `$expr` switches from a query predicate
context to an expression context. Modifying the example from the previous section, a `$match` stage that uses `$expr`
should be escaped like this:

```json
[
  {
    "$match": {
      "$expr": {
        "$eq": ["$user_id", { "$literal": "{{ user_id}}" }]
      }
    }
  }
]
```



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/supported-aggregation-pipeline-features.mdx ---
# Supported Pipeline Features

---
sidebar_position: 5
sidebar_label: Supported Pipeline Features
description: See which aggregation pipeline features work with automatic type inference.
keywords:
  - hasura
  - native operations
  - aggregate pipeline
  - custom queries
  - native queries
  - syntax
  - gotchas
seoFrontMatterUpdated: false
toc_max_heading_level: 4
---

# Supported aggregation pipeline features

Native queries use [aggregation pipelines][] as the format for custom MongoDB queries. Native query configurations must
include type declarations for inputs and outputs for the native query's pipeline. The DDN CLI is able to type check
aggregation pipelines to write type declarations automatically (see [Manage native queries with the DDN CLI][]). But
this feature is in beta, and only supports a subset of aggregation pipeline features.

[aggregation pipelines]: https://www.mongodb.com/docs/manual/aggregation/#std-label-aggregation-pipeline-intro
[Manage native queries with the DDN CLI]: ./native-queries.mdx#manage-native-queries-with-the-ddn-cli

## Supported pipeline stages

An aggregation pipeline is a list of [aggregation stages][] that each filter, transform, or sort documents that flow
through them. Hasura's beta pipeline type inference currently supports these stages:

[aggregation stages]: https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/

| Stage          | Connector Version |
| -------------- | ----------------- |
| `$documents`   | 1.5.0             |
| `$group`       | 1.5.0             |
| `$limit`       | 1.5.0             |
| `$match`       | 1.5.0             |
| `$project`     | 1.5.0             |
| `$replaceRoot` | 1.5.0             |
| `$replaceWith` | 1.5.0             |
| `$skip`        | 1.5.0             |
| `$sort`        | 1.5.0             |
| `$unwind`      | 1.5.0             |

## Supported query predicate operators

Certain stages, such as `$match`, use a specific format called a **query predicate**. These are not general purpose
expressions - they are specialized for determining a match against an input document. Hasura's beta pipeline type
inference currently supports these query predicate operators:

| Query Predicate Operator                        | Connector Version |
| ----------------------------------------------- | ----------------- |
| `$all`                                          | 1.5.0             |
| `$and` / `$or` / `$nor`                         | 1.5.0             |
| `$elemMatch`                                    | 1.5.0             |
| `$eq` / `$ne` / `$gt` / `$lt` / `$gte` / `$lte` | 1.5.0             |
| `$exists`                                       | 1.5.0             |
| `$expr`                                         | 1.5.0             |
| `$in` / `$nin`                                  | 1.5.0             |
| `$mod`                                          | 1.5.0             |
| `$not`                                          | 1.5.0             |
| `$size`                                         | 1.5.0             |
| `$type`                                         | 1.5.0             |

## Supported aggregation expression operators

Aggregation pipelines use **aggregation expressions** to reference document values, or to express logic. Note that these
are not the operators typically used in the `$match` stage, but you do see these in, for example, `$replaceWith` and
`$project` stages.

Hasura's beta pipeline type inference currently supports these aggregation expression operators:

| Expression Operator                                                                                                   | Connector Version |
| --------------------------------------------------------------------------------------------------------------------- | ----------------- |
| `$abs`                                                                                                                | 1.5.0             |
| `$add` / `$divide` / `$multiply` / `$subtract`                                                                        | 1.5.0             |
| `$allElementsTrue` / `$anyElementTrue`                                                                                | 1.5.0             |
| `$and` / `$or`                                                                                                        | 1.5.0             |
| `$arrayElemAt`                                                                                                        | 1.5.0             |
| `$eq` / `$gt` / `$gte` / `$lt` / `$lte` / `$ne`                                                                       | 1.5.0             |
| `$not`                                                                                                                | 1.5.0             |
| `$sin` / `$cos` / `$tan` / `$asin` / `$acos` / `$atan` / `$asinh` / `$acosh` / `$atanh` / `$sinh` / `$cosh` / `$tanh` | 1.5.0             |
| `$split`                                                                                                              | 1.5.0             |

## Supported accumulators

Accumulators are used in stages like `$group` and `$bucket`.

Hasura's beta pipeline type inference currently supports these accumulators:

| Accumulator     | Connector Version |
| --------------- | ----------------- |
| `$avg`          | 1.5.0             |
| `$count`        | 1.5.0             |
| `$min` / `$max` | 1.5.0             |
| `$push`         | 1.5.0             |
| `$sum`          | 1.5.0             |



--- File: ../ddn-docs/docs/reference/connectors/mongodb/native-operations/vector-search.mdx ---
# Vector Search

---
sidebar_position: 6
sidebar_label: Vector Search
description:
  "Vector search allows you to run vector similarity queries on your MongoDB database. This page explains how to
  configure vector search in Hasura using native queries."
keywords:
  - native queries
  - vector search
  - vector
  - similarity
  - neighbor
  - nearest neighbor
  - atlas
seoFrontMatterUpdated: false
---

# Vector Search

Vector search allows you to run queries on vector types in your MongoDB database. This page explains how to configure
vector search in Hasura using native queries.

:::info Prerequisites

In order to run a vector search query, you need to have a MongoDB database on [Atlas][MongoDB: atlas]. You'll also need
data in the database which has been vectorized. The following example uses the tutorial found on
[MongoDB's](https://www.mongodb.com/products/platform/atlas-vector-search) site.

:::

## Example

This uses the `sample_mflix.embedded_movies` collection to perform an approximate nearest neighbor search on a vector in
the `plot_embeddings` field.

### Native Query configuration

In the `native_queries` folder of our connector configuration directory add a json file named `movieSearch.json` and
enter the following:

```json
{
  "name": "moviesSearch",
  "representation": "function",
  "inputCollection": "embedded_movies",
  "description": "Movies approximate nearest neighbor search",
  "arguments": {
    "ninGenres": {
      "type": {
        "arrayOf": {
          "scalar": "string"
        }
      }
    },
    "inGenres": {
      "type": {
        "arrayOf": {
          "scalar": "string"
        }
      }
    },
    "gteYear": {
      "type": {
        "scalar": "int"
      }
    },
    "lteYear": {
      "type": {
        "scalar": "int"
      }
    },
    "numCandidates": {
      "type": {
        "scalar": "int"
      }
    },
    "limit": {
      "type": {
        "scalar": "int"
      }
    }
  },
  "resultDocumentType": "MoviesSearch",
  "objectTypes": {
    "MoviesSearch": {
      "fields": {
        "__value": {
          "type": {
            "arrayOf": {
              "object": "MoviesSearchOutput"
            }
          }
        }
      }
    },
    "MoviesSearchOutput": {
      "fields": {
        "title": {
          "type": {
            "scalar": "string"
          }
        },
        "plot": {
          "type": {
            "scalar": "string"
          }
        },
        "genres": {
          "type": {
            "arrayOf": {
              "scalar": "string"
            }
          }
        },
        "year": {
          "type": {
            "scalar": "int"
          }
        },
        "score": {
          "type": {
            "scalar": "double"
          }
        }
      }
    }
  },
  "pipeline": [
    {
      "$vectorSearch": {
        "index": "vector-search-tutorial",
        "path": "plot_embedding",
        "filter": {
          "$and": [
            {
              "genres": {
                "$nin": "{{ ninGenres }}",
                "$in": "{{ inGenres }}"
              }
            },
            {
              "year": {
                "$gte": "{{ gteYear }}",
                "$lte": "{{ lteYear }}"
              }
            }
          ]
        },
        "queryVector": [
          -0.020156775, -0.024996493, 0.010778184, -0.030058576, -0.03309321, 0.0031229265, -0.022772837, 0.0028351594,
          0.00036870153, -0.02820117, 0.016245758, 0.0036232488, 0.0020519753, -0.0076454473, 0.0073380596,
          -0.007377301, 0.039267123, -0.013433489, 0.01428371, -0.017279103, -0.028358135, 0.0020160044, 0.00856761,
          0.009653277, 0.0107912645, -0.026683854, 0.009594415, -0.020182934, 0.018077003, -0.015709465, 0.003310956,
          0.0014878864, -0.015971072, -0.002411684, -0.029561523, -0.030450987, -0.013106481, -0.005385822,
          -0.018652538, 0.012642129, -0.005189617, 0.018835662, -0.0048102876, -0.0261214, -0.016167276, -0.007972456,
          0.0023381072, -0.010058766, -0.009012341, 0.008358325, 0.018665617, 0.02163485, -0.012975678, -0.010745483,
          -0.002571918, -0.014479915, 0.007226877, 0.015003128, 0.013165343, -0.028279653, 0.0053727417, -0.020588424,
          -0.017383745, 0.023518417, 0.01262905, -0.011922712, 0.007638907, -0.0073249796, -0.014859244, -0.00001101736,
          0.017043658, 0.010111088, 0.0074623227, 0.009555174, 0.008338705, -0.002240005, -0.0010603234, -0.004973792,
          0.003391073, 0.021543289, 0.013341927, 0.0005980159, 0.010693162, 0.005336771, 0.016062634, 0.005768421,
          0.005186347, 0.039790336, 0.0021942237, -0.0026275094, 0.010431555, 0.0042151334, -0.0050359233, 0.025768232,
          -0.021451725, 0.01833861, -0.01836477, -0.013433489, 0.030006256, -0.014793842, 0.017475309, 0.0020585153,
          -0.012975678, -0.017266022, -0.01593183, -0.014257549, 0.0010676811, -0.007887433, -0.0045911926,
          0.00012303676, -0.0014976967, 0.03552615, 0.0065630507, -0.037435878, 0.011929252, -0.00939167, 0.016768971,
          0.01223664, 0.007789331, -0.037200432, 0.013145722, 0.00896002, 0.021857215, 0.010333453, 0.021582529,
          -0.007089534, -0.007154935, -0.02485261, 0.0040254686, -0.00088864425, 0.023466095, -0.020719228,
          -0.006690584, -0.021006994, -0.018286288, 0.025545865, -0.0096598165, 0.008803056, -0.023021365, -0.040078104,
          0.015408617, 0.017043658, -0.011242535, 0.0063537657, -0.026618453, 0.0071614753, -0.014623798, 0.00067322777,
          -0.00083427917, -0.028070368, 0.03714811, -0.004529061, 0.0054087127, 0.0028727653, 0.008384486, 0.010026066,
          -0.006190262, -0.0002493436, 0.0029953935, -0.026226042, -0.018417092, 0.009941043, 0.0036494094, -0.00982332,
          0.013551212, 0.02574207, -0.0022645304, -0.0006004685, 0.012805633, -0.024303235, 0.008194821, -0.014179068,
          -0.02977081, 0.003095131, -0.0015941641, 0.029953934, 0.0052680993, 0.025388902, -0.031392768, -0.021386323,
          0.014898485, 0.022419669, 0.00897964, 0.013243824, 0.006854088, 0.0066415328, -0.003839074, -0.01877026,
          0.021216279, -0.015055449, -0.0015508354, 0.013211124, -0.008783435, 0.0052157775, -0.68938524, -0.01221702,
          -0.04125533, -0.016232677, 0.020039052, -0.0026422248, -0.0037050007, 0.0064682183, -0.0047579664,
          0.0032749851, -0.0035382267, 0.031942144, -0.00035643874, -0.011628405, -0.043086577, -0.0196074,
          -0.0066088317, -0.014872325, 0.028331975, 0.010294212, -0.013930541, 0.031994462, -0.018626377, 0.017462227,
          0.026343765, -0.010274592, 0.0046827546, -0.029430721, -0.011746128, 0.0024362097, 0.0023054064, 0.0027730279,
          -0.002406779, 0.003917556, 0.059436977, 0.008665713, -0.0018901062, 0.06037876, 0.017880797, 0.05185039,
          0.0067102043, -0.020300657, 0.005604917, 0.018704858, 0.012073136, 0.0144145135, 0.012413224, -0.0074819434,
          0.015801027, -0.0061412104, 0.008613391, -0.0039077457, -0.0036232488, 0.008469507, 0.014087505, 0.0124066835,
          0.019267311, -0.002573553, 0.005055544, -0.009417831, -0.009103903, 0.011150973, -0.012046975, 0.0058567133,
          -0.0053727417, 0.018260127, -0.005588567, 0.015591742, 0.007495024, -0.02567667, 0.024211673, 0.021386323,
          -0.012890656, -0.016114954, 0.009515933, 0.009679437, 0.025532786, -0.0076454473, -0.02575515, 0.008319084,
          -0.0068410076, -0.017082898, -0.026173722, -0.0049901423, 0.01918883, -0.008646091, -0.031759016, 0.014820003,
          0.011850771, 0.01836477, 0.012700991, -0.0011437106, 0.005058814, 0.0151993325, -0.0060692686, 0.027416352,
          0.0037344315, 0.0013546307, 0.018325528, -0.03152357, -0.008809595, 0.014649959, -0.008345244, 0.0066415328,
          -0.005523165, 0.0043492066, -0.0015892589, 0.0048855, 0.034453563, -0.03837766, 0.0068410076, -0.0042151334,
          -0.0067429054, 0.0055689462, -0.011733048, -0.0212032, 0.016847452, -0.0022220195, 0.0059351954, -0.00449963,
          0.02251123, -0.01020265, 0.023361452, -0.0032455544, 0.016180357, 0.0049443613, -0.0064747585, -0.03259616,
          0.012321662, 0.020104453, 0.009954124, -0.019411195, 0.0048102876, -0.000392614, 0.012184318, 0.0044276887,
          0.005634348, -0.020562263, 0.015722545, -0.005179807, -0.0067952266, 0.0027861083, 0.0024198592,
          -0.0020585153, 0.0018525004, -0.045100946, -0.010176489, -0.012956058, 0.0013497255, 0.0105361985,
          0.003796563, -0.0106016, -0.013126101, 0.0050359233, 0.015003128, -0.0075800456, -0.015722545, -0.01755379,
          -0.00978408, -0.02940456, 0.017606111, 0.016612006, -0.016912855, 0.025441224, 0.0054741143, 0.00448001,
          0.009470152, 0.015382457, -0.008332164, -0.019123428, 0.024564842, 0.016860534, 0.008286383, -0.007141855,
          0.006559781, 0.016625088, -0.01840401, -0.011602244, -0.00489858, -0.0073184394, -0.008809595, -0.0018459603,
          -0.01629808, -0.005542786, 0.0064257076, 0.010379234, 0.014663039, 0.034872133, -0.013355007, 0.027285548,
          0.011654565, -0.004032009, 0.02323065, -0.02653997, -0.0009941043, 0.002946342, 0.010667001, 0.008345244,
          0.018626377, 0.04821406, 0.031392768, 0.010281132, 0.026069079, 0.002735422, 0.01182461, -0.01593183,
          0.006585941, -0.010071847, 0.024564842, -0.0025261368, 0.004293615, -0.0068606283, -0.0066448026,
          -0.0074100015, -0.0014347476, 0.021530207, -0.010418476, 0.018495573, -0.0034924455, -0.014165987,
          -0.004784127, -0.012472086, 0.004417878, -0.0030313642, -0.010084927, -0.010954768, 0.01508161, 0.0010047321,
          0.0042347535, -0.03345946, -0.00027346043, 0.014793842, -0.019882087, 0.012772933, 0.021490967, 0.0031932332,
          0.0093589695, 0.00090172456, 0.0048102876, 0.0070045115, -0.0045584915, 0.015840268, 0.024342475,
          -0.0091300635, 0.0039796876, 0.003796563, 0.025022654, -0.008103259, -0.025022654, 0.03021554, -0.008201361,
          -0.0070502926, 0.0011821339, 0.021072397, 0.004849529, -0.02495725, 0.012184318, 0.0019228071, -0.007226877,
          0.020562263, 0.018861823, -0.0017593032, 0.01345965, 0.0022727058, 0.003023189, -0.026971621, -0.0030558899,
          0.017723834, -0.01998673, -0.010608139, 0.011491061, -0.025179617, 0.0069652707, 0.003924096, 0.021177039,
          0.0045650317, -0.0009973744, 0.007586586, -0.004032009, -0.008129419, -0.010091467, -0.04279881, 0.019790525,
          0.01595799, 0.0044309585, -0.0033747226, -0.018665617, -0.012818714, -0.016206518, 0.014113666, -0.0020912162,
          0.01427063, -0.020248337, -0.0112752365, -0.020588424, -0.011039791, 0.008744194, -0.015147011, 0.0022269245,
          -0.010438096, -0.0017772885, -0.028750544, -0.008861917, -0.016991336, 0.033668745, 0.034636687, 0.009888723,
          0.0023953337, 0.006991431, -0.003346927, 0.003103306, -0.0044571194, 0.011249076, 0.0033779927, 0.00012446742,
          -0.0027027212, -0.025859794, -0.011942333, 0.02694546, 0.028227331, 0.0064289775, -0.03385187, -0.020719228,
          0.00489531, 0.10663077, 0.041752383, -0.021700252, -0.008103259, 0.0049574412, -0.01675589, -0.020182934,
          -0.006585941, 0.007684688, -0.002859685, 0.027023941, 0.00856107, 0.0037017306, 0.016978256, 0.025885954,
          -0.010372694, 0.0025964435, 0.011706887, 0.021360163, -0.021674091, -0.024983412, 0.0034074234, 0.0032030435,
          0.022262705, -0.01266829, -0.002249815, 0.032779284, -0.0034303141, -0.016101874, -0.005156916, -0.0212032,
          0.005362931, 0.009077743, -0.013917461, -0.0017315074, 0.010980929, -0.019450437, 0.013865139, 0.028227331,
          -0.008757275, -0.0033649125, -0.012857955, 0.011039791, 0.009764459, 0.00029594224, -0.026317604, 0.025048813,
          0.037749805, -0.025807472, -0.005425063, 0.021791814, -0.010012985, -0.00066995766, -0.016952096,
          0.0031147513, -0.016598927, 0.0084368065, 0.004787397, -0.0064355177, 0.0015164997, -0.021216279,
          -0.023845425, 0.013969782, -0.011255615, 0.0042576445, -0.024250913, -0.009908343, -0.02289056, -0.023361452,
          -0.010987469, -0.013394248, 0.0032553647, -0.019018786, 0.021438645, 0.029587684, -0.010490417, 0.01263559,
          -0.018417092, -0.008731114, 0.01875718, -0.0072399573, -0.029090632, -0.017736914, -0.04031355, -0.019712042,
          0.012772933, -0.030320182, -0.022341188, -0.02041838, 0.011752668, 0.028829027, -0.017043658, 0.024996493,
          0.006334145, -0.0024263994, -0.0077370093, 0.017802317, 0.017396826, 0.030398665, 0.011464901, 0.03016322,
          -0.014558396, -0.0036690298, -0.009954124, -0.006703664, -0.00035705187, -0.014519156, 0.0075342646,
          -0.00896656, 0.040078104, 0.024420958, -0.016886694, -0.00092543266, -0.0017494928, 0.01672973, 0.016533526,
          0.002648765, 0.0187441, -0.0055460557, 0.004735076, 0.03186366, 0.0003435628, 0.007495024, 0.023453014,
          -0.012504786, -0.0074557825, -0.0027844731, -0.04570264, 0.010477337, 0.0030101088, -0.015670223, 0.03351178,
          -0.020261416, 0.00050849747, -0.009653277, -0.023466095, -0.007396921, -0.011909632, 0.003436854, -0.02979697,
          -0.039031677, -0.014584557, 0.0019555078, 0.0042216736, -0.0060594585, -0.023400694, -0.00023462824,
          -0.017763074, -0.016180357, 0.0132372845, -0.020496862, -0.007390381, -0.0058697937, -0.0096598165,
          0.0039796876, -0.019306554, -0.012622509, -0.0012287326, 0.010863206, 0.024368636, 0.027730279, 0.016795132,
          0.019908248, -0.006343955, 0.0014592733, -0.005425063, 0.019450437, 0.004532331, -0.031889822, 0.008476048,
          0.019712042, -0.00047906674, -0.0028286192, 0.011883471, -0.012426305, 0.0041497317, 0.001756033,
          -0.0013603533, -0.008031317, -0.010281132, -0.0071222344, -0.026330685, -0.007920134, -0.026866978,
          -0.03026786, -0.0015328501, 0.027442513, -0.005922115, 0.005186347, 0.003436854, 0.036703378, -0.0053204205,
          0.013165343, 0.0016939015, -0.0041431915, -0.017213702, -0.012439385, -0.015212413, 0.014532236, 0.0093589695,
          -0.0053400407, 0.017422987, -0.028881347, -0.014179068, 0.011307937, 0.040104263, -0.007593126, -0.000631943,
          -0.0003404971, -0.0055198953, -0.00063030794, -0.004852799, -0.0024214943, -0.029718488, 0.023322212,
          0.011079031, 0.012988758, 0.0071614753, -0.034034993, -0.01551326, 0.004012388, 0.006442058, 0.032386873,
          0.0076519875, 0.0465921, 0.01757995, -0.0135381315, -0.016978256, 0.024983412, 0.0003280299, 0.0026209692,
          0.022380428, -0.010640841, 0.0027648527, -0.007959375, -0.005922115, 0.0075342646, -0.03597088, -0.018874902,
          0.03510758, -0.015356296, 0.004597733, -0.0015328501, -0.019947488, -0.013446569, 0.020614585, -0.0056016473,
          0.035186063, 0.0005248479, -0.030712591, -0.019136509, 0.004202053, -0.010339993, 0.014754602, 0.0072922786,
          -0.015460939, 0.027494833, -0.02974465, -0.0033616424, 0.0105819795, -0.028881347, 0.01720062, -0.0073707607,
          0.0054479535, -0.0019522378, -0.018103164, -0.009110443, -0.024630243, 0.005624538, 0.01879642, -0.019345794,
          -0.0027681228, -0.015971072, 0.022354268, -0.0038194535, 0.018901063, -0.017357586, -0.02493109, 0.006703664,
          -0.0021173768, -0.005667049, -0.004535601, -0.016441964, 0.0034172337, -0.02447328, -0.003310956, -0.02078463,
          -0.011589164, 0.013263445, -0.014728441, -0.0187441, -0.019476596, 0.013224204, 0.015238573, -0.012380524,
          0.00019058435, 0.010778184, 0.025022654, -0.036127847, 0.01470228, -0.007671608, 0.032857765, 0.002982313,
          0.009829861, 0.0072203367, -0.0028237142, 0.025990596, -0.029012151, 0.0016955365, 0.012033895, -0.0049901423,
          -0.013629694, 0.0072464976, 0.0012704261, 0.0018868363, 0.017043658, 0.00448001, -0.009555174, -0.016520444,
          0.02570283, -0.00939167, 0.01998673, 0.002001289, -0.023662299, 0.0041072206, -0.024839528, -0.007396921,
          -0.0034793653, -0.032020625, -0.0036003583, -0.010719323, 0.022995204, -0.01757995, -0.0043851775,
          -0.023884665, -0.018430172, -0.009018881, 0.00091562246, -0.0055689462, -0.012537487, 0.016455043, 0.03264848,
          0.018560974, 0.014623798, 0.0025555675, -0.0060986993, 0.0058272826, -0.008462967, -0.012720612,
          -0.0042576445, -0.027207067, 0.014152907, -0.0029610575, 0.010241891, -0.011222915, -0.01140604, -0.022197304,
          -0.003433584, -0.0056899395, 0.004372097, 0.061896075, -0.005846903, -0.011863851, 0.004535601, -0.0074819434,
          0.016847452, -0.0012647035, 0.021085477, 0.02409395, -0.030137058, -0.0012197399, 0.009607496, -0.008220982,
          -0.007893973, -0.007893973, 0.007972456, 0.010012985, 0.009143144, 0.0044734697, 0.015264734, -0.0032520946,
          0.002208939, 0.011968493, -0.0012998568, -0.0114322, -0.056454662, -0.013217663, 0.0017593032, -0.00244275,
          -0.021399405, -0.010732403, 0.00694565, 0.0033207664, 0.0025539326, 0.01102671, -0.012589809, 0.010706242,
          -0.012413224, 0.01427063, -0.000049970913, -0.0056016473, 0.027965724, 0.018652538, -0.009535554,
          0.0068867886, 0.004699105, -0.001245083, -0.009071202, -0.0032946058, -0.03756668, 0.034453563, -0.00408106,
          0.013361547, -0.0065107294, 0.009300108, -0.016415803, 0.0059973267, -0.017422987, 0.0048822295, 0.022158062,
          -0.025611266, 0.01022227, -0.0061771814, -0.014218308, -0.00044636594, -0.019110348, -0.013747416,
          -0.013629694, -0.021896457, -0.0051634563, -0.020509942, -0.018731019, 0.0043328563, -0.032386873,
          -0.023086766, 0.0196074, 0.20614585, -0.014649959, -0.009712138, 0.01345965, -0.010928608, 0.0196074,
          0.015814107, 0.017383745, -0.0024656404, 0.021399405, 0.013668935, -0.0063864663, -0.0015303975,
          -0.0012924991, -0.0030575248, -0.015539421, -0.009692517, -0.012190859, -0.02287748, 0.002936532,
          0.00069325697, 0.013158802, -0.0070110518, -0.013629694, 0.01585335, -0.019829765, 0.013747416, 0.016036473,
          0.011693806, 0.0071483953, -0.010156869, -0.013799738, -0.00034703725, -0.010706242, -0.02289056,
          0.0039339066, -0.0015835363, -0.014532236, 0.012445925, -0.00009779583, 0.0053335004, 0.0055329753,
          -0.005281179, -0.007475403, 0.00040385488, -0.012942977, -0.015277814, 0.012956058, 0.00006162057,
          0.007056833, -0.02571591, -0.018731019, -0.0061771814, 0.034427404, 0.0010570535, 0.0079528345, 0.024172433,
          0.021386323, -0.019803606, -0.006821387, -0.011262156, 0.026605371, -0.0036951904, -0.008207901, -0.019698963,
          0.042981934, -0.026212962, 0.00856761, 0.015173172, 0.0024149541, -0.0008036222, -0.005752071, -0.02898599,
          -0.008443347, -0.0064224373, -0.014479915, 0.036467932, -0.00086820626, 0.026396086, 0.002001289,
          -0.0074361623, -0.0086918725, -0.007835112, 0.021464806, 0.0008984545, -0.02489185, 0.019515838, 0.026644614,
          -0.0137212565, 0.00448982, 0.004211863, -0.022380428, -0.014100585, -0.01629808, 0.0074884836, 0.02652689,
          0.011634945, 0.049626734, -0.023583818, -0.0021958589, -0.015735626, 0.02733787, 0.0036428692, -0.031261966,
          -0.012674831, 0.006196802, -0.009535554, 0.016886694, 0.010771644, -0.021490967, 0.014100585, -0.007063373,
          0.00043778197, -0.012151618, -0.0058894143, 0.009182385, -0.005768421, -0.013995943, 0.004725266, -0.01347273,
          -0.020797709, -0.018037762, 0.020274498, 0.011595704, 0.0017364125, -0.02248507, 0.005954816, 0.0062196925,
          -0.014257549, -0.025127295, 0.015356296, 0.005179807, 0.021726413, -0.0034499345, -0.017082898, 0.019803606,
          0.005209238, 0.0005939283, -0.0035807376, -0.011661106, 0.006559781, 0.0033207664, 0.0017233322,
          -0.00059924216, -0.000341519, -0.0140221035, 0.00084286317, -0.003306051, -0.005634348, -0.00816212,
          -0.009319728, -0.024447119, -0.014950806, -0.024564842, 0.0137212565, -0.010084927, 0.000044886958,
          -0.0033943432, 0.0025359471, 0.012478625, -0.023086766, 0.014519156, 0.020876192, -0.023282971, -0.0030804155,
          -0.014545316, -0.16805595, 0.01262905, 0.020719228, -0.012413224, 0.026592292, -0.0024198592, 0.041072205,
          0.002658575, -0.013708176, -0.0068867886, -0.0018639456, 0.000031627806, -0.043452825, -0.028018046,
          -0.0105819795, 0.01266829, -0.009450532, 0.008292923, 0.0058534434, -0.006782146, 0.032229908, 0.0005955633,
          -0.0023103117, 0.003140912, 0.00037687673, -0.0049247406, -0.008070557, 0.017279103, -0.012759852,
          -0.011608784, -0.019450437, 0.016167276, 0.02248507, 0.030529467, 0.015905669, 0.0061150496, -0.016834373,
          0.017344505, 0.006667693, -0.005461034, 0.0066742334, 0.01998673, 0.024591003, -0.007717389, 0.0096598165,
          0.03225607, 0.018626377, -0.020248337, 0.0017740185, 0.012589809, 0.0014927916, -0.040235065, 0.01713522,
          0.016206518, 0.017776156, 0.024734886, 0.0040516295, -0.009627116, 0.002001289, -0.010496957, -0.0121058365,
          -0.017266022, 0.008279843, -0.02122936, -0.01349889, -0.02251123, 0.004820098, -0.000071533, -0.022628954,
          0.015238573, -0.01833861, -0.016572766, -0.0031523572, -0.008064018, 0.019973649, 0.0089207785, -0.03228223,
          0.0040647094, -0.004784127, -0.0017920039, -0.0013775212, 0.047246117, 0.0030804155, -0.010660461, 0.02982313,
          0.006088889, -0.019371955, -0.024447119, -0.011687267, -0.013708176, 0.017187541, -0.018286288, 0.019267311,
          0.0011960318, 0.0046271635, 0.016886694, 0.0069129495, 0.00029062838, 0.013629694, -0.016494283, -0.017069818,
          0.0058240127, 0.013943622, 0.001675916, 0.01347273, 0.023335291, 0.008129419, 0.0047187256, 0.032099105,
          0.0007701039, 0.0068344674, 0.0004672127, -0.00610851, 0.026396086, -0.010738943, 0.024591003, 0.008220982,
          -0.019908248, 0.024682565, -0.009404751, 0.0594893, -0.009731758, -0.022628954, 0.013865139, -0.016049553,
          0.0033371167, -0.107572556, -0.022341188, 0.008050937, -0.0089731, 0.004983602, 0.010771644, -0.013034539,
          -0.013368088, -0.0071287747, 0.0091758445, -0.017409906, -0.022118822, -0.011170594, -0.010908987,
          0.050490037, 0.014584557, 0.018312449, 0.0014968792, -0.0057161, 0.024342475, -0.02699778, 0.020091372,
          -0.00094587065, -0.021347083, -0.003711541, 0.0016677409, -0.030738752, 0.040208906, 0.008109799,
          -0.017527629, -0.0009058122, 0.017776156, 0.0052779093, -0.0046206233, 0.0067952266, -0.01226934,
          -0.009162764, -0.01595799, 0.021582529, -0.027390191, -0.00011210243, -0.003145817, 0.01672973, -0.009999905,
          0.003832534, -0.01793312, -0.0004868332, 0.027573315, 0.001756033, -0.012112376, -0.009718678, 0.0025473924,
          -0.027547155, -0.019084187, 0.010693162, 0.025558947, -0.02168717, -0.0068802484, -0.010869746, -0.028698223,
          -0.0051634563, -0.012131997, -0.014963887, 0.022210384, 0.01510777, -0.0026504, -0.013577373, 0.0058599836,
          0.011281776, -0.0009393305, -0.00204053, 0.030110897, -0.029326078, 0.006491109, -0.01671665, 0.0006049648,
          -0.024342475, -0.008325624, 0.03722659, -0.007710849, -0.0055656764, -0.02043146, -0.015317055, -0.015212413,
          0.002815539, 0.022262705, 0.00818828, 0.021778734, -0.0037409717, -0.02485261, 0.0033779927, 0.013217663,
          -0.0059319255, -0.018940303, 0.02409395, 0.015761785, -0.009672897, 0.011301396, -0.011582624, 0.0029725027,
          -0.015343216, -0.00735114, -0.075761214, 0.016821291, 0.0028040938, 0.0017233322, 0.01595799, -0.0054741143,
          -0.007096074, -0.011641486, -0.003554577, 0.009829861, -0.037828285, 0.024983412, 0.003793293, -0.010895907,
          -0.011916172, -0.017893879, 0.029640006, 0.0027452323, 0.004977062, 0.0138913, 0.0132830655, 0.010725862,
          0.014205228, -0.003839074, 0.020470701, 0.0048626093, -0.010967849, 0.035343025, -0.004568302, -0.007665068,
          0.0040091183, -0.02367538, -0.006821387, 0.012112376, -0.0012475356, -0.02041838, -0.030869557, -0.004865879,
          0.036127847, 0.019528918, 0.00087147637, 0.0016366751, -0.006072539, -0.012380524, -0.016886694, 0.0014224849,
          0.0058632535, 0.0053138803, 0.024525601, -0.008227522, 0.016167276, 0.021373244, -0.019855926, -0.011602244,
          -0.012223559, 0.009116983, 0.00448001, 0.0027027212, 0.0112294555, -0.025048813, 0.005958086, 0.005578757,
          0.012040435, -0.019528918, -0.008096718, -0.023439934, 0.00047497914, 0.0073315194, 0.025061894, -0.016455043,
          0.003992768, 0.002038895, -0.0003484679, 0.004444039, -0.014846164, 0.0018263398, 0.017305264, -0.0047154557,
          -0.006729825, 0.011288317, -0.009764459, -0.03220375, -0.015369376, 0.009594415, 0.031078842, 0.020967754,
          -0.007802411, 0.022354268, -0.010778184, 0.01833861, 0.004581382, 0.0072399573, 0.010673542, -0.012112376,
          -0.023073684, 0.0066448026, -0.027887244, 0.0063504954, 0.012956058, 0.032151427, -0.018103164, 0.0048855,
          -0.018286288, -0.036938824, -0.012354363, 0.020039052, 0.004921471, -0.03790677, 0.0212686, 0.02982313,
          0.015434778, 0.0041039507, -0.016245758, 0.012171238, -0.006415897, 0.0072464976, -0.0024362097, -0.025218857,
          -0.021399405, 0.036860343, 0.0056572384, 0.017004417, 0.03432276, -0.013825899, 0.028724384, 0.008528369,
          0.018652538, -0.02443404, -0.025637427, 0.006497649, -0.015447859, 0.01917575, -0.016520444, -0.008678793,
          -0.021072397, 0.015840268, -0.006324335, 0.025925195, -0.03594472, 0.0384823, 0.01308032, 0.0054217926,
          0.00448328, -0.027207067, -0.016847452, 0.0036003583, 0.01061468, -0.019816685, -0.004659864, 0.023387613,
          -0.005461034, 0.004326316, 0.0037278912, -0.007540805, 0.00860031, 0.0015524705, 0.020039052, -0.0028367946,
          0.0049509015, 0.009162764, 0.009705598, 0.013982862, 0.004852799, 0.0061869915, -0.0083910255, 0.012975678,
          -0.034558207, -0.029064473, -0.03058179, -0.019450437, 0.01062122, -0.014179068, -0.010012985, 0.007874353,
          -0.014126746, -0.009731758, -0.03398267, -0.000115883464, -0.0029725027, -0.024290156, 0.012864495,
          -0.00937859, -0.035264544, 0.0027959184, 0.012982218, -0.012609429, 0.0065270797, 0.010712783
        ],
        "numCandidates": "{{ numCandidates }}",
        "limit": "{{ limit }}"
      }
    },
    {
      "$project": {
        "_id": 0,
        "title": 1,
        "genres": 1,
        "plot": 1,
        "year": 1,
        "score": {
          "$meta": "vectorSearchScore"
        }
      }
    },
    {
      "$group": {
        "_id": null,
        "__value": {
          "$addToSet": {
            "title": "$title",
            "genres": "$genres",
            "plot": "$plot",
            "year": "$year",
            "score": "$score"
          }
        }
      }
    },
    {
      "$project": {
        "_id": 0,
        "__value": 1
      }
    }
  ]
}
```

### Usage

Now you can use the flexibility of Hasura's GraphQL API to search your vectors in many ways.

#### Search movies

```graphql
query MyQuery {
  moviesSearch(
    ninGenres: ["Drama", "Western", "Crime"]
    inGenres: ["Action", "Adventure", "Family"]
    gteYear: 1960
    lteYear: 2000
    numCandidates: 200
    limit: 2
  ) {
    genres
    plot
    score
    year
    title
  }
}
```

[MongoDB: atlas]: https://www.mongodb.com/atlas/database



--- File: ../ddn-docs/docs/reference/connectors/mysql/index.mdx ---
# MySQL

---
title: MySQL
sidebar_position: 1
description:
  "Learn how to configure the MySQL connector and utilize native operations to extend your API's capability."
sidebar_label: MySQL
keywords:
  - mysql
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# MySQL

## Introduction

Hasura DDN includes a Native Data Connector for MySQL, providing integration with MySQL databases. This connector
allows you to leverage MySQLâ€™s powerful relational database capabilities while taking advantage of Hasuraâ€™s
metadata-driven approach. Here, weâ€™ll explore the key features of the MySQL connector and walk through the
configuration process within a Hasura DDN project.

## MySQL docs

- [Connector configuration](/reference/connectors/mysql/configuration.mdx)



--- File: ../ddn-docs/docs/reference/connectors/mysql/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura MySQL connector, including connection URI details, and
  native queries."
keywords:
  - mysql
  - configuration
---

# Configuration Reference

## Introduction

The configuration is a metadata object that lists all the database entities â€” such as tables â€” that the data connector
has to know about in order to serve queries. It never changes during the lifetime of the data connector service
instance. When your database schema changes you will have to update the configuration accordingly, see
[updating with introspection](#updating-with-introspection).

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "jdbcUrl": "",
  "jdbcProperties": {},
  "schemas": [],
  "tables": [],
  "functions": [],
  "nativeQueries": {}
}
```

### Property: JDBC URL

The JDBC connection URL to connect to the MySQL database. This is a required field.

The value can either be a literal string, or a reference to an environment variable:

```json
{
  "jdbcUrl": "jdbc:MySQL:thin:@//localhost:1521/xe?user=foo&password=bar",
  "jdbcUrl": { "variable": "MYSQL_JDBC_URL" }
}
```

### Property: JDBC Properties

This is a JSON object containing key-value pairs of additional properties to be passed to the JDBC driver. For example,
with MySQL to enable running multiple statements in a given query:

```json
{
  "jdbcProperties": { "allowMultiQueries": "true" }
}
```

### Property: Schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included.

Example:

```json
{
  "schemas": ["public", "other_schema"]
}
```

### Property: Tables

This is an array of table definitions, generated automatically during introspection.

Example:

```json
{
  "tableName": "Album",
  "tableType": "TABLE",
  "description": "",
  "columns": [
    {
      "name": "AlbumId",
      "description": "",
      "type": "int",
      "numeric_scale": 0,
      "nullable": false,
      "auto_increment": true,
      "is_primarykey": true
    },
    {
      "name": "Title",
      "description": "",
      "type": "varchar",
      "numeric_scale": null,
      "nullable": false,
      "auto_increment": false,
      "is_primarykey": false
    },
    {
      "name": "ArtistId",
      "description": "",
      "type": "int",
      "numeric_scale": 0,
      "nullable": false,
      "auto_increment": false,
      "is_primarykey": false
    }
  ],
  "pks": ["AlbumId"],
  "fks": {
    "FK_AlbumArtistId": {
      "foreign_collection": "Artist",
      "column_mapping": {
        "ArtistId": "ArtistId"
      }
    }
  }
}
```

### Property: Functions

This is an array of function definitions.

Example:

```json
{
  "function_catalog": "public",
  "function_schema": "public",
  "function_name": "add",
  "argument_signature": "(N NUMBER, M NUMBER)",
  "data_type": "TABLE (N NUMBER, M NUMBER)",
  "comment": "Adds two numbers"
}
```

### Property: Native Queries

This is a JSON object containing key-value pairs of Native Queries to be used in the data connector.

Two types of Native Queries are supported: **Inline** and **Parameterized**.

Example:

```json
{
  "native_query_inline": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT 1 AS result FROM DUAL"
        }
      ]
    },
    "columns": {
      "result": {
        "type": "named",
        "name": "INT"
      }
    },
    "arguments": {},
    "description": ""
  },
  "ArtistById_parameterized": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT * FROM CHINOOK.ARTIST WHERE ARTISTID = "
        },
        {
          "type": "parameter",
          "value": "ARTISTID"
        }
      ]
    },
    "columns": {
      "ARTISTID": {
        "type": "named",
        "name": "INT"
      },
      "NAME": {
        "type": "nullable",
        "underlying_type": {
          "type": "named",
          "name": "STRING"
        }
      }
    },
    "arguments": {
      "ARTISTID": {
        "description": null,
        "type": {
          "type": "named",
          "name": "INT"
        }
      }
    },
    "description": null,
    "isProcedure": false
  }
```

## Updating with introspection

Whenever the schema of your database changes you will need to update your data connector configuration accordingly to
reflect those changes.

Running `update` in a configuration directory will do the following:

- Connect to the database with the specified `jdbcUrl`, and then overwrite all data in the `tables` field

- Fill in default values for any fields absent from the configuration



--- File: ../ddn-docs/docs/reference/connectors/oracle/index.mdx ---
# Oracle

---
title: Oracle
sidebar_position: 1
description:
  "Learn how to configure the Oracle connector and utilize native operations to extend your API's capability."
sidebar_label: Oracle
keywords:
  - oracle
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# Oracle

## Introduction

Hasura DDN includes a Native Data Connector for Oracle, providing integration with Oracle databases. This connector
allows you to leverage Oracleâ€™s powerful relational database capabilities while taking advantage of Hasuraâ€™s
metadata-driven approach. Here, weâ€™ll explore the key features of the Oracle connector and walk through the
configuration process within a Hasura DDN project.

## Oracle docs

- [Connector configuration](/reference/connectors/oracle/configuration.mdx)



--- File: ../ddn-docs/docs/reference/connectors/oracle/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura Oracle connector, including connection URI details, and
  native queries."
keywords:
  - oracle
  - configuration
---

# Configuration Reference

## Introduction

The configuration is a metadata object that lists all the database entities â€” such as tables â€” that the data connector
has to know about in order to serve queries. It never changes during the lifetime of the data connector service
instance. When your database schema changes you will have to update the configuration accordingly, see
[updating with introspection](#updating-with-introspection).

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "jdbcUrl": "",
  "jdbcProperties": {},
  "schemas": [],
  "tables": [],
  "functions": [],
  "nativeQueries": {}
}
```

### Property: JDBC URL

The JDBC connection URL to connect to the Oracle database. This is a required field.

The value can either be a literal string, or a reference to an environment variable:

```json
{
  "jdbcUrl": "jdbc:oracle:thin:@//localhost:1521/xe?user=foo&password=bar",
  "jdbcUrl": { "variable": "ORACLE_JDBC_URL" }
}
```

### Property: JDBC Properties

This is a JSON object containing key-value pairs of additional properties to be passed to the JDBC driver. For example,
with MySQL to enable running multiple statements in a given query:

```json
{
  "jdbcProperties": { "allowMultiQueries": "true" }
}
```

### Property: Schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included.

Example:

```json
{
  "schemas": ["public", "other_schema"]
}
```

### Property: Tables

This is an array of table definitions, generated automatically during introspection.

Example:

```json
{
  "tableName": "Album",
  "tableType": "TABLE",
  "description": "",
  "columns": [
    {
      "name": "AlbumId",
      "description": "",
      "type": "int",
      "numeric_scale": 0,
      "nullable": false,
      "auto_increment": true,
      "is_primarykey": true
    },
    {
      "name": "Title",
      "description": "",
      "type": "varchar",
      "numeric_scale": null,
      "nullable": false,
      "auto_increment": false,
      "is_primarykey": false
    },
    {
      "name": "ArtistId",
      "description": "",
      "type": "int",
      "numeric_scale": 0,
      "nullable": false,
      "auto_increment": false,
      "is_primarykey": false
    }
  ],
  "pks": ["AlbumId"],
  "fks": {
    "FK_AlbumArtistId": {
      "foreign_collection": "Artist",
      "column_mapping": {
        "ArtistId": "ArtistId"
      }
    }
  }
}
```

### Property: Functions

This is an array of function definitions.

Example:

```json
{
  "function_catalog": "public",
  "function_schema": "public",
  "function_name": "add",
  "argument_signature": "(N NUMBER, M NUMBER)",
  "data_type": "TABLE (N NUMBER, M NUMBER)",
  "comment": "Adds two numbers"
}
```

### Property: Native Queries

This is a JSON object containing key-value pairs of Native Queries to be used in the data connector.

Two types of Native Queries are supported: **Inline** and **Parameterized**.

Example:

```json
{
  "native_query_inline": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT 1 AS result FROM DUAL"
        }
      ]
    },
    "columns": {
      "result": {
        "type": "named",
        "name": "INT"
      }
    },
    "arguments": {},
    "description": ""
  },
  "ArtistById_parameterized": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT * FROM CHINOOK.ARTIST WHERE ARTISTID = "
        },
        {
          "type": "parameter",
          "value": "ARTISTID"
        }
      ]
    },
    "columns": {
      "ARTISTID": {
        "type": "named",
        "name": "INT"
      },
      "NAME": {
        "type": "nullable",
        "underlying_type": {
          "type": "named",
          "name": "STRING"
        }
      }
    },
    "arguments": {
      "ARTISTID": {
        "description": null,
        "type": {
          "type": "named",
          "name": "INT"
        }
      }
    },
    "description": null,
    "isProcedure": false
  }
```

## Updating with introspection

Whenever the schema of your database changes you will need to update your data connector configuration accordingly to
reflect those changes.

Running `update` in a configuration directory will do the following:

- Connect to the database with the specified `jdbcUrl`, and then overwrite all data in the `tables` field

- Fill in default values for any fields absent from the configuration



--- File: ../ddn-docs/docs/reference/connectors/postgresql/index.mdx ---
# PostgreSQL

---
title: PostgreSQL
sidebar_position: 1
description:
  "Learn how to configure the PostgreSQL connector and utilize native operations to extend your API's capability."
sidebar_label: PostgreSQL
keywords:
  - postgresql
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# PostgreSQL

## Introduction

The Native Data Connector for PostgreSQL is our flagship connector, with rich support for all kinds of queries and
mutations. In the sections below, we'll try to give an overview of the features of the PostgreSQL connector and how to
configure it in a Hasura DDN project.

:::tip Looking to get started?

If you've ended up here and aren't concerned about tweaking your configuration, and rather are looking to get started
with PostgreSQL and Hasura DDN as quickly as possible, check out our
[PostgreSQL tutorial](/how-to-build-with-ddn/with-postgresql.mdx) or
[learn how to connect](/data-sources/connect-to-a-source.mdx) to a PostgreSQL instance.

:::

## PostgreSQL docs

- [Connector configuration](/reference/connectors/postgresql/configuration.mdx)
- [Native operations](/reference/connectors/postgresql/native-operations/index.mdx)



--- File: ../ddn-docs/docs/reference/connectors/postgresql/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura PostgreSQL connector, including connection URI details,
  and native queries."
keywords:
  - postgresql
  - configuration
---

# Configuration Reference, Version 5

:::info

Supported since data connector version `v1.0.0`

:::

The format of the configuration data varies between versions. When a connector subgraph is added to a project, a
`schema.json` file is created in the configuration directory which describes the format of the `configuration.json`
file.

## Introduction

The configuration is a metadata object that lists all the database entities â€” such as tables â€” that the data connector
has to know about in order to serve queries. It never changes during the lifetime of the data connector service
instance. When your database schema changes you will have to update the configuration accordingly, see
[updating with introspection](#updating-with-introspection).

Below you will find information about how to work effectively with the configuration authoring tools provided by Hasura,
along with the data connector and descriptions of the meanings of each configuration field.

## Versioning policy

The PostgreSQL data connector and its configuration format are versioned independently.

**The data connector will always support at least the latest two configuration versions**, and tooling is provided to
ease the upgrade of configurations. A Hasura DDN project will always use the `head` version of the PostgreSQL data
connector. We recommend doing the same for data connector instances used for local development as well.

During its lifetime, the `head` version of the configuration format may receive backwards-compatible extensions. The
reference documentation of the configuration version will always list the version of the connector which introduced the
configuration extension. The configuration version number is only bumped when it changes in a way that is not backwards
compatible.

Certain fields have default values. The defaults are allowed to evolve over time without requiring a version bump.
Whenever you initialize a new project or update an existing one, all unspecified fields will have their defaults output,
ensuring that only intentional updates to default fields will ever affect the observable behavior of a project, save for
enhancements to performance and security.

The currently supported versions are:

- `"version": "5"` (current)
- `"version": "4"` (deprecated)
- `"version": "3"` (deprecated)

## Configuration workflows

The data connector provides a plugin to the hasura CLI to assist you in authoring configuration.

We provide the `ndc-postgres-cli`, which is a small executable, whose builds can be accessed
[here](https://github.com/hasura/ndc-postgres/releases/).

:::warning Current status

The intended way to commonly use this plugin is through the main `ddn` CLI.

Some of the commands, such as `initialize` and `update`, do not need to be invoked directly as part of the `ddn` CLI
workflow.

The commands can be invoked by changing the directory into the connector directory (e.g.
`cd my_subgraph/connector/my_pg/`) and invoking the command using the following syntax:

```sh
ddn connector plugin --connector connector.yaml -- --help
```

:::

### The initial configuration

Running `initialize` in an empty directory will produce a single file, `configuration.json`, containing a minimal
configuration which expects to be given a connection string via the environment variable `CONNECTION_URI`.

This is the same configuration you would get if you were to add a new PostgreSQL data connector to a project with the
`ddn` cli.

SSL configuration can be provided via the following variables, which can either be suppied via Hasura Cloud, or as
environment variable arguments to the binary or Docker container:

- `CLIENT_CERT`: the contents of the SSL client certificate.
- `CLIENT_KEY`: the contents of the SSL client key.
- `ROOT_CERT`: the contents of the SSL root certificate.

If your certificates are stored in files, these files can be read into environment variables. As an example, we could
read the client certificate into the environment variable like so:

```bash
CLIENT_CERT="$(cat client-certficate.pem)"
```

### Updating with introspection

Whenever the schema of your database changes you will need to update your data connector configuration accordingly to
reflect those changes.

Running `update` in a configuration directory will do the following:

- Connect to the database with the specified `connectionUri`, and then overwrite all data in the `metadata` field
  (except for the native operations) based on the contents of the database and the values given in the
  `introspectionOptions` field.

- Fill in default values for any fields absent from the configuration, as described in the
  [Versioning Policy](#versioning-policy) section.

Various fields in the `introspectionOptions` object influence the outcome of the introspection process, See
[Configure options](#manually-editing).

### Upgrading the configuration format version

The `ndc-postgres-cli` plugin can assist in upgrading any supported version of a connector configuration to the latest
one.

`upgrade --from-dir <existing configuration dir> --to-dir <new directory>` will produce an upgraded version of the
configuration in `<existing configuration dir>` and place it in `<new directory>`.

For example:

```sh
cd <connector-path> # E.g. my_subgraph/connector/my_pg/
ddn connector plugin --connector connector.yaml -- upgrade --dir-from . --dir-to .
```

Sometimes a configuration format upgrade will need some manual editing of the result, if the new version requires
information that cannot be derived from the previous version. This can vary from case to case, but the upgrade command
will output instructions if there are known manual changes required.

### Manually editing

There are occasions when the automatic introspection falls short of your needs. For instance, it may not detect a
particular entity type, or it may pick names according to conventions with which you disagree.

If you find yourself in this situation you may still be able to bring your configuration into an acceptable state by
editing it manually. In this case you'd be well advised to keep your configuration files under version control, as
re-running the `update` command will overwrite your manually-crafted changes.

If there is a pattern to the changes you find yourself applying manually it's possible that your use case could warrant
a new `introspectionOptions` field that could integrate the pattern with the normal introspection process. Feel free to
raise a feature request issue on the [Issue Tracker][Graphql-Engine Issue Tracker].

One section of the configuration that will always need manual authorship is the user-defined
[`nativeQueries`](/reference/connectors/postgresql/native-operations/index.mdx).

[Graphql-Engine Issue Tracker]: http://github.com/hasura/graphql-engine/issues

## Changes from the previous version

Version 5 separates the `nativeQueries` into two fields, `queries` and `mutations`, and nests them under
`nativeOperations`, as well as nested `scalarTypes` and `compositeTypes` under `types` and renames them to `scalar` and
`composite` respectively.

## The initial configuration

Running `ndc-postgres-cli initialize` in an empty directory will produce a single file, `configuration.json`, reproduced
here in abbreviated form:

```yaml
{
  "version": "5",
  "$schema": "./schema.json",
  "connectionSettings":
    {
      "connectionUri": { "variable": "CONNECTION_URI" },
      "isolationLevel": "ReadCommitted",
      "poolSettings":
        {
          "maxConnections": 50,
          "poolTimeout": 30,
          "idleTimeout": 180,
          "connectionLifetime": 600,
          "checkConnectionAfterIdle": 60,
        },
    },
  "mutationsVersion": null,
  "metadata":
    {
      "tables": {},
      "nativeOperations": { "queries": {}, "mutations": {} },
      "types": { "scalar": {}, "composite": {} },
    },
  "introspectionOptions":
    {
      "excludedSchemas":
        ["information_schema", "pg_catalog", "tiger", "crdb_internal", "columnar", "columnar_internal"],
      "unqualifiedSchemasForTables": ["public"],
      "unqualifiedSchemasForTypesAndProcedures": ["public", "pg_catalog", "tiger"],
      "comparisonOperatorMapping":
        [
          { "operatorName": "=", "exposedName": "_eq", "operatorKind": "equal" },
          { "operatorName": "<=", "exposedName": "_lte", "operatorKind": "custom" },
          ...,
        ],
      "introspectPrefixFunctionComparisonOperators": ["box_above", "box_below", ...],
      "typeRepresentations": { "bit": "string", "bool": "boolean", ... },
    },
}
```

The various default values of `introspectionOptions` fields are output explicitly. This provides both immediacy and
versioning flexibly.

Making defaults explicitly present means you don't have to look up in the reference documentation to know what
configuration is in force, and it allows the defaults to evolve over time without requiring a version bump of the
configuration format.

### A minimal configuration

The very minimal configuration that is acceptable elides all fields that have defaults and contains just the `version`
and `connectionUri` fields:

```yaml
{ "version": "5", "connectionSettings": { "connectionUri": { "value": "postgres://..." } } }
```

or, to get the connection uri from an environment variable:

```yaml
{ "version": "5", "connectionSettings": { "connectionUri": { "variable": "CONNECTION_URI" } } }
```

Note that the choice of environment variable name `CONNECTION_URI` is only a matter of convention. Any environment
variable can be used.

## `version`

This version uses the string-value `"5"`.

## `connectionSettings`

### `connectionUri`

The `connectionUri` field indicates the uri of the database which the connector will be querying. This can be given
either as a literal value, or sourced from an environment variable (to help with sound credentials storage for
instance).

The PostgreSQL database URL should follow the [PostgreSQL connection URI form][libpq: Connection Strings]

**Examples:**

```yaml
"connectionUri": "postgresql://user:password@host/databasename"
```

```yaml
"connectionUri": { "variable": "CONNECTION_URI" }
```

Note that the choice of environment variable name `CONNECTION_URI` is only a matter of convention. Any environment
variable can be used.

[libpq: Connection Strings]: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING

### `poolSettings`

This field indicates how the data connector should manage connection pooling.

The default value is:

```yaml
"poolSettings":
  {
    "maxConnections": 50,          // Maximum number of pool connections
    "poolTimeout": 30,             // Maximum time to acquire a connection from the pool (seconds)
    "checkConnectionAfterIdle": 60 // Time at which an idle connection will be checked as still being alive (seconds / null)
    "idleTimeout": 180,            // Time at which an idle connection will be released from the pool (seconds)
    "connectionLifetime": 600,     // Maximum time for an individual connection (seconds)
  }
```

The `idleTimeout` and `connectionLifetime` options may be set to `null`. This indicates that connections will never be
retired from the pool. The `checkConnectionAfterIdle` option may also be set to `null`. This indicates that the
connection should be checked each time we acquire it from the pool.

Note the distinction between setting the fields to `null` versus omitting them. `null` indicates an indefinite value and
omission indicates the default value.

### `isolationLevel`

This field indicates the isolation level of the transaction in which a query is executed.

It may take on the following values:

- `ReadCommitted` (the default). Prevents reading data from another uncommitted transaction.
- `RepeatableRead`. Reading the same data twice is guaranteed to return the same result.
- `Serializable`. Concurrent transactions behave identically to serializing them one at a time.

## `metadata`

The metadata section collects declarations of all the database entities that are known to the data connector.

### `tables`

The `tables` field collects all the tables and views, and the collection name that will be used for each of them.

Consult the [json schema reference][Configuration JSON Schema] for details.

**Example**

```yaml
"tables":
  {
    "Album":
      {
        // Exposed collection name "schemaName": "public",
        "tableName": "Album",
        // The actual name of the table in the database "columns":
          {
            "AlbumId":
              {
                // The exposed field name "name": "AlbumId",
                // The actual name of the column in the table "type": { "scalarType": "int4" },
                "nullable": "nonNullable",
                "description": "The identifier of an album",
              },
            "ArtistId":
              {
                "name": "ArtistId",
                "type": { "scalarType": "int4" },
                "nullable": "nonNullable",
                "description": "The id of the artist that authored the album",
              },
            "Title":
              {
                "name": "Title",
                "type": { "scalarType": "varchar" },
                "nullable": "nonNullable",
                "description": "The title of an album",
              },
          },
        "uniquenessConstraints": { "PK_Album": ["AlbumId"] },
        "foreignRelations":
          {
            "FK_AlbumArtistId":
              {
                "foreignSchema": "public",
                "foreignTable": "Artist",
                "columnMapping": { "ArtistId": // Column of this table "ArtistId" // Column of the referenced table },
              },
          },
        "description": "The record of all albums",
      },
  }
```

#### The types of a column

Valid column types include all of scalar types, (single-dimension) array types, and composite types.

**Examples**

```yaml
"type": { "scalarType": "varchar" }
```

```yaml
"type": { "arrayType": { "scalarType": "varchar" } }
```

```yaml
"type": {
  "compositeType": "person_address"
},
```

```yaml
"type": { "arrayType": { "compositeType": "person_address" } }
```

### `nativeOperations`

The `queries` and `mutations` fields collect user-specified SQL queries that may become either queryable
collections or mutations in the generated connector schema.

Each query is specified as SQL, and the types of any query arguments and result columns must be specified explicitly.

The following fields should be specified for a Native Operation entry:

- `sql`: SQL expression to use for the Native Operation. We can interpolate values using `{{variable_name}}` syntax,
  such as `SELECT * FROM authors WHERE name = {{author_name}}`. Can be specify either a path to file in the connector
  configuration directory with `{ "file": "<file>" }`, or an inline SQL query such as `{ "inline": "SELECT 1 as one" }`.
- `columns`: The returning columns from the Native Operation.
- `arguments`: Names and types of arguments that can be passed to this Native Operation.
- `description`: Information about the Native Operation.

**Example**

```json
"queries": {
  "artist_below_id": {
    "sql": {
      "inline": "SELECT * FROM public.\"Artist\" WHERE \"ArtistId\" < {{id}}"
    },
    "columns": {
      "ArtistId": {
        "name": "ArtistId",
        "type": {
          "scalarType": "int4"
        }
      },
      "Name": {
        "name": "Name",
        "type": {
          "scalarType": "varchar"
        }
      }
    },
    "arguments": {
      "id": {
        "name": "id",
        "type": {
          "scalarType": "int4"
        }
      }
    }
  }
}
```

Note that the arguments are provided as query parameters - they are not textually interpolated into the query. As such,
a native query argument are value expressions and cannot be used in the place of e.g. a table name.

See also the main [Native Operations](/reference/connectors/postgresql/native-operations/) documentation.

### `types`

Information pertaining the database types used by the connector.

#### `types.composite`

The `composite` field collects information on all explicitly defined composite types (i.e., those that do not arise
implicitly from a table).

**Example**

Assuming your database has the following types defined:

```sql
CREATE TYPE person_name AS
  (
    first_name text,
    last_name text
  );

CREATE TYPE person_address AS
  (
    address_line_1 text,
    address_line_2 text
  );

CREATE TYPE person AS
  (
    name person_name,
    address person_address
  );
```

The introspection process will generate the corresponding metadata declarations:

```yaml
"composite":
  {
    "person":
      {
        "name": "person",
        "schema": "public",
        "fields":
          {
            "address": { "name": "address", "type": { "compositeType": "person_address" }, "description": null },
            "name": { "name": "name", "type": { "compositeType": "person_name" }, "description": null },
          },
        "description": null,
      },
    "person_address":
      {
        "name": "person_address",
        "schema": "public",
        "fields":
          {
            "address_line_1": { "name": "address_line_1", "type": { "scalarType": "text" }, "description": null },
            "address_line_2": { "name": "address_line_2", "type": { "scalarType": "text" }, "description": null },
          },
        "description": null,
      },
    "person_name":
      {
        "name": "person_name",
        "schema": "public",
        "fields":
          {
            "first_name": { "name": "first_name", "type": { "scalarType": "text" }, "description": null },
            "last_name": { "name": "last_name", "type": { "scalarType": "text" }, "description": null },
          },
        "description": null,
      },
  }
```

#### `types.scalar` {#scalartypes}

The `metadata.types.scalar` field captures the scalar types that end up appearing in the data connector schema and their
associated comparison operators and aggregation functions.

The introspection process will attempt to ensure that only relevant types that actually appear in collection fields or
input argument appear in the metadata.

**Example**

```yaml
"scalar":
  {
    "bool":
      {
        // The name that appears in the data connector schema "typeName": "bool",
        "schemaName": "pg_catalog",
        "description": null,
        "aggregateFunctions":
          {
            "bool_and": { "returnType": "bool" },
            "bool_or": { "returnType": "bool" },
            "every": { "returnType": "bool" },
          },
        "comparisonOperators":
          {
            "_eq":
              {
                // The name that appears in the data connector schema "operatorName": "=",
                // Name of the operator in the database "operatorKind": "equal",
                // The canonical equality operator (for relationships) "argumentType": "char",
                "isInfix":
                  true // Indication of whether the operator is syntactically a binary // infix operator or a binary
                  prefix-function,
              },
            "_gt":
              {
                "operatorName": ">",
                "operatorKind": "custom",
                // A non-equality operator "argumentType": "char",
                "isInfix": true,
              },
            ...,
          },
        "typeRepresentation": "boolean",
      },
  }
```

The `aggregateFunctions` field declares the aggregation functions that are defined for the scalar type. The
introspection process is capable of discovering all compatible aggregation functions.

The only aggregation functions supported are those that take a single argument, which include most builtin functions.

:::info Global scope

Note that the support for aggregation function is not yet aware of schema namespacing. User-defined aggrate functions
should be defined in the `search_path` of the database user used by the data connector (e.g., the `public` schema).

:::

The `comparisonOperators` field declares the comparison operators that are defined for the scalar type. A comparison
operator is any function that takes two arguments and returns a `bool` value. An operator is recorded under the scalar
type declaration of its first argument.

The introspection process is capable of discovering all compatible comparison operators, including those accessible
through implicit casts.

Many comparison operators in PostgreSQL are syntactically applied as binary infix operators and denoted by
non-alphabetical symbols. Therefore we map the names of infix operators. This process is controlled by the
`introspectionOptions.comparisonOperatorMapping` field.

Regular functions which use prefix-application syntax are also usable as comparison operators. These too are
discoverable by the introspection process. However, there is a large body of pre-existing functions that appear to be
comparison functions according to their type (i.e., they take two arguments and return a bool) yet can't be meaningfully
said to be performing a comparison operation (such as
[`pg_has_role`](https://www.postgresql.org/docs/current/functions-info.html#FUNCTIONS-INFO-ACCESS-TABLE)). Therefore,
only the names mentioned in
[`introspectionOptions.introspectPrefixFunctionComparisonOperators`](#introspectPrefixFunctionComparisonOperators) are
considered by the introspection process.

:::info Global scope

Note that the support for comparison operators is not yet aware of schema namespacing. User-defined comparison operators
should be defined in the `search_path` of the database user used by the data connector (e.g., the `public` or
`pg_catalog` schemas).

:::

The `typeRepresentation` field defines the type representations of the scalar type. If a type does not have a defined
type representation, the default type representation is opaque json.

The type representation of a scalar type indicates what values are considered acceptable in requests, and what values
are expected to be returned in the response.

## `introspectionOptions`

This field collects various options that may be used to drive the configuration and introspection process.

### `excludedSchemas`

The `introspectionOptions.excludedSchemas` field is a list of schema names that the introspection process will ignore.
The default value includes schemas used for internal database bookkeeping and reflection, such as `information_schema`,
for PostgreSQL and common extensions.

**Example (default)**

```yaml
"excludedSchemas": [
    "information_schema", # PostgreSQL
    "pg_catalog", # PostgreSQL
    "tiger", # PostGIS extension
    "crdb_internal", # CockroachDB
    "columnar", # Citus
    "columnar_internal", # Citus
  ]
```

### `unqualifiedSchemasForTables`

The `introspectionOptions.unqualifiedSchemasForTables` field identifies schemas for which the introspection process will
generate table metadata entries that are not prefixed by the schema name.

Tables defined in schemas in this list will have metadata entries that of the form `<table_name>`. Those defined in
schemas not in this list will have metadata entries that of the form `<schema_name>_<table_name>`.

**Example (default)**

```yaml
"unqualifiedSchemasForTables": [
  "public"
],
```

### `unqualifiedSchemasForTypesAndProcedures`

The `introspectionOptions.unqualifiedSchemasForTypesAndProcedures` field identifies schemas for which the introspection
process will generate type and comparison operator metadata entries that are not prefixed by the schema name, similar to
the same for tables.

:::info Global scope

Comparison operators and types only support unqualified access i.e. global scoping currently. Consequentially, types and
operators defined in any schema not in this list will not be discoverable.

:::

**Example (default)**

```yaml
"unqualifiedSchemasForTypesAndProcedures": [
  "public",
  "pg_catalog",
  "tiger"
],
```

### `comparisonOperatorMapping`

The `introspectionOptions.comparisonOperatorMapping` field describes the names that will be used to represent infix
comparison operators such as `=`, `!=`, `LIKE`, `<`, etc.

In order for the introspection process to discover a given operator it must have an entry in this list.

**Example (default, truncated)**

```yaml
"comparisonOperatorMapping":
  [
    { "operatorName": "=", "exposedName": "_eq", "operatorKind": "equal" },
    { "operatorName": "<=", "exposedName": "_lte", "operatorKind": "custom" },
    ...,
  ]
```

### `introspectPrefixFunctionComparisonOperators` {#introspectPrefixFunctionComparisonOperators}

The `introspectionOptions.introspectPrefixFunctionComparisonOperators` gives the list of prefix-functions that are
candidates for comparison operators.

Because of the wide variety of predefined functions in existence this is an inclusion list rather than an exclusion
list. Functions not appearing in this list will not be discoverable by the introspection process.

**Example (default, truncated)**

```yaml
"introspectPrefixFunctionComparisonOperators": ["box_above", "box_below", "box_contain", ...]
```

### `typeRepresentations`

The `typeRepresentations` field define the TypeRepresentation that the introspection process will assign to the types it
encounters.

**Example**

The default initial type representation mappings:

```yaml
"typeRepresentations":
  {
    "bit": "string",
    "bool": "boolean",
    "bpchar": "string",
    "char": "string",
    "date": "date",
    "float4": "float32",
    "float8": "float64",
    "int2": "int16",
    "int4": "int32",
    "int8": "int64AsString",
    "numeric": "bigDecimalAsString",
    "text": "string",
    "time": "time",
    "timestamp": "timestamp",
    "timestamptz": "timestamptz",
    "timetz": "timetz",
    "uuid": "uUID",
    "varchar": "string",
  }
```

The type representation guides how the Postgresql type will be exposed in the final DDN API of the project, and may
influence the output format of queries as well.

For example, type representations enable the API generation to assign e.g. the GraphQL type `String` to all of the
Postgresql types `varchar`, `text` etc.

#### `int8` and `numeric` types

The default type representations of `int8` and `numeric` types are `int64AsString` and `bigDecimalAsString`
respectively. These indicate that values accepted in requests and returned in response will have a string value json
representation, to improve compatibility with common JavaScript clients.

It is possible to configure this option to represent these types as numbers instead. To do so, configure the type
representation of the `int8` type to `int64`, and/or configure the type representation of the `numeric` type to
`bigDecimal` as follows:

## `mutationsVersion`

:::warning Experimental feature under ongoing development

Developing support for mutations of tables is an ongoing effort that has still not settled on a concrete form. The
`mutationsVersion` field is essentially a feature flag toggle that will let you opt-in to trying this feature as it
evolves.

Beware that the set of supported mutations versions is expected to have a rapid turnover, and there is no guarantee that
two successive configuration versions will have any overlap in the mutations versions they support.

:::

The `mutationsVersion` field indicates the flavor of table mutations to expose in the schema of the data connector. By
default this field is omitted, resulting in no mutations being generated.

Introduction and removal of supported mutations versions will only happen in the context of a version bump of the main
configuration.

The currently supported value are `"v1"`, `"v2"`, where the latest version is preferred.

### `mutationsVersion: "v1"`

A table `my_table` gets a `v1_insert_my_table` mutation in the connector schema which takes a single argument `_object`
of object type equivalent to the type of the table. It is a runtime error to insert into columns that are generated. The
mutation result type is that of the table, allowing selection of fields from the row that was inserted.

A table `my_table` with a single-column primary key `my_id_column` gets a `v1_delete_my_table_by_my_id_column` mutation
with a single argument `my_id_column` of type equal to that of the column. The mutation result type is that of the
table, allowing selection of fields from the row that was deleted.

This version of mutations does not support filter-based permissions.

### `mutationsVersion: "v2"`

"v2" of auto-generated mutations introduces insert, delete and update point mutations per table that include permission
arguments.

We generate the following procedures:

- A single insert procedure is generated per table, and has the following form:

  ```graphql
  v2_insert_<table>(
      objects: [<table-object>],
      post_check: <boolexpr>
  )
  ```

  Using it, we can insert multiple objects and include a post check for permissions.

- A delete procedure is generated per table X unique constraint, and has the following form:

  ```graphql
  v2_delete_<table>_by_<column_and_...>(
      key_<column1>: <value>,
      key_<column2>: <value>,
      ...,
      pre_check: <boolexpr>
  )
  ```

  Using it, we can delete a single row using the uniqueness constraint, and include a pre check for permissions.

- An update procedure is generated per table X unique constraint, and has the following form:

  ```graphql
  v2_update_<table>_by_<column_and_...>(
      key_<column1>: <value>,
      key_<column2>: <value>,
      ...,
      update_columns: { <column>: { _set: <value> }, ... },
      pre_check: <boolexpr>,
      post_check: <boolexpr>
  )
  ```

  Using it, we can update a single row using the uniqueness constraint by updating the relevant columns, and include a
  pre check and post check for permissions.

For each `pre_check` and `post_check` arguments of a mutation, an
[Argument Preset](/reference/metadata-reference/permissions#commandpermissions-argumentpreset) can be set in the
mutation's [Command Permissions](/reference/metadata-reference/permissions#commandpermissions-commandpermission).

These checks are now optional as of release `v1.2.0`, allowing greater flexibility based on your specific permissions
requirements. If a `pre_check` or `post_check` is not specified, the mutation will proceed without those checks by
default.

## `mutationsPrefix: ""`

The `mutationsPrefix` configuration option is an optional string that controls the prefix applied to mutation names.

- **Existing Projects**: By default, if `mutationsPrefix` is not set (i.e., null), the prefix will default to
  `v2_mutation_name`.
- **Customization**: You can customize this prefix by setting `mutationsPrefix` to any string, such as
  `foo_mutation_name`.
- **New Projects (Default)**: For new projects, `mutationsPrefix` is set to an empty string (`""`), removing the prefix
  entirely, which is the preferred setup for most users.

Setting `mutationsPrefix`: "" will completely remove the prefix, simplifying mutation names in line with common user
preferences.

[Configuration JSON Schema]: https://github.com/hasura/ndc-postgres/blob/main/static/schema.json
[NDC Postgres Issue Tracker]: http://github.com/hasura/ndc-postgres/issues



--- File: ../ddn-docs/docs/reference/connectors/postgresql/native-operations/index.mdx ---
# Native Operations

---
sidebar_position: 1
sidebar_label: Native Operations
description:
  "Native Operations allow you to run custom SQL queries on your PostgreSQL database. This allows you to run queries
  that are not supported by Hasura's GraphQL engine. This page explains how to configure various types of native queries
  in Hasura."
keywords:
  - native operations
seoFrontMatterUpdated: false
---

# Native Operations

## Introduction

Native Operations allow you to run custom SQL queries on your PostgreSQL database. This allows you to run queries that
are not supported by Hasura DDN's GraphQL engine. This unlocks the full power of your database, allowing you to run
complex queries, mutations, and even vector searches â€” all directly from your Hasura GraphQL API.

## Get started

- [Syntax](/reference/connectors/postgresql/native-operations/syntax.mdx)
- [Native Queries](/reference/connectors/postgresql/native-operations/native-queries.mdx)
- [Native Mutations](/reference/connectors/postgresql/native-operations/native-mutations.mdx)
- [Vector Search](/reference/connectors/postgresql/native-operations/vector-search.mdx)



--- File: ../ddn-docs/docs/reference/connectors/postgresql/native-operations/syntax.mdx ---
# Syntax

---
sidebar_position: 2
sidebar_label: Syntax
description: Learn how to write Native Operations with the proper syntax in Hasura.
keywords:
  - hasura
  - native operations
  - troubleshooting
  - sql
  - gotchas
seoFrontMatterUpdated: false
---

# Native Operations SQL Syntax

There are a few requirements for writing valid Hasura DDN Native Operations SQL:

## Arguments are defined in double curly braces

Arguments are specified with double curly braces as variables in your statement, like `{{id}}`, `{{name}}`, etc. These
are placeholders for the actual values.

## Arguments as scalar values only

The arguments are not interpolated, but are translated to parameters of parameterized queries (arguments like `{{id}}`
and `{{name}}` are translated to `$1`, `$2`, etc.) and passed as data, and therefore can only be used in place of scalar
values, not table names, column names, or other arbitrary SQL.

## No quoting of string arguments

Since the arguments are translated to parameters of parameterized queries, they are passed as data and do not need to be
quoted. For example, use `{{name}}` instead of `'{{name}}'`.

## Single statements only

The SQL of a Native Operation should be a single SQL statement that returns a result and **should not contain a
semicolon** at the end, as the SQL is used inside another SQL query.

## String value arguments in patterns using concatenation

Since you can't directly interpolate arguments (like variables) into your SQL statements in the same way you might in
other contexts, in order to use a string argument inside a pattern, such as in a `LIKE` predicate, the argument can be
concatenated with the relevant pattern parts.

For example:

```sql
SELECT * FROM "Artist" WHERE "Name" LIKE '%John%'
```

can be written as

```sql
SELECT * FROM "Artist" WHERE "Name" LIKE '%' || {{name}} || '%'
```

using the `||` concatenation operator to concatenate the parts of the pattern with the argument variable.

## No "hasura\_" prefixed arguments

Argument names that begin with the prefix `hasura_` are reserved for internal use.



--- File: ../ddn-docs/docs/reference/connectors/postgresql/native-operations/native-mutations.mdx ---
# Native Mutations

---
sidebar_position: 3
sidebar_label: Native Mutations
description:
  "Native Mutations allow you to run custom SQL queries on your PostgreSQL database. This page explains how to configure
  Native Mutations in Hasura."
keywords:
  - native operations
  - custom mutations
seoFrontMatterUpdated: false
---

# Native Mutations

## Introduction

Native Mutations allow you to run custom SQL queries on your PostgreSQL database that can be exposed via the Hasura
GraphQL engine and modify the database state.

:::danger Permissions not yet supported

Native Mutations do not yet support permissions. This means that any user with access to the GraphQL API can execute
Native Mutations. Ensure that you have appropriate security measures in place to prevent unauthorized access. An update
on this feature will be provided in an upcoming release.

:::

## Structure

A Native Mutation is a single SQL statement that returns results and can take arguments, and might modify the database.
The SQL structure of a Native Mutation is specified in the
[Native Operation syntax](/reference/connectors/postgresql/native-operations/syntax.mdx) page, but require an important
tweak:

The SQL should include a `RETURNING` clause, listing the returned columns (use `RETURNING *` to return all columns).

## Create a Native Mutation

To create a new Native Mutation, create a new SQL file inside the connector configuration directory, then use the `ddn`
CLI to add it to the connector configuration. For example:

1. Create a new directory structure under the connector configuration:

   ```sh
   mkdir -p my_subgraph/connector/chinook_pg/native_operations/mutations/
   ```

2. Create a new SQL file `my_subgraph/connector/chinook_pg/native_operations/mutations/insert_artist.sql` with the
   following content:

   ```sql
   INSERT INTO public."Artist" VALUES (
     ( SELECT "ArtistId" + 1
       FROM "Artist"
       ORDER BY "ArtistId" DESC
       LIMIT 1
     ),
     {{name}}
   )
   RETURNING *
   ```

3. Create a new entry in the ndc-postgres configuration:

   ```sh
   ddn connector plugin --connector my_subgraph/connector/chinook_pg/connector.yaml -- \
     native-operation create --operation-path native_operations/mutations/insert_artist.sql --kind mutation
   ```

4. Update your metadata to track the new native query:

   ```sh
   ddn connector-link update chinook_pg --add-all-resources
   ```

5. Then, generate the metadata for your new command before creating a new build:

   ```sh
    ddn command add chinook_pg "InsertArtist"
   ```

:::tip Other operations

You can also create bespoke mutations for updates and deletes. As an example, this mutation can be used to update an
artist's name:

```sql
-- in update_artist_name.sql
UPDATE public."Artist"
SET "Name" = {{new_name}}
WHERE "ArtistId" = (
  SELECT "ArtistId"
  FROM public."Artist"
  WHERE "Name" = {{current_name}}
  LIMIT 1
)
RETURNING *;
```

:::

## List Native Operations

To list the existing Native Operations for a specific connector, use the `list` command. For example:

```sh
ddn connector plugin --connector my_subgraph/connector/chinook_pg/connector.yaml -- \
  native-operation list
```

## Delete a Native Mutation

A Native Query can be deleted with the `delete` command. For example:

```sh
ddn connector plugin --connector my_subgraph/connector/chinook_pg/connector.yaml -- \
  native-operation delete --name insert_artist --kind mutation
```

## Usage

With the example above, you can then run the mutation in your GraphQL API like this:

```graphql
mutation {
  insert_artist(name: "New Artist") {
    ArtistId
    Name
  }
}
```



--- File: ../ddn-docs/docs/reference/connectors/postgresql/native-operations/native-queries.mdx ---
# Native Queries

---
sidebar_position: 3
sidebar_label: Native Queries
description:
  "Native Queries allow you to run custom SQL queries on your PostgreSQL database. This page explains how to configure
  Native Queries in Hasura."
keywords:
  - native queries
seoFrontMatterUpdated: false
---

# Native Queries

## Introduction

Native Queries allow you to run custom SQL queries on your PostgreSQL database. These allow you to run queries that are
not supported by Hasura's GraphQL engine.

## Structure

A Native Query is a single SQL statement that returns results and can take arguments. The SQL structure of a Native
Query is specified in the [Native Operation syntax](/reference/connectors/postgresql/native-operations/syntax.mdx) page.

## Create a Native Query

To create a new Native Query, create a new SQL file inside the connector configuration directory, then use the `ddn` CLI
to add it to the connector configuration. For example:

1. Create a new directory structure under the connector configuration:

   ```sh
   mkdir -p my_subgraph/connector/chinook_pg/native_operations/queries/
   ```

2. Create a new SQL file `my_subgraph/connector/chinook_pg/native_operations/queries/artist_by_name_between.sql` with
   the following content:

   ```sql
   SELECT *
   FROM "Artist"
   WHERE "Name" LIKE '%' || {{name}} || '%'
     AND "ArtistId" > {{lower_bound}}
     AND "ArtistId" < {{upper_bound}}
   ```

3. Create a new entry in the connector configuration:

   ```sh
   ddn connector plugin --connector my_subgraph/connector/chinook_pg/connector.yaml -- \
     native-operation create --operation-path native_operations/queries/artist_by_name_between.sql --kind query
   ```

4. If your data connector is running, run the following command to update your metadata to track the new native query:

   ```sh
   ddn connector-link update chinook_pg --add-all-resources
   ```

   If you get a `connection refused` error, it means that your data connector is not running. You can either start the
   data connector and try again, or, run the following commands to update your metadata to track the new native query:

   ```sh
   # start the connector and introspect your DB
   ddn connector introspect chinook_pg

   # update your metadata to track the new native query
   ddn model add chinook_pg "artist_by_name_between"
   ```

## List Native Operations

To list the existing Native Operations for a specific connector, use the `list` command. For example:

```sh
ddn connector plugin --connector my_subgraph/connector/chinook_pg/connector.yaml -- \
  native-operation list
```

## Delete a Native Query

A Native Query can be deleted with the `delete` command. For example:

```sh
ddn connector plugin --connector my_subgraph/connector/chinook_pg/connector.yaml -- \
  native-operation delete --name artist_by_name_between --kind query
```

### Usage

With the example above, you can then use the query in your GraphQL API like this:

```graphql
query {
  artist_by_name_between(name: "Black", lower_bound: 10, upper_bound: 50) {
    ArtistId
    Name
  }
}
```



--- File: ../ddn-docs/docs/reference/connectors/postgresql/native-operations/vector-search.mdx ---
# Vector Search

---
sidebar_position: 4
sidebar_label: Vector Search
description:
  "Vector search allows you to run vector similarity queries on your PostgreSQL database. This page explains how to
  configure vector search in Hasura using native queries."
keywords:
  - native queries
  - vector search
  - vector
  - similarity
  - neighbor
  - nearest neighbor
  - pgvector
seoFrontMatterUpdated: false
---

# Vector Search

Vector search allows you to run queries on vector types in your PostgreSQL database. This page explains how to configure
vector search in Hasura using Native Queries.

:::info Prerequisites

In order to run a vector search query, you need to have a PostgreSQL database with the
[`pgvector` extension](https://github.com/pgvector/pgvector) installed. You'll also need data in the database which has
been vectorized using a [model such as `word2vec`](https://www.tensorflow.org/text/tutorials/word2vec).

:::

## Example

Imagine a `products` table like the one we use in the docs-sample-app with the following columns:

| Column Name       | Type        |
| ----------------- | ----------- |
| id                | uuid        |
| name              | text        |
| description       | text        |
| price             | int4        |
| manufacturer_id   | uuid        |
| category_id       | uuid        |
| image             | text        |
| country_of_origin | text        |
| created_at        | timestamptz |
| updated_at        | timestamptz |
| vector            | vector      |

We want to be able to run vector searches on the `vector` column of this table. We can do this by writing a query to
project the vector similarity given a query vector and expose it as a model in Hasura. Then, we can create a
relationship from this projection to the full products table and query it flexibly in the GraphQL API.

### Create a vector search Native Query

1. Create a new SQL file under the connector configuration directory
   `my_subgraph/connector/my_pg/native_operations/queries/products_vector_distance.sql` with the following content:

   ```sql
   SELECT
   id,
   vector <=> {{query_vector}} AS distance
   FROM products
   ```

2. Create an entry for the Native Query in the connector configuration:

   ```sh
   ddn connector plugin --connector my_subgraph/connector/my_pg/connector.yaml -- \
     native-operation create --operation-path native_operations/queries/products_vector_distance.sql --kind query
   ```

### Hasura metadata

First, you would need to track the newly-available collection from the step above as types / models in your project's
metadata ([see instructions here](/data-modeling/overview.mdx)). Then, you would need to add a relationship from the
generated `ProductsVectorDistance` type to the main `Products` model, like this:

```yaml
kind: Relationship
version: v1
definition:
  name: product
  source: ProductsVectorDistance
  target:
    model:
      name: Products
      relationshipType: Object
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: id
```

### Usage

Now you can use the flexibility of Hasura's GraphQL API to search your vectors in many ways.

#### Search products based on vector

```graphql
query MyQuery {
  productsVectorDistance(args: { query_vector: "<input vector>" }, limit: 10, order_by: { distance: Asc }) {
    id
    distance
    product {
      name
    }
  }
}
```

#### Search products based on price and vector

```graphql
query MyQuery {
  productsVectorDistance(
    args: { query_vector: "<input vector>" }
    limit: 10
    order_by: { distance: Asc }
    where: { product: { price: { _gt: 500 } } }
  ) {
    id
    distance
    product {
      name
    }
  }
}
```

### Permissions

You can also set the appropriate model permissions to restrict which rows can be accessed by an API client. The
permissions predicate for the vectors can also traverse relationships.

For example, if we want to restrict manufacturers to being able to search vectors only for their own products, and
assuming the currently logged in manufacturer's ID is present in the session variable `x-hasura-manufacturer-id`, we
would add the following to the metadata:

```yaml
kind: ModelPermissions
version: v1
definition:
  modelName: ProductsVectorDistance
  permissions:
    - role: manufacturer
      select:
        filter:
          relationship:
            name: product
            predicate:
              fieldComparison:
                field: manufacturer_id
                operator: _eq
                value:
                  sessionVariable: x-hasura-manufacturer-id
```



--- File: ../ddn-docs/docs/reference/connectors/sqlserver/index.mdx ---
# SQL Server

---
title: SQL Server
sidebar_position: 1
description:
  "Learn how to configure the SQL Server connector and utilize native operations to extend your API's capability."
sidebar_label: SQL Server
keywords:
  - sqlserver
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# SQL Server

## Introduction

Hasura DDN includes a Native Data Connector for SQL Server, providing integration with SQL Server databases. This connector
allows you to leverage SQL Serverâ€™s powerful relational database capabilities while taking advantage of Hasuraâ€™s
metadata-driven approach. Here, weâ€™ll explore the key features of the SQL Server connector and walk through the
configuration process within a Hasura DDN project.

## SQL Server docs

- [Connector configuration](/reference/connectors/sqlserver/configuration.mdx)



--- File: ../ddn-docs/docs/reference/connectors/sqlserver/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura SQL Server connector, including connection URI details, and
  native queries."
keywords:
  - sqlserver
  - configuration
---

# Configuration Reference

## Introduction

The configuration is a metadata object that lists all the database entities â€” such as tables â€” that the data connector
has to know about in order to serve queries. It never changes during the lifetime of the data connector service
instance. When your database schema changes you will have to update the configuration accordingly, see
[updating with introspection](#updating-with-introspection).

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "version": 1,
  "mssql_connection_string": {
    "variable": "CONNECTION_URI"
  },
  "metadata": {
    "tables": {},
    "nativeQueries": {},
    "nativeMutations": {},
    "aggregateFunctions": {},
    "comparisonOperators": {},
    "storedProcedures": {}
  }
}
```

### Property: Version

Version of the configuration file used in the project.

### Property: MSSQL Connection String

The `connectionUri` field indicates the uri of the database which the connector will be querying. This can be given
either as a literal value, or sourced from an environment variable (to help with sound credentials storage for
instance).

**Examples:**

```yaml
"connectionUri": "Server=<Hostname>,<port>;Uid=<username>;Database=<databasename>;Pwd=<password>"
```

```yaml
"connectionUri": { "variable": "CONNECTION_URI" }
```
## Property: Metadata

The metadata section collects declarations of all the database entities that are known to the data connector.

### `tables`

The `tables` field collects all the tables and views, and the collection name that will be used for each of them.

Consult the [json schema reference][Configuration JSON Schema] for details.

**Example**

```yaml
"tables":
  {
    "Album": // Exposed collection name
      {
         "schemaName": "dbo",
        "tableName": "Album", // The actual name of the table in the database 
        "columns":
          {
            "AlbumId": // The exposed field name
              {
                "name": "AlbumId", // The actual name of the column in the table 
                "type": "int",
                "nullable": "nonNullable",
                "description": null,
              },
            "ArtistId":
              {
                "name": "ArtistId",
                "type": "int",
                "nullable": "nonNullable",
                "description": null,
              },
            "Title":
              {
                "name": "Title",
                "type": "nvarchar",
                "nullable": "nonNullable",
                "description": null,
              },
          },
        "uniquenessConstraints": { "PK_Album": ["AlbumId"] },
        "foreignRelations":
          {
            "FK_AlbumArtistId":
              {
                "foreignTable": "Artist",
                "columnMapping": {
                    "ArtistId": "ArtistId" // Column of this table : Column of the referenced table
                }
              },
          },
        "description": null,
      },
  }
```

### `nativeQueries`

Native Queries collect user-specified SQL queries that that may become either queryable collections in the generated
connector schema.

This is a JSON object containing key-value pairs of Native Queries to be used in the data connector.

Example:

```json
{
  "native_query_inline": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT 1 AS result FROM DUAL"
        }
      ]
    },
    "columns": {
      "result": {
        "type": "named",
        "name": "INT"
      }
    },
    "arguments": {},
    "description": ""
  },
  "ArtistById_parameterized": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT * FROM CHINOOK.ARTIST WHERE ARTISTID = "
        },
        {
          "type": "parameter",
          "value": "ARTISTID"
        }
      ]
    },
    "columns": {
      "ARTISTID": {
        "type": "named",
        "name": "INT"
      },
      "NAME": {
        "type": "nullable",
        "underlying_type": {
          "type": "named",
          "name": "STRING"
        }
      }
    },
    "arguments": {
      "ARTISTID": {
        "description": null,
        "type": {
          "type": "named",
          "name": "INT"
        }
      }
    },
    "description": null,
    "isProcedure": false
  }
}
```

### `nativeMutations`

Native Mutations collect user-specified SQL queries that that may become mutations in the generated connector schema.

This is a JSON object containing key-value pairs of Native Mutations to be used in the data connector.

```
{
  "nativeMutations": {
    "insert_artist_and_return_id": {
      "sql": "INSERT INTO [dbo].[Artist] (ArtistId, Name) OUTPUT inserted.*  VALUES ({{ArtistId}}, {{Name}})",
      "columns": {
        "ArtistId": {
          "name": "ArtistId",
          "type": "int",
          "nullable": "nonNullable",
          "description": null
        },
        "Name": {
          "name": "Name",
          "type": "varchar",
          "nullable": "nullable",
          "description": null,
          "castAs": "varchar(100)"
        }
      },
      "arguments": {
        "ArtistId": {
          "name": "ArtistId",
          "type": "int",
          "nullable": "nonNullable",
          "description": null
        },
        "Name": {
          "name": "Name",
          "type": "varchar",
          "nullable": "nullable",
          "description": null
        }
      },
      "description": null
    }
  }
}
```

### `aggregateFunctions`

The `aggregateFunctions` field captures the aggregate funtions associated with the scalar types present in the data connector schema.

The introspection process will attempt to ensure that only relevant types that actually appear in collection fields or
input argument appear in the metadata.

**Example**

```yaml
"aggregateFunctions": 
  {
    "bigint": {
      "APPROX_COUNT_DISTINCT": {
        "returnType": "bigint"
      },
      "AVG": {
        "returnType": "bigint"
      },
      "COUNT": {
        "returnType": "int"
      },
      "COUNT_BIG": {
        "returnType": "bigint"
      },
      "MAX": {
        "returnType": "bigint"
      },
      "MIN": {
        "returnType": "bigint"
      },
      "STDEV": {
        "returnType": "float"
      },
      "STDEVP": {
        "returnType": "float"
      },
      "SUM": {
        "returnType": "bigint"
      },
      "VAR": {
        "returnType": "float"
      },
      "VARP": {
        "returnType": "float"
      }
    }
  }
```

### `comparisonOperators`

The `comparisonOperators` field captures the comparision operators associated with the scalar types present in the data connector schema.

The introspection process will attempt to ensure that only relevant types that actually appear in collection fields or
input argument appear in the metadata.

**Example**

```yaml
"comparisonOperators": 
  {
    "bigint": {
      "_eq": {
        "operatorName": "=",
        "argumentType": "bigint",
        "operatorKind": "equal"
      },
      "_gt": {
        "operatorName": ">",
        "argumentType": "bigint",
        "operatorKind": "custom"
      },
      "_gte": {
        "operatorName": ">=",
        "argumentType": "bigint",
        "operatorKind": "custom"
      },
      "_in": {
        "operatorName": "IN",
        "argumentType": "bigint",
        "operatorKind": "in"
      },
      "_lt": {
        "operatorName": "<",
        "argumentType": "bigint",
        "operatorKind": "custom"
      },
      "_lte": {
        "operatorName": "<=",
        "argumentType": "bigint",
        "operatorKind": "custom"
      },
      "_neq": {
        "operatorName": "!=",
        "argumentType": "bigint",
        "operatorKind": "custom"
      }
    }
  }
```

A comparison operator is any function that takes two arguments and returns a `bool` value. An operator is recorded under
the scalar type declaration of its first argument.

### `storedProcedures`

This is a JSON object containing key-value pairs of Stored Procedures to be used in the data connector.

Example

```
{
  "storedProcedures": {
    "GetArtistsByName": {
      "name": "GetArtistsByName",
      "schema": "dbo",
      "arguments": {
        "Name": {
          "name": "Name",
          "type": "varchar",
          "nullable": "nullable",
          "isOutput": false,
          "description": null
        }
      },
      "returns": {
        "CustomerId": {
          "name": "CustomerId",
          "type": "int",
          "nullable": "nonNullable",
          "description": null
        },
        "Phone": {
          "name": "Phone",
          "type": "varchar",
          "nullable": "nonNullable",
          "description": null
        },
        "TotalPurchases": {
          "name": "TotalPurchases",
          "type": "int",
          "nullable": "nonNullable",
          "description": null
        }
      },
      "description": null
    }
  }
}
```

## Updating with introspection

Whenever the schema of your database changes you will need to update your data connector configuration accordingly to
reflect those changes.

Running `update` in a configuration directory will do the following:

- Connect to the database with the specified `mssql_connection_string`, and then overwrite all data in the `tables` field

- Fill in default values for any fields absent from the configuration



--- File: ../ddn-docs/docs/reference/connectors/trino/index.mdx ---
# Trino

---
title: Trino
sidebar_position: 1
description:
  "Learn how to configure the Trino connector and utilize native operations to extend your API's capability."
sidebar_label: Trino
keywords:
  - trino
  - configuration
  - connector
seoFrontMatterUpdated: false
---

# Trino

## Introduction

Hasura DDN includes a Native Data Connector for Trino, providing integration with Trino databases. This connector
allows you to leverage Trinoâ€™s powerful relational database capabilities while taking advantage of Hasuraâ€™s
metadata-driven approach. Here, weâ€™ll explore the key features of the Trino connector and walk through the
configuration process within a Hasura DDN project.

## Trino docs

- [Connector configuration](/reference/connectors/trino/configuration.mdx)



--- File: ../ddn-docs/docs/reference/connectors/trino/configuration.mdx ---
# Configuration

---
sidebar_position: 2
sidebar_label: Configuration
description:
  "Reference documentation for the setup process for the Hasura Trino connector, including connection URI details, and
  native queries."
keywords:
  - trino
  - configuration
---

# Configuration Reference

## Introduction

The configuration is a metadata object that lists all the database entities â€” such as tables â€” that the data connector
has to know about in order to serve queries. It never changes during the lifetime of the data connector service
instance. When your database schema changes you will have to update the configuration accordingly, see
[updating with introspection](#updating-with-introspection).

## Structure

The configuration object is a JSON object with the following fields:

```json
{
  "jdbcUrl": "",
  "jdbcProperties": {},
  "schemas": [],
  "tables": [],
  "functions": [],
  "nativeQueries": {}
}
```

### Property: JDBC URL

The JDBC connection URL to connect to the Trino database. This is a required field.

The value can either be a literal string, or a reference to an environment variable:

```json
{
  "jdbcUrl": "jdbc:trino://localhost:8090/xe?user=foo&password=bar",
  "jdbcUrl": { "variable": "TRINO_JDBC_URL" }
}
```

### Property: JDBC Properties

This is a JSON object containing key-value pairs of additional properties to be passed to the JDBC driver. For example,
with MySQL to enable running multiple statements in a given query:

```json
{
  "jdbcProperties": { "allowMultiQueries": "true" }
}
```

### Property: Schemas

This is an optional array of schema names to include in the introspection process. If not provided, all schemas will be
included.

Example:

```json
{
  "schemas": ["public", "other_schema"]
}
```

### Property: Tables

This is an array of table definitions, generated automatically during introspection.

Example:

```json
{
  "tableName": "Album",
  "tableType": "TABLE",
  "description": "",
  "columns": [
    {
      "name": "AlbumId",
      "description": "",
      "type": "int",
      "numeric_scale": 0,
      "nullable": false,
      "auto_increment": true,
      "is_primarykey": true
    },
    {
      "name": "Title",
      "description": "",
      "type": "varchar",
      "numeric_scale": null,
      "nullable": false,
      "auto_increment": false,
      "is_primarykey": false
    },
    {
      "name": "ArtistId",
      "description": "",
      "type": "int",
      "numeric_scale": 0,
      "nullable": false,
      "auto_increment": false,
      "is_primarykey": false
    }
  ],
  "pks": ["AlbumId"],
  "fks": {
    "FK_AlbumArtistId": {
      "foreign_collection": "Artist",
      "column_mapping": {
        "ArtistId": "ArtistId"
      }
    }
  }
}
```

### Property: Functions

This is an array of function definitions.

Example:

```json
{
  "function_catalog": "public",
  "function_schema": "public",
  "function_name": "add",
  "argument_signature": "(N NUMBER, M NUMBER)",
  "data_type": "TABLE (N NUMBER, M NUMBER)",
  "comment": "Adds two numbers"
}
```

### Property: Native Queries

This is a JSON object containing key-value pairs of Native Queries to be used in the data connector.

Two types of Native Queries are supported: **Inline** and **Parameterized**.

Example:

```json
{
  "native_query_inline": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT 1 AS result FROM DUAL"
        }
      ]
    },
    "columns": {
      "result": {
        "type": "named",
        "name": "INT"
      }
    },
    "arguments": {},
    "description": ""
  },
  "ArtistById_parameterized": {
    "sql": {
      "parts": [
        {
          "type": "text",
          "value": "SELECT * FROM CHINOOK.ARTIST WHERE ARTISTID = "
        },
        {
          "type": "parameter",
          "value": "ARTISTID"
        }
      ]
    },
    "columns": {
      "ARTISTID": {
        "type": "named",
        "name": "INT"
      },
      "NAME": {
        "type": "nullable",
        "underlying_type": {
          "type": "named",
          "name": "STRING"
        }
      }
    },
    "arguments": {
      "ARTISTID": {
        "description": null,
        "type": {
          "type": "named",
          "name": "INT"
        }
      }
    },
    "description": null,
    "isProcedure": false
  }
}
```

## Updating with introspection

Whenever the schema of your database changes you will need to update your data connector configuration accordingly to
reflect those changes.

Running `update` in a configuration directory will do the following:

- Connect to the database with the specified `jdbcUrl`, and then overwrite all data in the `tables` field

- Fill in default values for any fields absent from the configuration



--- File: ../ddn-docs/docs/upgrade/overview.mdx ---
# Upgrade

---
title: Upgrade
sidebar_position: 1
description: Learn how to upgrade from older versions of Hasura to Hasura DDN.
sidebar_label: Basics
keywords:
  - hasura
  - docs
  - upgrade
  - v2
---

import Thumbnail from "@site/src/components/Thumbnail";

# Upgrade From v2

## Introduction

The Hasura Data Delivery Network presents a new way to build applications with Hasura. This section of documentation
will help you understand the similarities and differences between the older versions of Hasura and Hasura DDN, guide you
through the upgrade process, and provide you with resources to aid in migrating your existing applications to Hasura
DDN.

## Why upgrade?

Hasura DDN delivers a wealth of features that will help you build applications faster and more efficiently. You can
explore our documentation to get a better sense of what Hasura DDN has to offer.

### Individual developers

As an **individual developer working on a small or hobby project**, you can use Hasura DDN for free. That's right: free!
We'll host your API for you at no cost, forever.

**Before:**

- Limited by Hasura v2's metadata model that was tightly coupled with PostgreSQL and GraphQL, making it hard to think of
  it as a general-purpose API platform.
- Struggled with the complexities of managing server infrastructure or paying for hosted services (yep, even Hasura
  Cloud). Worried about the long-term costs and scalability of your project.
- Adding custom business logic was non intuititve and less flexible.

**After:**

- Server infrastructure worries are a thing of the past â€” [Hasura DDN takes care of hosting](https://hasura.io/pricing)
  for free. Enjoy peace of mind knowing that your project can grow without added costs.
- You can fully concentrate on building your product, with backend infrastructure handled.
- Enhanced capabilities to add business logic in Typescript, Python and Go, with the ability to host business logic
  functions on Hasura Cloud.

### Small teams

As a **small team**, you can leverage Hasura DDN to streamline your development process and reduce overhead. With Hasura
DDN, your team can quickly prototype and iterate on features using immutable builds, ensuring that deployments are
consistent and reliable. This means more time to focus on delivering value to your customers and less time spent
debugging.

**Before:**

- Hasura v2's build and runtime coupling led to performance issues, slowing down your deployment processes.
- No ability to preview the API, publish or rollback quickly.
- Lack of clarity on subgraph changes for your published API and long feedback loops due to the inability to test during
  local development.

**After:**

- Speed up your workflow with [immutable builds](/project-configuration/overview.mdx) that make rapid prototyping a
  breeze.
- Ensure smooth and dependable [deployments](/deployment/hasura-ddn/tutorial/index.mdx), cutting down on debugging time.
- Simplify your development lifecycle with [declarative metadata](/reference/metadata-reference/index.mdx) that defines
  your API structure.

### Mature teams

For **mature teams in large organizations**, Hasura DDN transforms your approach to scaling and integrating
applications. With built-in federation using subgraphs and Hasura's metadata-driven data access layer, you can grow your
application as your business expands, seamlessly adding new data sources and services without extensive rewrites. These
two pillars of valueâ€”federation and metadataâ€”further streamline your architecture, providing a unified and scalable way
to manage your API across multiple environments.

**Before:**

- Hasura v2â€™s monolithic metadata made it challenging for multiple teams to collaborate, leading to inefficiencies and
  slower development cycles.
- Integration of new data sources and services required extensive rewrites.
- Managing APIs across various environments was complex and time-consuming.

**After:**

- Easily scale with [built-in federation](/project-configuration/subgraphs/index.mdx) that integrates subgraphs into
  your Hasura DDN setup.
- Add [new data sources](/data-sources/overview.mdx) seamlessly, avoiding the need for major overhauls.
- Manage your API effortlessly across different environments with the world's first
  [declarative metadata access layer](/reference/metadata-reference/index.mdx) that unifies control.

## Learn more

- [Learn how to upgrade.](/upgrade/guide.mdx)
- [Understand the differences between Hasura DDN and older versions of Hasura.](/upgrade/feature-availability/index.mdx)



--- File: ../ddn-docs/docs/upgrade/guide.mdx ---
# Upgrade Guide

---
sidebar_position: 2
description: Learn how to upgrade from older versions of Hasura to Hasura DDN.
title: Upgrade Guide
keywords:
  - hasura
  - docs
  - upgrade
  - v2
---

import Thumbnail from "@site/src/components/Thumbnail";

# Upgrade Guide

## Introduction

Currently, there is no in-place upgrade path from Hasura v2 to Hasura DDN. To upgrade your existing Hasura v2 instances
to Hasura DDN, please follow one of the below strategies.

**Note:** Hasura v2 is still supported and will be maintained according to our
[LTS policy](https://hasura.io/docs/2.0/policies/versioning/).

:::info Help is available

Our field engineering team is here to assist with the migration process. If you need help or have questions, please
reach out to us [here](https://hasura.io/contact-us).

:::

## Upgrade Strategies

When upgrading from Hasura v2 to Hasura DDN, you have multiple approaches to ensure a smooth transition while
maintaining service continuity for your existing API clients. Depending on your needs, you can either introduce the DDN
API as a separate endpoint, gradually phase out the v2 API, or integrate your existing v2 API as a remote schema in your
new DDN project.

**Note:** The [Strangler Fig pattern](https://learn.microsoft.com/en-us/azure/architecture/patterns/strangler-fig) can
be applied in two ways:

- **GraphQL-based Routing:** Use Hasura DDN to manage both systems by integrating Hasura v2 as a remote schema,
  progressively migrating functionalities while maintaining a unified API endpoint.
- **URL-based Routing:** Separate the old and new systems with distinct API endpoints,and gradually shift traffic from
  the old endpoint to the new one.

### GraphQL-based Routing using DDN

Leverage your existing Hasura v2 API as a remote schema within your new Hasura DDN project to ensure a seamless
migration process and minimize disruptions.

```mermaid
graph TD
    A[API Consumers] -->|GraphQL Queries| B[Hasura DDN API]
    B -->|New Features| C[Database connectors]
    B -->|Existing Features| D[GraphQL connector]
    D -->|Remote Schema| E[Hasura v2 API]
    C --> F[Database]
    E --> F
    style A fill:#ffffff,stroke:#000000,stroke-width:2px,color:#000000
    style B fill:#5cb85c,stroke:#000000,stroke-width:2px,color:#000000
    style C fill:#5cb85c,stroke:#000000,stroke-width:2px,color:#000000
    style D fill:#5cb85c,stroke:#000000,stroke-width:2px,color:#000000
    style E fill:#d9534f,stroke:#000000,stroke-width:2px,color:#000000
    style F fill:#ffffff,stroke:#000000,stroke-width:2px,color:#000000
```

- **Create a DDN project:**
  - Begin by creating a new Hasura DDN project that will serve as the foundation for all new developments and
    migrations.
  - Ensure the DDN project is set up with the necessary configurations, including security policies, access controls,
    and environment settings.
- **Connect Hasura v2 as a remote schema:**

  - Incorporate your existing Hasura v2 project as a remote schema within the DDN project. This allows you to continue
    using the v2 API while progressively transitioning to DDN.
  - Use the NDC GraphQL connector to bring in v2 as a remote schema in DDN. Follow the directions provided
    [here](https://github.com/hasura/ndc-graphql) in the README to correctly configure the connection.
  - Ensure that the remote schema setup includes proper role handling and namespace management to avoid conflicts
    between v2 and DDN schemas.

- **Expose the DDN API to consumers:**

  - Make the DDN API available to consumers, offering a unified endpoint that encompasses both DDN and v2
    functionalities.
  - Keep the v2 project running behind the scenes to maintain continuity for existing consumers who have not yet
    migrated.
  - Provide detailed documentation and examples to help consumers understand how to interact with the DDN API,
    especially if there are changes from v2.

- **Iterate and update:**

  - Continue iterating on the v2 project as necessary, particularly for bug fixes, minor updates, or critical patches.
  - Simultaneously, ensure that any relevant updates or changes in v2 are reflected in the DDN project. This may involve
    updating GraphQL Connector schema and redeploying the connector and the supergraph.
  - Establish a version control strategy to manage changes in both v2 and DDN, reducing the risk of inconsistencies.

- **Add all new features to DDN:**

  - Develop and implement all new features directly within the DDN project to take advantage of its advanced
    capabilities especially around collaboration.
  - Consider adopting a modular approach to feature development, which allows for easier testing, deployment, and future
    migrations.

- **Migrate existing functionality to DDN:**

  - Gradually migrate existing functionalities from v2 to DDN, prioritizing high-impact or frequently used features.
  - During migration, test each functionality extensively in the DDN environment to ensure that it works as expected and
    does not introduce regressions.
  - Coordinate with API consumers to manage the transition, providing them with clear timelines and support during the
    migration process.

- **Deprecate v2 models via DDN:**

  - Gradually deprecate v2 GraphQL root fields using DDN metadata, signaling to consumers that they should transition to
    using DDN directly.
  - In DDN, use the `@deprecated` flag to mark fields in the v2 subgraph that are planned for removal.
  -     Example: If the `user` model in v2 is being replaced by a `UserNew` model in DDN, ensure that consumers are aware of the change and have sufficient time to update their implementations.
  - Implement a deprecation policy that includes clear timelines, documentation, and communication strategies to avoid
    disruptions.

- **Monitor and Support:**

  - Continuously monitor the performance and usage of both v2 and DDN APIs, using analytics and logging to identify any
    issues or areas for improvement.
  - Provide ongoing support to consumers, addressing their concerns and assisting with the migration process. This could
    include dedicated support channels, migration guides, and technical workshops.

- **Plan for Final Decommissioning:**

  - As more functionalities are migrated and consumers transition to DDN, begin planning for the final decommissioning
    of the v2 project.
  - This should involve a phased approach, where critical functionalities are retained until the very end, and less
    critical or redundant features are decommissioned first.
  - Communicate decommissioning plans well in advance to give consumers ample time to complete their transitions.

- **Cons:**
  - Dual Maintenance: Any changes in the v2 project will require updates in both v2 and DDN, leading to additional
    maintenance overhead.
  - GraphQL Connector Limitations:
    - Subscriptions: DDN cannot proxy v2 subscriptions, which may require a separate handling strategy for real-time
      data needs.
      - Subscription support is now available in DDN, enabling real-time data updates through GraphQL on DDN models.
        Note that v2 subscriptions cannot be directly used with DDN. Learn more about subscriptions
        [here](/graphql-api/subscriptions/index.mdx).
    - Directives: Certain directives, like caching mechanisms and Apollo federation features, cannot be proxied through
      the DDN, which might limit some advanced use cases.
      - Apollo Federation is supported in DDN. Caching is on the roadmap and will be released by end of September.
    - Unions and Interfaces: GraphQL unions and interfaces may face limitations when being proxied, necessitating
      careful schema design and potential workarounds.

**This approach provides a structured and gradual migration path, leveraging the strengths of both systems while
minimizing disruptions to your API consumers. However, careful planning and communication are essential to manage the
complexities involved.**

### URL-based Routing

This strategy involves using URL-based routing to direct traffic between the old and new GraphQL APIs similar to the

- **Routing Example:**
  - `/old` â†’ v2
  - `/new` â†’ DDN
- **Pros:**
  - Allows you to maintain a clear separation between the old and new systems.
- **Cons:**
  - It might not be feasible for clients to handle two separate GraphQL endpoints.
  - Hasura v2 does not have API deprecation features so would be tough to communicate to API consumers about migration.

#### Parallel Deployment

```mermaid
graph TD
    A[API Consumers] --> B[API Router]
    B -->|/old| C[Hasura v2 API]
    B -->|/new| D[Hasura DDN API]
    C --> E[Existing Database]
    D --> E
    style A fill:#ffffff,stroke:#000000,stroke-width:2px,color:#000000
    style B fill:#f0ad4e,stroke:#000000,stroke-width:2px,color:#000000
    style C fill:#d9534f,stroke:#000000,stroke-width:2px,color:#000000
    style D fill:#5cb85c,stroke:#000000,stroke-width:2px,color:#000000
    style E fill:#ffffff,stroke:#000000,stroke-width:2px,color:#000000
```

- Move all new development to Hasura DDN while maintaining two separate APIs: one for Hasura v2 and one for Hasura DDN.
  This allows you to take advantage of new features and optimizations in DDN without disrupting existing workflows in
  v2.
- Establish Clear Boundaries: Define clear boundaries between the functionalities that remain in v2 and those that are
  developed in DDN. This will help avoid duplication of effort and ensure a smooth transition for consumers.
- Update Documentation: Provide thorough documentation for the new DDN API, including examples, usage guidelines, and
  any changes from the v2 API. Ensure that your development and support teams are well-informed about the differences
  between the two APIs.
- Consumer Communication: Inform API consumers about the introduction of the new DDN API, providing details on how to
  use the new API if needed.
- Monitor Performance: Regularly monitor the performance and usage of both APIs. This will help you identify any issues
  early on and ensure that both systems are running efficiently.
- Plan for Future Migration: Although v2 and DDN will run in parallel for some time, plan for the eventual migration of
  all v2 features to DDN. This plan should include timelines, resource allocation, and a strategy for decommissioning
  the v2 API when appropriate.
- Evaluate the Impact: Regularly evaluate the impact of running parallel APIs on your infrastructure, costs, and team
  workload. Adjust your strategy as needed to ensure a smooth and efficient transition.

This approach provides flexibility, allowing your team to leverage the benefits of Hasura DDN while maintaining
stability in the existing Hasura v2 environment.

#### Phased Deprecation

```mermaid
graph TD
    A[API Consumers] --> B[API Router]
    B -->|Migrated Features| C[Hasura DDN API]
    B -->|Remaining Features| D[Hasura v2 API]
    C --> E[Database]
    D --> E
    D -.->|Gradual Migration| C
    style A fill:#ffffff,stroke:#000000,stroke-width:2px,color:#000000
    style B fill:#f0ad4e,stroke:#000000,stroke-width:2px,color:#000000
    style C fill:#5cb85c,stroke:#000000,stroke-width:2px,color:#000000
    style D fill:#d9534f,stroke:#000000,stroke-width:2px,color:#000000
    style E fill:#ffffff,stroke:#000000,stroke-width:2px,color:#000000
```

- Start migrating features from Hasura v2 to Hasura DDN incrementally. Begin with non-critical or less complex features
  to minimize risk.
- As features are successfully migrated to DDN, remove those features from the Hasura v2 project. This ensures that no
  duplicate functionality exists across both systems, reducing maintenance overhead.
- Communicate any feature removal from v2 to your API consumers well in advance. Provide clear documentation on how to
  access the migrated features in DDN, including any changes in API endpoints or data structures.
- Implement a deprecation policy for features in v2 that are slated for removal. This may involve marking endpoints as
  deprecated with a clear timeline for their eventual removal.
- Monitor the usage of both v2 and DDN during the transition. Track which clients are still using v2 features and
  encourage them to transition to DDN by providing support and resources.
- Offer support and guidance to API consumers during the migration process. This could include providing sample code,
  migration guides, or direct assistance from your development team.
- Consider implementing versioning strategies if there are breaking changes. This can help maintain backward
  compatibility and give consumers more time to migrate at their own pace.
- Continuously test both systems in parallel to ensure that the migrated features are functioning as expected in DDN.
  This will help catch any issues early and avoid disruptions.
- Once all critical features have been migrated and consumers have successfully transitioned, begin phasing out the
  Hasura v2 project. This could involve shutting down v2 endpoints, archiving the v2 project, and reallocating resources
  to support DDN.

## Learn More

With any of these strategies, you'll need a clear understanding of the features and functionalities that need to be
migrated. We recommend creating a plan to do feature by feature manual migration. Check out
[this page](/upgrade/feature-availability/index.mdx) for a detailed comparison of the features available between Hasura
v2 and Hasura DDN, and directions on how to migrate them.



--- File: ../ddn-docs/docs/upgrade/feature-availability/index.mdx ---
# Feature Availability

---
sidebar_position: 1
description: Learn what features are available in Hasura DDN when compared to Hasura v2.
title: Feature Availability
keywords:
  - hasura
  - docs
  - upgrade
  - features
---

import Thumbnail from "@site/src/components/Thumbnail";

# Feature Availability

## Introduction

We're constantly rolling out updates to Hasura DDN to provide you with new features and improvements. This page will
help you understand the differences between Hasura v2 and Hasura DDN, so you can make an informed decision about
upgrading.

We've broken parity down into two categories:

- [API features](/upgrade/feature-availability/api-features.mdx)
- [Tooling](/upgrade/feature-availability/tooling.mdx)

:::info What's on the roadmap?

You can see our roadmap â€”Â and engage with our team! â€” anytime by visiting our
[public roadmap](https://github.com/orgs/hasura/projects/201).

:::



--- File: ../ddn-docs/docs/upgrade/feature-availability/api-features.mdx ---
# API

---
sidebar_position: 2
description: Learn what features are available in Hasura DDN when compared to Hasura v2.
title: API
keywords:
  - hasura
  - docs
  - upgrade
  - features
---

import Thumbnail from "@site/src/components/Thumbnail";

# API

## Introduction

On this page, you'll find side-by-side comparisons of the features available in Hasura v2 and Hasura DDN. Take a look at
the table below to see which features are available in each version and read on to learn more about each feature.

:::info Roadmap

You can see our roadmap â€”Â and engage with our team! â€” anytime by visiting our
[public roadmap](https://github.com/orgs/hasura/projects/201).

:::

| Feature                     | v2      | DDN    |
| --------------------------- | ------- | ------ |
| Instant GraphQL API         | âœ…      | âœ…     |
| Multiple Data Sources       | âœ…      | âœ…     |
| Query                       | âœ…      | âœ…     |
| Mutation                    | âœ…      | âœ… (C) |
| Subscription                | âœ…      | âœ…     |
| Streaming                   | âœ…      | WIP    |
| Aggregate Query             | âœ…      | âœ… (C) |
| Native Query                | âœ…      | âœ…     |
| Native Mutation             | âŒ      | âœ…     |
| Action                      | âœ…      | âœ…     |
| Event Trigger               | âœ…      | WIP    |
| Cron Trigger                | âœ…      | âŒ     |
| Remote Schema               | âœ…      | âœ…     |
| CI/CD                       | âœ…      | âœ…     |
| Federation                  | âœ…      | âœ…     |
| Apollo Federation           | âœ…      | âœ…     |
| API Limits                  | âœ…      | WIP    |
| Allow Lists                 | âœ…      | âœ…     |
| Permissions                 | âœ…      | âœ…     |
| Authentication Integrations | âœ…      | âœ…     |
| Admin Secret                | âœ…      | âŒ     |
| Relay API                   | âœ…      | WIP    |
| RESTified Endpoints         | âœ…      | âœ…     |
| Schema Registry             | âœ…      | âœ…     |
| Read Replica                | âœ… (EE) | âœ…     |
| Caching                     | âœ… (EE) | âœ…     |

**\*EE**: Available on Cloud and Enterprise editions only.

**\*C**: Supported by the individual connector.

## Features

### Instant GraphQL API

In Hasura v2, you added a connection string to your project and Hasura instantly generated a GraphQL API.

**In Hasura DDN, this is also true!** However, you're now given more flexibility and have the tools to rapidly iterate
and expand that API to include a dynamic range of sources. With Hasura DDN, you generate this API using a code-first
declarative experience â€” assisted by the CLI â€” instead of the console as in v2.

You can learn more about [the GraphQL API here](graphql-api/overview.mdx).

### Multiple Data Sources

In Hasura v2, connecting data sources involved configuring Hasura to connect to your database and setting up the
necessary permissions and relationships within the console. This enabled Hasura to generate a GraphQL API based on your
database schema.

**In Hasura DDN, connecting data sources is simplified and more flexible**. You can easily connect multiple types of
data sources, including relational databases, REST APIs, and other GraphQL services, all using a concept called
[**native data connectors**](/data-sources/overview.mdx). This expanded capability allows for more diverse and
comprehensive data integration, facilitating a more unified API experience. You can learn more about what native data
connectors we offer on the [Connector Hub](https://hasura.io/connectors), or build your own using one of our SDKs
([TypeScript](https://github.com/hasura/ndc-sdk-typescript), [Rust](https://github.com/hasura/ndc-sdk-rs),
[Python](https://github.com/hasura/ndc-sdk-python), or [Go](https://github.com/hasura/ndc-sdk-go)).

The modular architecture of Hasura DDN ensures that changes in one part of the API do not negatively impact others,
allowing for greater autonomy and flexibility in development.

Further, you can easily configure all your relationships and permissions using a simple declarative syntax. Our
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) makes this easier than
ever with scaffolding, auto-completion, and validation â€” ensuring your metadata is consistent and up-to-date before you
ship it to prod.

### Query

In Hasura v2, queries allowed you to fetch data from your connected databases through the auto-generated GraphQL API.

**In Hasura DDN, querying remains a core functionality**. The querying process has been enhanced with additional
features and optimizations, making data retrieval faster and more efficient. Learn more about GraphQL queries
[here](/graphql-api/queries/index.mdx).

### Mutation

In Hasura v2, mutations enabled you to modify data in your connected databases using the auto-generated GraphQL API.

**In Hasura DDN, mutations continue to play a critical role**. Certain connectors provide auto-generate mutations out of
the box. For others, you can use a lambda connector or native mutation. Learn more about GraphQL mutations
[here](/graphql-api/mutations/index.mdx).

### Subscription

In Hasura v2, for some data sources, subscriptions provided real-time updates by automatically sending data changes to
the client.

**Hasura DDN subscriptions deliver real-time data updates through the auto-generated GraphQL API**. Clients can
instantly receive data changes, making it ideal for real-time applications. Learn more about GraphQL subscriptions
[here](/graphql-api/subscriptions/index.mdx).

### Streaming

In Hasura v2, streaming queries allowed for fetching data in a continuous manner, suitable for real-time use cases.

**In Hasura DDN, streaming is also a work in progress (WIP)**, with plans to enhance the streaming capabilities for more
robust and scalable real-time data delivery.

### Aggregate Query

In Hasura v2, aggregate queries enabled you to perform operations like counting, summing, and averaging on your data.

**In Hasura DDN, aggregate queries are fully supported** yet differ slightly from the v2 aggregates API. You can learn
more about them [here](/reference/metadata-reference/aggregate-expressions.mdx).

### Native Query

In Hasura v2, native queries allowed direct interaction with the underlying database through custom SQL.

**In Hasura DDN, native queries are fully supported**, enabling complex data retrieval operations directly within your
API.

### Native Mutation

In Hasura v2, native mutations were not supported, limiting the ability to perform direct modifications on the database.

**In Hasura DDN, native mutations are now supported**, providing more flexibility for advanced data operations.

### Action

In Hasura v2, Actions allowed you to integrate REST APIs or define custom business logic that could be executed as part
of your GraphQL API. You could quickly and easily integrate OpenAPI-compliant services and expose them as Actions using
the console. This feature enabled bringing in existing REST APIs and extending the functionality of the API beyond what
was directly available through the database schema.

**In Hasura DDN, lambda connectors take the place of Actions**. These connectors allow you to define more complex
business logic, enabling the enrichment and transformation of your data, and seamlessly integrate it with other services
via your existing API. These functions are introspected by Hasura DDN and then exposed as part of your GraphQL API.
Currently, we support business logic via the [TypeScript](/business-logic/overview.mdx) and
[Python connectors](/business-logic/overview.mdx).

You can also use the [OpenAPI connector](https://hasura.io/connectors/openapi) to immediately integrate any
OpenAPI-compliant APIs into your supergraph.

This setup allows for the creation of richer and more dynamic APIs that can handle a wide range of business
requirements. And, we'll host it all for you ðŸŽ‰

### Event Trigger

In Hasura v2, event triggers allowed you to automatically trigger webhooks based on changes in your database.

**In Hasura DDN, event triggers are currently a work in progress (WIP)**, with plans to integrate plugin support for
even more customizable triggers.

### Cron Trigger

In Hasura v2, cron triggers allowed you to schedule periodic tasks that would trigger specific operations within your
API.

**In Hasura DDN, cron triggers are not supported**.

### Remote Schema

In Hasura v2, Remote Schemas allowed you to stitch together multiple GraphQL schemas into a single unified API.

In Hasura DDN, remote GraphQL schemas are easier to manage and integrate using the
[GraphQL API data connector](https://hasura.io/connectors/graphql). This means external GraphQL APIs are treated like
any other data source and have the full assortment of permissions and relationships at your disposal out of the box.

### CI/CD

In Hasura v2, CI/CD was possible using the Hasura CLI in addition to a GitHub integration for automated deployments to
Hasura Cloud.

**In Hasura DDN, CI/CD is fully supported** â€” in fact, we built it into the core of the product using the concept of
[immutable builds](/project-configuration/overview.mdx) allowing you to deploy changes in a safe and controlled manner,
ensuring that changes are propagated seamlessly across all environments and easily tested before deployment.

### Federation

In Hasura v2, federation was possible using a number of different methods, including stitching together multiple Hasura
instances, creating an API gateway, or instituting a multi-protocol approach.

**In Hasura DDN, federation is fully supported** â€” just like CI/CD, it's built into the core of the product. We use the
concept of [subgraphs](/project-configuration/subgraphs/index.mdx) to allow you to create a unified API across 
independent teams and services, ensuring that each subgraph can be developed and deployed independently while still 
being part of a larger, cohesive API.

### Apollo Federation

In Hasura v2, Apollo Federation was supported for creating a unified API across multiple services.

**In Hasura DDN, Apollo Federation is supported**. To know more, please check
[these docs](/graphql-api/apollo-federation.mdx).

### API Limits

In Hasura v2, API limits were available to help manage and control the usage of your API.

**In Hasura DDN, API limits are not supported**.

### Allow Lists

In Hasura v2, allow lists were available to restrict access to specific queries and mutations. This feature was useful
for controlling access to sensitive data or operations.

**In Hasura DDN, allow lists can be generated using [Engine Plugins](/plugins/overview.mdx)**.

### Permissions

In Hasura v2, permissions were available to control access to your data and operations, ensuring that only authorized
users could interact with your API.

**In Hasura DDN, permissions are fully supported**. You can define permissions at the model, field, and command level,
allowing for fine-grained control over who can access your data and what they can do with it. Learn more about
permissions [here](/reference/metadata-reference/permissions.mdx).

### Authentication Integrations

In Hasura v2, authentication integrations were available to authenticate users and control access to your API. Hasura v2
supported a variety of authentication providers, including Auth0, Firebase, and custom JWT.

**In Hasura DDN, authentication integrations are fully supported**. You can [authenticate users](/auth/overview.mdx)
using a variety of providers via webhooks or JWTs.

### Admin Secret

In Hasura v2, the admin secret was used to authenticate requests to the Hasura API, allowing you to perform
administrative tasks.

**In Hasura DDN, the admin secret is not supported**. Instead, you can use create an admin-level token to perform
administrative tasks.

### Relay API

In Hasura v2, the Relay API was supported, allowing you to use Relay-specific features and optimizations in your GraphQL
API.

**In Hasura DDN, the Relay API is supported with global ID identification, enabling compatibility with Relay clients for
node fetching and caching**. However, the Relay Cursor Connections Specification, used for paginated data, is not yet
supported.

### RESTified Endpoints

In Hasura v2, RESTified endpoints were available to expose your GraphQL API as a REST API.

**In Hasura DDN, RESTified endpoints are available via [engine plugins](/plugins/restified-endpoints/index.mdx),**
allowing you to create REST endpoints for any GraphQL query or mutation.

### Schema Registry

In Hasura v2, the schema registry was available to track changes to your API schema and metadata. This feature was
useful for auditing and versioning your API.

**In Hasura DDN, the schema registry is fully supported**. Our
[schema diffing feature](graphql-api/graphql-schema-diff.mdx) allows you to track changes to your API schema and
metadata, ensuring that you're always in control of your API's evolution.

### Read Replica

In Hasura v2, read replicas were available only on Cloud and Enterprise Editions, providing improved read performance
and redundancy.

**In Hasura DDN, read replicas are fully supported**, ensuring high availability and performance.

### Caching

In Hasura v2, caching was available only on Cloud and Enterprise Editions, allowing you to cache query results for
faster response times.

**In Hasura DDN, caching is available via [engine plugins](/plugins/caching/index.mdx),** allowing you to create a
configurable cached response for any GraphQL query.



--- File: ../ddn-docs/docs/upgrade/feature-availability/tooling.mdx ---
# Tooling

---
sidebar_position: 3
description: Learn what features are available in Hasura DDN when compared to Hasura v2.
title: Tooling
keywords:
  - hasura
  - docs
  - upgrade
  - features
---

import Thumbnail from "@site/src/components/Thumbnail";

# Tooling

## Introduction

We've built Hasura DDN from the ground up for scale, both in terms of performance and the ease with which teams can
collaborate on any given API. Hasura DDN introduces several improvements to enhance the development experience,
particularly in areas where collaboration and large-scale management are critical.

| Feature             | v2      | DDN |
| ------------------- | ------- | --- |
| The Hasura console  | âœ…      | âœ…  |
| The Hasura CLI      | âœ…      | âœ…  |
| IDE Integrations    | âŒ      | âœ…  |
| Database Migrations | âœ…      | âŒ  |
| Prometheus          | âœ… (EE) | âœ…  |
| OpenTelemetry       | âœ… (EE) | âœ…  |

**\*EE**: Available on Cloud and Enterprise editions only.

## Features

### The Hasura console

In Hasura v2, the console played a central role as the primary interface for authoring and editing your GraphQL API and
the underlying data sources. The console provided a user-friendly graphical interface for various tasks, making it
easier to configure and interact with your Hasura instance without needing to write complex code or commands.

**In Hasura DDN, the console serves primarily as an exploration and management tool**. It allows you to visualize,
explore, test, and deploy your API. Unlike in v2, the console in Hasura DDN is not used to author or edit your API.
However, we've expanded the console's features to include granular analytics and the ability to add collaborators with
read-only status. This makes it easy to share and onboard an API quickly and efficiently.

### The Hasura CLI

In Hasura v2, the CLI was used for various tasks such as managing metadata, applying migrations, and working with
environment configurations. It provided a command-line interface to automate and script these operations, making it
easier to integrate Hasura into CI/CD pipelines.

**In Hasura DDN, the CLI becomes your primary tool for constructing an API**. It is used for creating local and cloud
projects, scaffolding out metadata, and creating iterative, immutable builds of your API. Additionally, the CLI helps
streamline deployment processes and allows for easy automation, maintaining the functionality of managing and
configuring your API from the command line.

The CLI also facilitates declarative work with Hasura metadata in conjunction with
[Hasura's VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura), which provides
auto-complete and error hinting. The new metadata folder and file structure enhances your ability to reason and navigate
your metadata intuitively.

The CLI is at the center of it all, interacting with key components to help you iterate faster. Check out the
[new command structure here](/reference/cli/index.mdx) or
[learn how to create your first project here](/quickstart.mdx).

### IDE Integrations

In Hasura v2, IDE integrations were not available, making it difficult to work with Hasura metadata and configuration
files in your favorite code editor.

**In Hasura DDN, we've introduced IDE integrations** to make it easier to work with Hasura metadata and author your API,
all from within your editor. Currently, we support
[VS Code via an extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura), with plans to expand to
other editors in the future.

### Database Migrations

In Hasura v2, database migrations were supported through the Hasura CLI, allowing you to manage changes to your database
schema and metadata.

**In Hasura DDN, database migrations are not supported**. However, there are a variety of excellent free alternatives
available, such as [Flyway](https://flywaydb.org/), [DbMate](https://github.com/amacneil/dbmate), and
[Liquibase](https://www.liquibase.org/), that can be used in conjunction with Hasura DDN to manage your database schema
changes. Check back soon where we'll have dedicated guides for using these tools in conjunction with CI/CD for your
projects.

### Prometheus

In Hasura v2, the Prometheus integration was available only on Cloud and Enterprise Editions for monitoring and alerting
based on API metrics.

**In Hasura DDN, the Prometheus exporter is natively supported** and free to use with both the Hasura engine and with
data connectors.

### OpenTelemetry

In Hasura v2, OpenTelemetry was available only on Cloud and Enterprise editions (denoted by (EE)) for distributed
tracing and observability.

**In Hasura DDN, much like Prometheus, OpenTelemetry is natively supported** and free to use with both the Hasura engine
and with data connectors.



--- File: ../ddn-docs/docs/help/overview.mdx ---
# Overview

---
title: Overview
sidebar_position: 1
description:
  "Get Help and Support with Hasura DDN. Explore the docs, ask the DocsBot, join the community, or ask the support team."
sidebar_label: Overview
keywords:
  - help
  - support
  - docs
  - docsbot
  - community
  - support team
  - enterprise clients
---

# Help and Support Overview

## Self-Serve Options

### Explore the docs

Start by exploring our [quickstart guide](/quickstart.mdx) to get up and running or dive into the
[tutorials](/how-to-build-with-ddn/overview.mdx) for a step-by-step approach written specifically for your first data
source. Once you're comfortable, expand your knowledge with sections like [data sources](data-sources/overview.mdx),
[data modeling](data-modeling/overview.mdx), [auth](auth/overview.mdx), [observability](observability/overview.mdx), and
[deployment](deployment/overview.mdx).

For quick answers to common questions, visit our [frequently asked questions](/help/faq.mdx) and
[glossary](/help/glossary.mdx) sections.

### Ask the DocsBot

Need additional guidance? The DocsBot, powered by OpenAI's GPT-4o model, is here to help! Click the "Docs Assistant"
button in the header to get tailored answers directly from our documentation.

### Ask the community

- We have a Discord server where you can ask questions and get help from the community and Hasura champions.
  [Join here](https://discord.com/invite/hasura).
- Additionally, you can create a GitHub issue for any bugs or feature requests
  [here](https://github.com/hasura/graphql-engine/issues).

## Enterprise clients

If you have a paid plan or a dedicated Enterprise contract, you can
[ask the support team for help](https://hasurahelp.zendesk.com/hc/en-us/requests/new).

For details on our Service-Level Agreements (SLAs), version support, data privacy, or submitting feature requests and
bug fixes, please refer to our [policies documentation](/help/policies/index.mdx). This comprehensive resource outlines
everything you need to know about our commitments, processes, and how we handle updates for Enterprise clients.



--- File: ../ddn-docs/docs/help/faq.mdx ---
# FAQ

---
sidebar_position: 2
sidebar_label: FAQ
description:
  "Explore the Frequently Asked Questions (FAQs) for Hasura DDN and get essential insights about its architecture, CI/CD
  features, benefits for developers, data connectivity, and more. Understand how Hasura DDN is transforming API
  management and modern application development."
keywords:
  - hasura ddn faq
  - hasura cli
  - microservices
  - api creation
  - app development
  - hasura ddn architecture
  - cloud service
  - data connectivity
  - hasura runtime engine
seoFrontMatterUpdated: true
---

# Hasura FAQ

## What is the general architecture of Hasura DDN?

Hasura DDN is built on the major advancements made in Hasura Version 3:

**Hasura Runtime Engine:** The Hasura Engine is responsible for processing and serving APIs based on provided metadata.
In DDN, the engine is has a clear separation of build time and runtime concerns which is analogous to compiled
programming languages. Metadata can now be authored, edited and built declaratively without causing any downtime. The
new engine can also serve production-grade APIs from the edge with extremely high performance since it accesses metadata
on a per-request basis and benefits from vastly improved startup times. Essentially the cold start problem has been
removed in DDN leading to zero-downtime rollouts.

**Native Data Connector (NDC) Specification:** The NDC specification describes how the API request will be executed and
how the engine will interact with the underlying data source and is a new data connectivity specification from Hasura.
In DDN, there is now no concept of "native databases" and access to all data sources and functionality including
PostgreSQL will be based on the native data connector specification, implemented via Connectors. This helps in providing
rich feature sets for _all_ data sources as compared to just a few select ones. Moreover, there is now a strong focus on
enabling the community to build data connector agents themselves. The specification and provided
[Rust](https://github.com/hasura/ndc-hub#rust-sdk) and [Typescript](https://github.com/hasura/ndc-sdk-typescript) SDKs
support building high-quality integrations for almost any conceivable data source (both databases and APIs), offering
features like native queries, push-down capabilities, and connection pooling. [Learn more](/data-sources/overview.mdx).

**Supergraph:** A supergraph is an architecture and operating model to build and scale multiple data domains (or
subgraphs) as a single graph of entities and operations.

**Subgraphs:** Subgraphs introduce a module system for metadata. They allow multi-team organizations working on a Hasura
project to segment access to different metadata objects. A project's metadata is the amalgamation of metadata objects
from all its subgraphs, and each subgraph can be accessed and updated independently. It also helps in avoiding conflicts
when there are, for example, tables with the same names in different data sources. [Learn
more](/project/configuration/overview.mdx **Hasura Cloud:** The cloud layer in Hasura DDN, Hasura Cloud, introduces the
Data Delivery Network (DDN) for global API performance and availability. This layer also offers tools for managing
projects, builds, subgraph and essentially represents the control plane for Hasura DDN.

**DDN CLI:** The new DDN CLI (Command-Line Interface), serves as a powerful tool for developers to interact with the
Hasura platform. It allows users to initialize projects, manage metadata, track objects, create builds, and deploy data
connectors that service an API in the cloud, easily. [Learn More](/reference/cli/index.mdx)

**DDN Console:** The DDN Console has been revamped to enable rapid onboarding, testing and troubleshooting. The console
now provides rich visualizations, deep dive metrics into models, enhanced traces, the new GraphiQL API explorer, new
operational health dashboard and a refurbished data manager that scales seamlessly with heavy loads.
[Learn More](https://console.hasura.io/)

**Builds:** Each metadata change in Hasura now results in an atomic build which is represented by an immutable artifact
that is loaded onto Hasura DDN instantly. Which means there is inherent version control and the ability to push metadata
changes instantly and without errors. [Learn more](/project/configuration/overview.mdx

## What will be the benefit for me, the developer? (Why would I want to use or switch to DDN?)

- Significant improvements to metadata authoring and management.
  - And end to the cold-start problem and instant changes to metadata. Deploy multiple times in an hour. Deploy with
    lightning speed and sub-second CI/CD.
  - No dependency on upstream services which means zero-downtime deployments.
  - Code driven workflows with advanced code generation tooling.
  - A single spec to define your whole API across diverse data sources and business logic.
  - Declarative metadata authoring with full control on API schema generation.
  - No metadata database - one less thing to manage in production.
- Connect to any data or run any business logic with Native Data Connectors (NDCs).
- Compose all your data sources, APIs and information into a single, powerful, inter-related supergraph.
- Work better with your team with improved version control and collaboration features.
- Unparalleled performance at any scale with optimized query execution and caching mechanisms.
- Access a global edge network for low-latency, high-performance APIs with the Data Delivery Network (DDN).
- Bring the programming language of your choice such as Typescript to compose business logic
- Perform advanced mutations and workflows within the request response lifecycle.

## What are the benefits of Subgraphs?

- **Modular Development:** Subgraphs promote modular development by allowing teams to independently develop, test, and
  deploy their code without impacting the functionality of other subgraphs. This can also be done in independent
  repositories for each subgraph, in a "multi-repo" setup. This modular approach simplifies code management and reduces
  the risk of breaking changes. Subgraphs can be tested in context of the full supergraph to ensure that they work as
  expected when combined.
- **Independent Deployment:** Subgraphs can also be deployed individually, ensuring that updates to one subgraph do not
  require downtime for the entire supergraph. This flexibility allows for continuous integration and delivery (CI/CD)
  practices that accelerate feature releases.
- **Improved Collaboration:** Subgraphs allow different teams to focus on their specialized data domains, fostering
  collaboration by enabling them to share their APIs and data through a unified interface. This collaborative but
  isolated workflow environment accelerates development times and reduces friction between teams.
- **Strong Governance:** Hasura DDN provides robust governance features, such as project collaboration roles and
  permissions that ensure data integrity and security across subgraphs. These features enable teams to enforce data
  policies and restrictions, protecting sensitive information and maintaining compliance with regulatory requirements.
- **Efficient Data Management:** Federation simplifies data management by enabling teams to work with smaller, more
  manageable data domains. Teams can focus on their specific data requirements without being overwhelmed by the entire
  data schema.

## What CI/CD features does Hasura DDN offer?

**Instant CI/CD:** Hasura brings instant CI/CD capabilities, enabling developers to test and validate changes rapidly.

**CLI-Controlled Development:** The Hasura Command-Line Interface (CLI) plays a central role in controlling and managing
metadata builds and connector deployments. It allows developers to manage projects, create builds and promote them to
production. This CLI-centric approach simplifies the deployment process and enables time-saving CI/CD pipelines.
[Learn more](/reference/cli/index.mdx)

**Builds and Project Shares:** With Hasura, metadata states are represented as builds. Once a build is thoroughly tested
and validated, it can be applied to a project as the live API and made available to API consumers globally. This
separation of build and production steps ensure a smoother transition from development to production.

**Extremely Fast Builds:** Hasura also introduces cloud innovations that allow metadata builds to be created instantly,
regardless of the number of models. This means that updating even a large number of models is a swift and seamless
process and developers can rapidly test multiple builds within a day.

**Zero Downtime Rollouts:** With Hasura, going to production is facilitated without restarts and with zero downtime.
This ensures that your API remains available to users while updates are being applied.

**Metadata Version Control:** Hasura includes in-product metadata version control. Each build is associated with a
unique build ID, making it easier to track and manage different versions of your metadata. This enhances transparency
and auditability during the development process.

## What can be automated?

Since metadata authoring is mostly code driven using the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) and the CLI, many
metadata related changes can be automated using a version management and CI/CD tool of your choice.

## How does Hasura DDN connect to data sources?

Native Data Connectors (NDC) are a framework introduced in Hasura DDN that enables developers to use pre-built data
connector agents or build their own.

You can think of Data Connectors as a bridge or driver between Hasura DDN and external data sources.

Data Connectors facilitate the integration between Hasura and external data sources, allowing Hasura to seamlessly
interact with a wide range of databases, services, and APIs. NDCs can be built for almost any conceivable source of data
and are the only way to connect to data sources in Hasura DDN.

Native Data Connectors can either be self-hosted or hosted by Hasura. [Learn more](/data-sources/overview.mdx)

## I just have a database, why do I need to know about Data Connectors?

Essentially, you donâ€™t. You can rely on our Hasura VS Code Extension and Hasura CLI to do much of the code generation
and validation, but with knowledge of Data Connectors you will better understand your Supergraph architecture and you
can build custom integrations to any data source or custom function or API.

If you're interested in this you can check out our Rust or
[Typescript SDKs](https://github.com/hasura/ndc-sdk-typescript), and also our
[Node.js Lambda Typescript Connector](https://github.com/hasura/ndc-nodejs-lambda).

## How do I build my own data connector?

You can find information about building a connector in the [NDC Spec](https://github.com/hasura/ndc-spec) and the
rendered [NDC Spec data connector tutorial](https://hasura.github.io/ndc-spec/tutorial/index.html) as well as the SDKs
for various languages:

- [Rust](https://github.com/hasura/ndc-sdk-rs)
- [TypeScript](https://github.com/hasura/ndc-sdk-typescript)
- [Python](https://github.com/hasura/ndc-sdk-python)
- [Go](https://github.com/hasura/ndc-sdk-go)

## Is Hasura DDN open source?

The [Hasura DDN engine](https://github.com/hasura/graphql-engine/tree/master/v3) and all
[Hasura data connectors](https://hasura.io/connectors) are open-source and can be run on your own infrastructure. With
the optimizations and features that Hasura DDN offers at the price point, our cloud service is the best way to use
Hasura and self-hosting should be an option when specific requirements are needed and enterprise policies dictate.

## What is the pricing structure for DDN?

Check out the pricing page here. [Hasura DDN Pricing](https://hasura.io/pricing/v3)

## What is the recommended approach to metadata authoring?

The DDN CLI and VS Code extension will help you author. You can use the CLI to create a local directory scaffolding for
you after which you can further author metadata using CLI commands and the VS Code extension. The console is intended to
tool and for running queries, visualization and monitoring.

We expect most metadata management will happen via the code driven tools that we have in DDN. We are investing in
extensive code generation capabilities via the Language Server Protocol baked into the IDE tools and users will still
get to enjoy the quick authoring experiences of v2 such as that of permission building, outside the console.

## Iâ€™m operating in a given cloud provider region. How can I have Hasura run in the same region?

Hasura DDN is strategically deployed in 10 regions worldwide. (We plan to increase coverage drastically as we move
closer to Beta and beyond) Depending on the origin of a request, we automatically activate an instance of Hasura in the
nearest region. This process is completed in under 10 milliseconds, ensuring consistently optimal performance, closest
to your users.

## Will Hasura DDN be compatible with Hasura v1/v2 and how do I migrate?

The default generated GraphQL schema (ie: the API your consumers use) of Hasura DDN is compatible with v2 schemas and
will not change. You can learn more about the similarities between Hasura v2 and Hasura DDN in
[these docs](/upgrade/feature-availability/api-features.mdx).

## Will Hasura v2 be supported after Hasura DDN is released?

We will still support, maintain and improve Hasura v2 as per our
[version support policy](https://hasura.io/docs/latest/policies/versioning/) and will be providing migration tools to
move Hasura v2 deployments to DDN in the near future.

## If I have an Alpha/Beta DDN project, will it be compatible with the GA release?

We recommend [creating a new project](/quickstart.mdx) and use the new CLI to populate metadata again.

## Will Hasura continue supporting existing plans, specifically, the Cloud Free and Professional plans?

Yes. We are committed to maintaining & supporting the Free and Professional plans to ensure there is no disruption to
our users.

## I heard DDN is built in Rust. Why Rust over Haskell?

We have a lot of love for Haskell. It's a beautiful language that we have been using for the Hasura engine since the
beginning, and will still be using it on Hasura v2. Without going into the weeds in a FAQ, we wanted to benefit from
Rust's performance, ecosystem and community for DDN. Rust is a modern, high-performance, and memory-safe language. It is
also gaining much traction in the cloud-native space, and we are excited to be using it for the significant parts of the
Hasura DDN codebase. If you are a Rust developer and want to do cool stuff, please check out our careers page.

## If currently running a v2.x will it be moved automatically to v3?

No, it will not. The v2 cloud projects will remain untouched and there will be no change of business. Moving to DDN will
be an opt-in process. Having said that, we strongly encourage upgrading to DDN early when features you require are
available. Our teams are available to make the migration process as smooth as possible and please keep an eye on our
[official announcements and releases](https://hasura.io/community/) for upcoming information about migration tools.

## Can we request to lock ourselves to 2.x?

The migration is an opt-in process, so you do not need to request to have yourselves locked to a particular version. If
however, you see no upgrade path even in the next two years, please give us a heads-up and we will work out options with
you.



--- File: ../ddn-docs/docs/help/glossary.mdx ---
# Glossary

---
sidebar_position: 2
sidebar_label: Glossary
description:
  "Explore the key terms and definitions used within Hasura DDN (Data Delivery Network), including concepts such as
  metadata specification, native data connectors, and more. Gain a better understanding of how to design and manage the
  Hasura architecture effectively."
keywords:
  - hasura
  - ddn
  - data delivery network
  - metadata authoring
  - data domains
  - native data connectors
  - ndc spec
  - hasura terms
seoFrontMatterUpdated: true
---

# Hasura DDN Glossary

## Hasura v3

Hasura v3 represents a major version change for the main underlying component of Hasura Cloud.

There are significant enhancements to the engine, including:

1. A new architecture
2. A switch to the Rust programming language
3. A new specification to work with the engine.

Because of the new specification, the engine is now decoupled from the API protocol, such as GraphQL, and as a result in
the coming years the `graphql-engine` will evolve to become the `graphql-api-engine` and finally the `api-engine`.

Apart from the enhancements to the `graphql-engine`, Hasura v3 also introduced the rollout of the new Native Data
Connector Specification and the associated connectors.

## Hasura Data Delivery Network (DDN)

Hasura DDN is a brand-new offering that is powered by the innovations in Hasura v3. While Hasura v3 data plane is open
source, DDN represents a managed service for that data plane with the promise of operationally excellent APIs and
replaces the application/API server infrastructure requirement for organizations.

On top of that, Hasura DDN is a globally distributed, and always available cloud for APIs and data connectivity, which
enables blazingly-fast and secure delivery of real-time data. The new runtime engine in Hasura DDN accesses metadata on
a per-request basis, enabling improved isolation and scalability.

Apart from the managed data plane, DDN also includes a brand-new control plane which comprises metadata authoring,
CI/CD, cloud infrastructure components, and collaboration features. In v2, we bundled the control plane and data plane
together. In v3, we have separated the control plane and data plane. The control plane is closed source and commercial
only.

## Control plane

The control plane manages the configuration, orchestration, and coordination of the data plane elements along with
providing tools to author metadata. It is responsible for setting up and managing the behavior, policies, and rules that
govern how data is processed and forwarded in the data plane. It also oversees the overall behavior of the system,
manages the API endpoints, maintains the API configurations, handles authentication and access control mechanisms, and
gathers analytics or metrics related to API usage.

## Data Plane

The data plane manages the actual handling of API requests and responses. It deals with the execution of API operations,
the transmission of data between clients and the API endpoints, and the processing of data payloads. The following are
the main components of a data plane:

**Hasura v3 GraphQL Engine:** The engine takes in an API request (a GraphQL query for example), converts it into an
intermediate representation that the connectors can handle, and creates a plan for the query execution across various
data sources.

**Hasura connectors:** These handle the actual API execution. They accept the intermediate representation from the
engine and use the most efficient mechanism to execute the query and fetch/mutate data from the underlying data source.

Both the components mentioned above are open sourced.

## Supergraph

A supergraph is an architecture and operating model to build and scale multiple data domains (or subgraphs) as a single
graph of data entities and operations.

Supergraphs help us to benefit from a centralized monolithic approach (high cohesion and easy governance) on a federated
microservices execution model (loose coupling and scaling ownership).

Today, supergraphs are more critical than ever because accelerating data and microservice sprawl are making the
complexity of data and API consumption untenable â€“ showing up in slower time-to-market, harder to address tech-debt and
complex team communication. [Learn more](https://hasura.io/supergraph).

## Subgraphs

A subgraph represents a self-contained module of metadata and its accompanying data connector(s) which encapsulates a
specific data domain. Subgraphs have a permission model, and an independent software development life cycle, and can be
developed, tested, and built independently. The supergraph guarantees the integrity of subgraph composition.

Subgraphs can be managed in their own isolated repositories and built completely independently of each other. Fields
exposed to the full supergraph can be prefixed to prevent schema conflicts. Data can be interlinked between subgraphs
using relationships, even between subgraphs in separate repositories.

A subgraph is analogous to a microservice owned by a particular team.

## Globals subgraph

When running the `ddn supergraph init` command, a `globals` subgraph is created by default for your convenience. This
subgraph is intended to hold global configuration objects for the supergraph, such as API configuration and auth
settings.

These configuration objects are `AuthConfig`, `CompatibilityConfig` and `GraphqlConfig` as well as the `subgraph.yaml`
configuration file which defines the globals subgraph itself.

These objects are located by default in the `globals` subgraph, but can be moved to any other subgraph if needed.

## Hasura Metadata

Hasura metadata models a supergraph and specifies the API for the supergraph. It is the configuration that is provided
declaratively to help connect to the data source and provide a working API. It introduces key supergraph modeling
constructs such as `Types`, `Models`, `Commands`, `Permissions`, and `Relationships`, which help in understanding and
implementing systems that align closely with the real-world domain they are meant to represent. Apart from modeling
constructs, the metadata also defines key configuration around API security, caching, deployment, and CI/CD, that helps
explain the entire API system for the organization. [Learn more](/reference/metadata-reference/index.mdx).

## .hml (Hasura Metadata Language)

The file extension for the files that conform to the Hasura metadata specification for the supergraph. It is a
derivative of `.yaml` extension (and shares the same syntax) and provides the same benefits such as 1) readability, 2)
data structures, 3) comments, and 4) portability, to author, reason, and share metadata.

## Supergraph modeling

The act of writing information to conform to the Hasura metadata specification and the process of identifying, defining,
and structuring the distinct sets of the supergraph elements or attributes.

## Immutable-Build Runtime System

The new runtime engine in Hasura DDN, which accesses metadata on a per-request basis, enables improved isolation and
scalability. This independent build system allows a runtime that eliminates shared state and cold start issues for
enhanced performance.

## Data Sources

Any external data source, database, or service that can be connected to Hasura DDN using a Data Connector agent. Every
data source must have connector URL and schema. [Learn more](/reference/metadata-reference/data-connector-links.mdx).

## Native Data Connector Specification (NDC Spec)

A standardized specification that allows you to extend the functionality of the Hasura server by providing web services
that resolve new sources of data and business logic and help define the metadata structure for APIs in Hasura DDN. The
specification defines types such as `collections`, `functions`, and `procedures` that help in describing the behavior of
agents or connectors that connect to the underlying data source. It provides a framework and guidelines on the types of
web service endpoints that a connector needs to implement.
[Learn more](/reference/metadata-reference/data-connector-links.mdx).

## Native Data Connectors

Data connectors are specialized agents that connect subgraphs to data sources, enabling communication in the data
source's native language.

They integrate Hasura with various external data sources and services based on the native data connector specification.

Each subgraph can have multiple data connectors, and the same data source can be connected to different subgraphs via
separate connector instances - allowing teams to work with the same source from different domain perspectives.

Data connectors are available for a wide variety of sources including databases, business logic functions, REST APIs,
and GraphQL APIs. They can be official Hasura-verified connectors or custom-built connectors to integrate with other
data sources. [Learn more](/data-sources/overview.mdx).

## Push-Down Capabilities

The ability in Hasura DDN to delegate certain query operations including Authorization to the underlying data source.
This can improve query optimization and performance and is the reason why data connectors in Hasura DDN are called
'native'.

## Connector Hub

Refers to the public site where all Native Data Connectors for Hasura DDN are listed. Users can discover connectors, get
more information about their specific features and find documentation on how to use each connector with Hasura DDN.
[Learn more](https://hasura.io/connectors/).

## Model

A metadata object that is fundamental to API design in Hasura DDN. The model is the entity that has a direct mapping to
the underlying native data connector object or collection.

A model includes reference to the data type and includes configuration details related to API configuration, arguments
and global ids.

It supports select, insert, update and delete operations. Within the select operation the different query operations
include filtering, aggregating, paginating and limiting. [Learn more](/reference/metadata-reference/models.mdx).

## Command

The Hasura entity that helps encapsulate business logic and represents an action that can be performed which returns
back some type of result. It directly maps to the native data connector object's functions and procedures.
[Learn more](/reference/metadata-reference/commands.mdx).

## Relationships

A metadata object in Hasura DDN that defines the relationship between two models or between a model and a command.

Defining a [relationship](/reference/metadata-reference/relationships.mdx) allows you to make queries across linked
information within and between subgraphs.

When working with relationships across subgraphs in other repositories, there are some differences to be aware of. Find
out more about cross-repo relationships
[here](project-configuration/subgraphs/working-with-multiple-subgraphs.mdx#cross-repo-relationships).

As always when authoring metadata, the
[Hasura VS Code extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura) can assist with
auto-complete and validation.

## Authorization Permissions

Authentication is managed at the supergraph level in Hasura DDN as defined by the `AuthConfig` object and cannot be
customized at the subgraph level.

Authorization however, which is a metadata object in Hasura DDN that defines the access control or authorization rules
on models and commands. [Learn more](/reference/metadata-reference/permissions.mdx), is managed at the subgraph level in
Hasura DDN. This means that the permissions for models and commands are defined within the context of those objects in
their respective subgraphs, and do not affect other subgraphs.

Authorization rules in one subgraph can also be defined to reference data in a foreign subgraph even if that subgraph is
in another repository.

## Global ID

Refers to the Relay global ID that encodes the type and ID of an object in a single string. In Hasura this is defined
per model and enables fetching any object directly, regardless of what kind of object it is. A result of this is that
you get the node root field in the GraphQL API schema to use with a Relay client.
[Learn more](/reference/metadata-reference/models.mdx).

## Collection

A collection is a Native Data Connector Spec object, which encapsulates part of a data source, providing standard
querying capabilities.

Each collection is defined by its name, any collection arguments (need to parametrize the collections), the object type
(a collection of fields) of its rows, and some additional metadata related to constraints.

Tracking a collection results in the creation of a 'model' object in Hasura metadata.
[Learn more](https://hasura.github.io/ndc-spec/specification/schema/collections.html).

## Function

A function is a Native Data Connector Specification object that can be invoked with arguments to get a result, and which
doesn't have side-effects and is thus "read only". Unlike collections, functions do not describe constraints and do not
have object types.

Tracking a function results in the creation of a `Command` Supergraph object in Hasura metadata.
[Learn more](https://hasura.github.io/ndc-spec/specification/schema/functions.html).

## Procedure

A procedure is a native data connector specification object, and it defines an action that the data connector implements
and can mutate data and have other side-effects. Each procedure has arguments and a return type.

Tracking a procedure results in the creation of a `Command` object in Hasura metadata.
[Learn more](https://hasura.github.io/ndc-spec/specification/schema/procedures.html).

## Builds

Each metadata change in Hasura DDN represents an immutable build. Every build has a unique GraphQL Endpoint that can be
tested independently. Builds exist in in projects and there is a one-to-many mapping between projects and builds. [Learn
more](/project/configuration/overview.mdx

## Supergraph config

Supergraph config tells Hasura DDN how to construct your supergraph. A config will contain information such as which
subgraphs to include and which resources to use for the build. [Learn more](/project/configuration/overview.mdx

## Connector config

Connector config tells Hasura DDN how to build your connector. It will contain information such as the type of connector
and the location to the context files needed to build the connector. [Learn more](/project/configuration/overview.mdx

## DDN CLI (Command-Line Interface)

A tool in Hasura DDN that enables developers to interact with DDN from the command line. It supports various commands
for creating builds, tracking objects, and deploying projects. [Learn more](/reference/cli/index.mdx).

## VS Code Extension

The Hasura VS Code extension enables features like inline validation of Hasura DDN metadata without having to create
builds, code scaffolding snippets which can be used to quickly scaffold DDN metadata objects, intelligent autocomplete
which shows autocomplete suggestions based on the current state of the DDN project, and other features like
go-to-definition for inter-related DDN metadata objects, documentation on hover of any DDN metadata object, a project
tree on sidebar which shows all DDN projects and metadata objects present in the currently opened folder.

It is strongly recommended to download the VS code extension while working with DDN projects, it can be downloaded from
[VS Code extension marketplace](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura).

## Metadata build service

Creates builds from the metadata and makes it available to Hasura v3 GraphQL Engine at the edge for it to serve the API
request. Provides essential error handling for fast debugging and troubleshooting.

## Control plane cloud API

This component is the underlying cloud service, which enables creating builds, applying builds and testing your API. We
consider this the brain of DDN Console and CLI â€“ the component that drives most of the DX functionalities.

## DDN Console

An interface in Hasura DDN that provides tools for metadata visualizing, API testing and deployment, team collaboration,
documentation, traces, and analytics.

## Cloud PAT

This refers to a personal authentication token that Hasura Cloud creates automatically for you on every new project
creation. This ensures that your GraphQL API always has a security mechanism. The auto-generated PAT is included in the
API header `cloud_pat`.



--- File: ../ddn-docs/docs/help/policies/index.mdx ---
# Hasura policies

---
title: Hasura policies
description: Hasura policies
sidebar_label: Overview
sidebar_position: 0
keywords:
  - hasura
  - policies
  - security
---

# Hasura Policies

This section contains documents and strategies which outline Hasura's operational policies.

- [Version Support Policy](/help/policies/versioning.mdx)



--- File: ../ddn-docs/docs/help/policies/versioning.mdx ---
# Version Support Policy

---
description: The version support policy for Hasura GraphQL Engine
sidebar_label: Version Support Policy
sidebar_position: 1
keywords:
  - version
  - support
---

# Hasura Version Support Policy

## Releases and support

Hasura DDN releases its software via independently versioned components, such as:

- **Engine**
- **Connectors**
- **CLI**

Each of these components is released independently. While they do not have "versioned" releases as a unified product,
their individual versions are critical for ensuring compatibility and understanding what to use and upgrade. Versions
for the CLI and connectors follow semantic versioning:

- **Major versions** may include incompatible or breaking changes.
- **Minor versions** introduce new functionality and bug fixes in a backwards-compatible manner.
- **Patch versions** provide backwards-compatible bug fixes.

:::info calver vs. semver

The **Engine** utilizes [calver (calendar versioning)](https://calver.org/) instead of semver, with versions aligned to
the year, month, and date of the release. This approach provides clarity on the recency of releases.

:::

### When to upgrade a component

Managing independently versioned components offers flexibility but requires careful planning to ensure compatibility and
functionality. Follow these guidelines to decide when and how to upgrade:

1. **Engine**:

   - **When to upgrade**: Upgrade when new features, performance improvements, or critical bug fixes are introduced in
     the release notes.
   - **Compatibility details**: The Engine utilizes a `CompatibilityConfig` object that defines the compatibility
     configuration of the Hasura metadata. Learn more [here](/reference/metadata-reference/compatibility-config.mdx).
   - **Why it matters**: The Engine is the core of your GraphQL API, and updates may introduce new capabilities or
     optimizations.

2. **Connectors**:

   - **When to upgrade**: Upgrade when your current connector version does not support features or updates in the
     Engine. Verify compatibility using the connector release notes.
   - **Why it matters**: Connectors ensure seamless integration with your data sources, and outdated versions may lead
     to errors or limited functionality.

3. **CLI**:
   - **When to upgrade**: Upgrade when new commands, workflows, or fixes are introduced, or when compatibility with the
     Engine or connectors is required.
   - **Why it matters**: The CLI is your primary tool for automation and deployment. Staying up-to-date ensures you can
     use all supported features effectively.

#### Example: Coordinating component upgrades

Imagine the Engine releases a new feature in version `2024.07.10`. Before upgrading, check the release notes for:

- Supported connectors for this version of the Engine.
- Required CLI versions to use the new Engine capabilities.

**Step-by-step upgrade process:**

1. Review the [Hasura Changelog](https://hasura.io/changelog) for details about the new Engine version.
2. Cross-check connector and CLI release notes to confirm compatibility with the new Engine version.
3. Upgrade the Engine first, followed by connectors and CLI, in that order, to maintain compatibility and minimize
   disruptions.

## Reporting bugs and requesting fixes

Hasura is committed to providing enterprise-grade reliability and support to ensure your business operates seamlessly.
For bug reporting and resolution, the following process ensures efficiency and transparency:

1. **Reporting Bugs**:

   - Bugs can be reported through the [Hasura Support Portal](https://support.hasura.io/hc/en-us/requests/new).
   - Customers are encouraged to provide detailed descriptions, logs, and replication steps to help expedite resolution.

2. **Bug Triage and Prioritization**:

   - Reported issues are triaged based on severity and impact, prioritizing critical bugs that affect system security,
     availability, or functionality.
   - Issues impacting production environments are escalated for immediate attention.

3. **Bug Fix Releases**:
   - Fixes for critical issues are rolled out as patch updates. Non-critical issues are included in the subsequent minor
     or major releases.
   - Customers are notified through release notes, the [Hasura Changelog](https://hasura.io/changelog), and direct
     communication for mission-critical updates.

## Security updates and communication

Security is a top priority for Hasura. To maintain the integrity of your systems, we adhere to the following practices:

1. **Proactive Security Patching**:

   - Vulnerabilities are addressed promptly with targeted patch updates.
   - Security patches are backward-compatible and applied to supported versions.

2. **Customer Communication**:

   - Security updates are communicated directly to enterprise customers through our Customer Success team and email
     notifications.
   - Detailed documentation and guidance are provided to ensure smooth implementation of critical updates.

## Rolling out updates

Hasura ensures a seamless update experience for enterprise customers by following these principles:

1. **Pre-Release Testing**:

   - Updates are rigorously tested across supported versions and environments to minimize disruption.

2. **Documentation and Support**:
   - Comprehensive release notes and upgrade guides accompany every update.
   - Our support team is available to assist with planning and executing updates, ensuring minimal impact on operations.
